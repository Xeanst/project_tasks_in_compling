{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<font size=\"6\">Языковое моделирование</font>"
      ],
      "metadata": {
        "id": "MUI379KICFyB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Языковые модели — важнейшая часть современного NLP. Практически во всех задачах, связанных с обработкой текста, напрямую или косвенно используются языковые модели. А наиболее известные недавние прорывы в области — это по большей части новые подходы к языковому моделированию. ELMO, BERT, GPT — это языковые модели."
      ],
      "metadata": {
        "id": "xyC9J2Kg79eK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Что такое языковая модель?"
      ],
      "metadata": {
        "id": "gxnSOnl7A-bC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Предположим, у нас есть модель поезда. Чем она отличается от настоящего поезда?\n",
        "- обладает некоторыми свойствами поезда (выглядит как он)\n",
        "- может вести себя аналогично поезду\n",
        "- хорошие модели обладают большим количеством вышеперечисленных качеств"
      ],
      "metadata": {
        "id": "JbRXbOvyB9Dy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"center\">\n",
        "    <table >\n",
        "     <tr>\n",
        "       <td>\n",
        "       \n",
        "<center><img src =\"https://i.postimg.cc/R0VNtBMc/train1.jpg\" width=\"250\"></center>\n",
        "\n",
        "<em>Нереалистичная модель</em>\n",
        "\n",
        "</td>\n",
        "\n",
        "<td>\n",
        "\n",
        "<center><img src =\"https://i.ibb.co/HgM1VcB/train2.jpg\" width=\"450\"></center>\n",
        "\n",
        "<em>Реалистичная модель</em>\n",
        "\n",
        "\n",
        "</td>\n",
        "     </tr>\n",
        "    </table>\n",
        "    </div>"
      ],
      "metadata": {
        "id": "aV_M_qwXEEmo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Аналогично — модель физического мира: построить некоторую конструкцию и понять, упадет ли она, какая будет траектория падения.\n",
        "- позволяет понять, какие события лучше согласуются с окружающим миром, какие более вероятны\n",
        "- может предсказать, что произойдет, учитывая некоторый \"контекст\""
      ],
      "metadata": {
        "id": "TtK84DTID5k3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src =\"https://i.ibb.co/hWLWzJH/physical-model.jpg\" width=\"700\"></center>"
      ],
      "metadata": {
        "id": "d-4VDi0pKgNz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "То же самое с языком. Языковая модель должна обладать какими-то свойствами языка и предсказывать вероятность события (текста, предложения, токена, символа) в некотором контексте."
      ],
      "metadata": {
        "id": "eM_BBCJ0LO8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"center\">\n",
        "    <table >\n",
        "     <tr>\n",
        "       <td>\n",
        "       \n",
        "<center><img src =\"https://i.ibb.co/d4hGRhY/keyboard.png\" width=\"400\"></center>\n",
        "\n",
        "<em>Клавиатура смартфона</em>\n",
        "\n",
        "</td>\n",
        "\n",
        "<td>\n",
        "\n",
        "<center><img src =\"https://i.ibb.co/TwD1Ryb/search.png\" width=\"600\"></center>\n",
        "\n",
        "<em>Поисковый запрос</em>\n",
        "\n",
        "\n",
        "</td>\n",
        "     </tr>\n",
        "    </table>\n",
        "    </div>"
      ],
      "metadata": {
        "id": "eOQ0jpHlNptO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Если модель способна предсказать вероятность следующего слова, то она уже достаточно много знает о языке.\n",
        "\n",
        "*Я люблю вкусную ...*\n",
        "\n",
        "На месте пропуска должно стоять неодушевленное существительное женского рода в винительном падеже, которое обозначает нечто съедобное (*еду, колбасу, рыбу* и т.д.)."
      ],
      "metadata": {
        "id": "sDuVcdKmOAOV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Как вычислить вероятность предложения?"
      ],
      "metadata": {
        "id": "XTcBh9iJO5OL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пусть у нас есть несколько цветных шаров. Как подсчитать вероятность вытащить зеленый шар?\n",
        "\n"
      ],
      "metadata": {
        "id": "8UfCmfhDQDVJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"center\">\n",
        "    <table >\n",
        "     <tr>\n",
        "       <td>\n",
        "       \n",
        "<center><img src =\"https://i.ibb.co/bB27GRc/balls.jpg\" width=\"350\"></center>\n",
        "\n",
        "<em>$$ \\frac {5}{5+5+5+5}= \\frac {1}{4}$$</em>\n",
        "\n",
        "</td>\n",
        "\n",
        "</tr>\n",
        "</table>\n",
        "</div>"
      ],
      "metadata": {
        "id": "vzzgDsueQpHL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попробуем сделать то же самое для подсчета вероятности предложений.\n",
        "\n",
        "$$ 1.\\,P(\\text {Colorless green ideas sleep furiously}) = \\frac {0}{|\\text{corpus}|} = 0$$\n",
        "\n",
        "$$ 2.\\,P(\\text {Furiously sleep ideas green colorless}) = \\frac {0}{|\\text{corpus}|} = 0$$\n",
        "\n",
        "При таком подходе предложения, которые никогда не встречались в корпусе, будут иметь нулевую вероятность. Однако языковая интуиция подсказывает, что предложение 1 должно быть более вероятным, чем предложение 2.\n",
        "\n",
        "Мы не сможем надежно оценить вероятности предложений, если будем рассматривать их как неделимые сущности. Можно оценивать вероятность не всего предложения целиком, а каждого из входящих в него слов в определенном контексте."
      ],
      "metadata": {
        "id": "cEfEvPp0Q51S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src =\"https://i.ibb.co/XL48yhW/sentence.png\" width=\"700\"></center>"
      ],
      "metadata": {
        "id": "nKd030fgTdHN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Языковая модель оценивает вероятность встретить предложение $S$ — последовательность слов $(w_1,\\cdots ,w_n)$. Вероятность предложения можно определить как произведение вероятности каждого слова с учетом предыдущих слов:\n",
        "$$P(w_1,w_2, \\dots, w_n) = P(w_1)P(w_2|w_1)P(w_3|w_1,w_2)\\dots P(w_n|w_1,w_2,\\dots,w_{n-1})= \\prod\\limits_{i = 1}^n P(w_i|w_1, \\dots, w_{i-1})$$\n",
        "Для каждого слова последовательности предсказывается вероятность встретить его в тексте при условии, что известно предыдущее слово: $w_2$ при условии $w_1$, $w_3$ при условии $w_1$ и $w_2$, и т.д."
      ],
      "metadata": {
        "id": "6PmMowSTUnOZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### N-граммные языковые модели"
      ],
      "metadata": {
        "id": "UK5kR0oNU2Ps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Истинные вероятности предложений неизвестны → можно обучить языковую модель оценивать эти вероятности. Вероятность рассчитывается на основе частоты встречаемости слов в корпусе текстов:\n",
        "$$P (w_i|w_1, w_2, \\dots, w_{i-1}) = \\frac{count(w_1, w_2, \\dots, w_i)}{count(w_1, w_2, \\dots, w_{i-1})}$$"
      ],
      "metadata": {
        "id": "QvwA_9vyfxf6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Однако снова возникнет проблема, что большинство последовательностей $w_1, \\dots, w_i$ не встречаются в корпусе.\n",
        "\n",
        "Можно воспользоваться [марковским свойством](https://en.wikipedia.org/wiki/Markov_property):\n",
        "\n",
        "- Вероятность слова зависит только от конечного количества предыдущих слов."
      ],
      "metadata": {
        "id": "m2QIaxs8f08X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Такие языковые модели называются *n-граммными*:\n",
        "\n",
        "$$P (w_i|w_1, w_2, \\dots, w_{i-1}) = P (w_i|w_{i-(n-1)}, \\dots, w_{i-1}) = \\frac{count(w_{i-(n-1)}, \\dots, w_{i-1}, w_{i})}{count(w_{i-(n-1)}, \\dots, w_{i-1})}$$\n",
        "\n",
        "*Биграммные* языковые модели — $n = 2$:\n",
        "$$ P (w_i|w_1, w_2, \\dots, w_{i-1}) = P (w_i|w_{i-1}) = \\frac{count(w_{i-1}, w_{i})}{count(w_{i-1})}$$\n",
        "\n",
        "$$ P(\\text {Where are we going}) = P(\\text {where}) \\times P(\\text {are|where}) \\times P(\\text {we|are}) \\times P(\\text{going|we})$$\n",
        "\n",
        "*Триграммные* языковые модели — $n = 3$:\n",
        "$$ P (w_i|w_1, w_2, \\dots, w_{i-1}) = P (w_i|w_{i-2}, w_{i-1}) = \\frac{count(w_{i-2}, w_{i-1}, w_{i})}{count(w_{i-2}, w_{i-1})}$$\n",
        "$$ P(\\text {Where are we going}) = P(\\text {where}) \\times P(\\text {are|where}) \\times P(\\text {we|where are}) \\times P(\\text{going|are we})$$"
      ],
      "metadata": {
        "id": "N6FOVeU_U_gi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Тем не менее, последовательность $w_{i-2}, w_{i-1}, w_i$ тоже может отсутствовать в корпусе. Чтобы не допустить зануления вероятности всего предложения, применяется сглаживание.\n",
        "\n",
        "Основная идея заключается в том, чтобы изменить количество вхождений таким образом, чтобы вероятность не становилась нулевой. Один из простый и эффективных алгоритмов — *сглаживание Лапласа* или *аддитивное сглаживание*. Представим, что мы видели каждую *n*-грамму хотя бы один раз (или $\\delta$ раз, $\\delta \\ge 0$).\n",
        "\n",
        "\n",
        "$$ P(w_i|w_{i-(n-1)}, \\dots, w_{i-1}) = \\frac{count(w_{i-(n-1)}, \\dots, w_{i-1}, w_{i}) + \\delta}{count(w_{i-(n-1)}, \\dots, w_{i-1}) + \\delta \\times |V|} $$"
      ],
      "metadata": {
        "id": "m2VWv0GreTKV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Генерация текста"
      ],
      "metadata": {
        "id": "8PSiPIMSlfzI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как только у нас будет языковая модель, мы сможем использовать ее для генерации текста. Мы делаем это по одному токену за раз: предсказываем распределение вероятности следующего токена с учетом предыдущего контекста и делаем выборку (sampling) из этого распределения."
      ],
      "metadata": {
        "id": "sP0nSDWaiWkC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div align=\"center\">\n",
        "    <table >\n",
        "     <tr>\n",
        "       <td>\n",
        "       \n",
        "<center><img src =\"https://i.ibb.co/LCv7Rrr/2.png\" width=\"400\"></center>\n",
        "<em>1</em>\n",
        "</td>\n",
        "\n",
        "<td>\n",
        "<center><img src =\"https://i.ibb.co/RyfR9Dx/3.png\" width=\"400\"></center>\n",
        "<em>2</em>\n",
        "</td>\n",
        "\n",
        "<td>\n",
        "<center><img src =\"https://i.postimg.cc/gJxjMHLj/4.png\" width=\"400\"></center>\n",
        "<em>3</em>\n",
        "</td>\n",
        "     </tr>\n",
        "     \n",
        "<div align=\"center\">\n",
        "    <table >\n",
        "  <tr>\n",
        "     <td>\n",
        "<center><img src =\"https://i.postimg.cc/jqJCPvrk/6.png\" width=\"400\"></center>\n",
        "<em>4</em>\n",
        "</td>\n",
        "\n",
        "<td>\n",
        "<center><img src =\"https://i.postimg.cc/BQT6w6bT/7.png\" width=\"400\"></center>\n",
        "<em>5</em>\n",
        "</td>\n",
        "\n",
        "<td>\n",
        "<center><img src =\"https://i.ibb.co/2g2DxPt/8.png\" width=\"400\"></center>\n",
        "<em>6</em>\n",
        "</td>\n",
        "     </tr>"
      ],
      "metadata": {
        "id": "TKJ7U_3tljIe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В качестве альтернативы можно применить жадный поиск (greedy decoding): на каждом шаге выбирается токен с наибольшей вероятностью. Однако обычно это работает не очень хорошо."
      ],
      "metadata": {
        "id": "j8xCINl-tH6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Генерация научных статей с помощью n-граммной модели"
      ],
      "metadata": {
        "id": "VQ0r0rxqtpZp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "k1gpzj4guo8e1riwj3om1k",
        "id": "yUUTv4PteCXH"
      },
      "source": [
        "Обучим языковую модель на [базе статей arXiv](https://www.kaggle.com/datasets/neelshah18/arxivdataset) и посмотрим, сможем ли мы создать новую статью.\n",
        "\n",
        "<center><img src =\"https://i.ibb.co/CW0DwSQ/ai.jpg\" width=\"600\"></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Подготовка данных"
      ],
      "metadata": {
        "id": "GByqOAZbzBSU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузим набор данных и запишем в переменную `data`."
      ],
      "metadata": {
        "id": "MNLe6ThuwK2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/Xeanst/NLP_course_FBB/main/data/arxivData.json"
      ],
      "metadata": {
        "id": "FHPWxSbRwBfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellId": "0c76vnyl3zui9yhtkodgrlf",
        "id": "W_KOl5xUeCXL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_json(\"arxivData.json\")\n",
        "data.sample(n=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Запишем название и краткое содержание статьи в отдельную переменную `lines`."
      ],
      "metadata": {
        "id": "nfKfBie-yKBs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellId": "lbyqb5rx7j8jpo591r06ak",
        "id": "VRAxswHgeCXM"
      },
      "outputs": [],
      "source": [
        "lines = data.apply(lambda row: row['title'] + ' ; ' + row['summary'].replace(\"\\n\", ' '), axis=1).tolist()\n",
        "\n",
        "sorted(lines, key=len)[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "7u97m5s8ekl5zd5a43a1yc",
        "id": "2rjAwyCXeCXM"
      },
      "source": [
        "Токенизируем данные с помощью метода WordPunctTokenizer из библиотеки NLTK. Обратите внимание, что для обучения языковой модели не нужно удалять стоп-слова и применять лемматизацию. Мы хотим, чтобы модель \"выучила\", как выглядят естественные тексты."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "lines = [' '.join(word_tokenize(l.lower())) for l in lines]\n",
        "\n",
        "sorted(lines, key=len)[:3]"
      ],
      "metadata": {
        "id": "bn_5JX4Q07WH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Построение и обучение модели"
      ],
      "metadata": {
        "id": "fPG8mR1XzFnI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "u68wydbiioqlp5gl96mhd",
        "id": "7YrNeimxeCXP"
      },
      "source": [
        "На первом этапе построения *n*-раммной языковой модели необходимо посчитать, сколько раз каждое слово встречалось после $(n - 1)$ предыдущих слов. Результатом должен быть словарь вида `{ tuple(prefix_tokens): {next_token_1: count_1, next_token_2: count_2}}`, где `prefix_tokens` — предшествующие токены.\n",
        "\n",
        "Так, при `n=3` для последовательности *differential contrastive divergence ;* получим:\n",
        "\n",
        "- `('differential', 'contrastive'): ({'divergence': 1})`,\n",
        "- `('contrastive', 'divergence'): ({';': 1})`\n",
        "\n",
        "Если количество предшествующих токенов меньше, чем (n - 1), требуется добавить паддинг `UNK` (unknown word):\n",
        "- первое слово в предложении (пустой префикс): \"\" -> `(UNK, UNK)`\n",
        "  - `('UNK','UNK'): ({'differential': 1})`\n",
        "- второе слово в предложении (короткий префикс): \"word\" -> `(UNK, word)`\n",
        "  - `('UNK', 'differential'): ({'contrastive': 1})`\n",
        "\n",
        "Также требуется добавить специальный токен `EOS` (end of sentence) в конце каждой последовательности\n",
        "- *this paper has been retracted .* -> `(this, paper, has, been, retracted, ., EOS)`\n",
        "  - `('retracted', '.'): Counter({'_EOS_': 1})`\n",
        "  - `('.', '_EOS_'): Counter({'_EOS_': 1})`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict, Counter, deque\n",
        "\n",
        "# `UNK` — отсутствующие токены, `EOS` — окончание последовательности\n",
        "UNK, EOS = \"_UNK_\", \"_EOS_\"\n",
        "n = 3\n",
        "\n",
        "counts = defaultdict(Counter)\n",
        "# counts[(word1, word2)][word3] = сколько раз слово word3 встретилось после слов (word1, word2)\n",
        "print(f'counts: {counts}')\n",
        "\n",
        "l = sorted(lines, key=len)[0]\n",
        "tok = l.split() # деление по пробелам\n",
        "tok.append(EOS) # добавление токена конца предложения\n",
        "print(f'tok: {tok}\\n')\n",
        "prefix = deque([UNK] * (n-1), maxlen=n-1) # двусторонняя очередь, поддерживает операции добавления и извлечения элементов, имеет фиксированную длину\n",
        "for t in tok:\n",
        "  counts[tuple(prefix)][t] += 1 # +1 вхождение слова word3 после (word1, word2)\n",
        "  print(f'prefix: {prefix}')\n",
        "  print(f't: {t}')\n",
        "  print(f'last key from counts: {list(counts.keys())[-1]}')\n",
        "  print(f'last value from counts: {list(counts.values())[-1]}\\n')\n",
        "  prefix.append(t) # меняем префикс\n",
        "counts[tuple(prefix)][t] += 1\n",
        "print(f'prefix: {prefix}')\n",
        "print(f't: {t}')\n",
        "print(f'last key from counts: {list(counts.keys())[-1]}')\n",
        "print(f'last value from counts: {list(counts.values())[-1]}\\n')\n",
        "print('counts:', *counts.items(), sep='\\n')"
      ],
      "metadata": {
        "id": "Uh3H1m_H9HFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Запишем алгоритм подсчета n-грамм в виде функции `count_ngrams`. Она должна принимать на вход последовательность строк с токенами, разделенными пробелами (`lines`), и значение `n`, а возвращать словарь `counts`. Протестируем работу функции."
      ],
      "metadata": {
        "id": "6yMNz0VcVY2s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellId": "og84gjipnumsakhiiu9ap",
        "id": "H1_iwsUUeCXP"
      },
      "outputs": [],
      "source": [
        "def count_ngrams(lines, n):\n",
        "\n",
        "    counts = defaultdict(Counter)\n",
        "\n",
        "    for l in lines:\n",
        "        tok = l.split()\n",
        "        tok.append(EOS)\n",
        "        prefix = deque([UNK] * (n-1), maxlen=n-1)\n",
        "        for t in tok:\n",
        "            counts[tuple(prefix)][t] += 1\n",
        "            prefix.append(t)\n",
        "        counts[tuple(prefix)][t] += 1\n",
        "\n",
        "    return counts"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "На 100 самых коротких текстах `example_lines` протестируем работу функции (`n=3`) и запишем результат в переменную `example_counts`."
      ],
      "metadata": {
        "id": "FhQC0RRg6uJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_lines = sorted(lines, key=len)[:100]\n",
        "example_counts = count_ngrams(example_lines, n=3)\n",
        "assert len(example_counts[('_UNK_', '_UNK_')]) == 78\n",
        "assert example_counts['_UNK_', 'a']['note'] == 3\n",
        "assert example_counts['author', '.']['_EOS_'] == 1"
      ],
      "metadata": {
        "id": "31H__FM96ZBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(example_counts.items())[:10]"
      ],
      "metadata": {
        "id": "JWMYNOd3XIxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "4j620npeqvj0k8ak8xqx8xk",
        "id": "rMKNYhLteCXQ"
      },
      "source": [
        "Как только мы научились считать $n$-граммы, можно построить вероятностную языковую модель. Самый простой способ вычисления вероятностей — пропорционально количеству:\n",
        "\n",
        "$$ P(w_t | prefix) = { count(prefix, w_t) \\over \\sum_{\\hat w} count(prefix, \\hat w) } $$\n",
        "\n",
        "Создадим класс `NGramLanguageModel`\n",
        "\n",
        "Конструктор класса `__init__`:\n",
        "- принимает на вход `n` (количество рассматриваемых токенов в контексте) и `lines` (последовательность строк с токенами, разделенными пробелами)\n",
        "- устанавливает атрибуты `n` и `probs` (словарь со структурой, аналогичной counts, но заполненный значениями вероятностей, а не количеством вхождений)\n",
        "- при создании словаря `probs` в числителе должно быть количество вхождений для текущего слова `[word]` после заданного префикса `[words]`, а знаменателе — сумма `sum()` возможных значений `.values()` после заданного префикса `[words]`\n",
        "\n",
        "Метод `get_possible_next_tokens`:\n",
        "- принимает на вход строку `prefix` с предшествующими (префиксными) токенами, разделенными пробелами\n",
        "- возвращает словарь, где ключи — токены, значения — их вероятности\n",
        "\n",
        "Метод `get_next_token_prob`:\n",
        "- принимает на вход строку `prefix` с предшествующими (префиксными) токенами, разделенными пробелами и следующий токен `next_token` для предсказания вероятности\n",
        "- возвращает вероятность `P(next_token|prefix)`, $0 \\le  P \\le 1$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellId": "c7cm76wmzlaa12bctznzei",
        "id": "UcYWGiNteCXQ"
      },
      "outputs": [],
      "source": [
        "class NGramLanguageModel:\n",
        "    def __init__(self, lines, n):\n",
        "\n",
        "        assert n >= 1\n",
        "        self.n = n\n",
        "\n",
        "        counts = count_ngrams(lines, self.n)\n",
        "\n",
        "        # probs[(word1, word2)][word3] = P(word3 | word1, word2)\n",
        "        self.probs = defaultdict(Counter)\n",
        "\n",
        "        # self.probs заполняется вероятностями в диапазоне от 0 до 1\n",
        "        for words in counts.keys():\n",
        "            for word in counts[words].keys():\n",
        "                self.probs[words][word] = counts[words][word]/sum(counts[words].values())\n",
        "\n",
        "    def get_possible_next_tokens(self, prefix):\n",
        "\n",
        "        prefix = prefix.split() # делим строку по пробелам\n",
        "\n",
        "        \"\"\"\n",
        "        Оставляем только (n-1) последних токенов.\n",
        "\n",
        "        Пусть n = 3:\n",
        "        prefix = [ w_0, w_1, w_2, w_3 ], len(prefix) = 4\n",
        "        len(prefix) - n + 1 = 4 - 3 + 1 = 2 --> оставляем токены начиная с w_2\n",
        "        prefix = [ w_0, w_1, w_2 ], len(prefix) = 3\n",
        "        len(prefix) - n + 1 = 3 - 3 + 1 = 1 --> оставляем токены начиная с w_1\n",
        "        prefix = [ w_0, w_1 ], len(prefix) = 2\n",
        "        len(prefix) - n + 1 = 2 - 3 + 1 = 0 --> оставляем токены начиная с w_0\n",
        "        prefix = [ w_0 ], len(prefix) = 1\n",
        "        len(prefix) - n + 1 = 1 - 3 + 1 = -1 --> оставляем также токены начиная с w_0\n",
        "        prefix = [ ], len(prefix) = 0\n",
        "        len(prefix) - n + 1 = 0 - 3 + 1 = -2 --> оставляем пустой список\n",
        "\n",
        "        Если len(prefix) > (n-1), то len(prefix) - n + 1 > 0 --> берется (n-1) токенов\n",
        "        Если len(prefix) <= (n-1), то len(prefix) - n + 1 <= 0 --> берутся все токены\n",
        "\n",
        "        Необходимо взять срез от prefix, нижняя граница — максимум из 0 и len(prefix) - n + 1\n",
        "        \"\"\"\n",
        "        prefix = prefix[max(0, len(prefix) - self.n + 1):]\n",
        "\n",
        "        \"\"\"\n",
        "        Добавляем паддинг при необходимости. На этом этапе уже осталось (n-1) токенов.\n",
        "\n",
        "        Пусть n = 3:\n",
        "        prefix = [], len(prefix) = 0\n",
        "        [ UNK ] * (n - 1 - len(prefix)) + prefix = [ UNK ] * (3 - 1 - 0) + [] = [ UNK , UNK ]\n",
        "        prefix = [ w_0 ], len(prefix) = 1\n",
        "        [ UNK ] * (n - 1 - len(prefix)) + prefix = [ UNK ] * (3 - 1 - 1) + [ w_0 ] = [ UNK , w_0 ]\n",
        "        prefix = [ w_0, w_1 ], len(prefix) = 2\n",
        "        [ UNK ] * (n - 1 - len(prefix)) + prefix = [ UNK ] * (3 - 1 - 2) + [ w_0, w_1 ] = [ w_0, w_1 ]\n",
        "        Если len(prefix) == 2, то 3 - 1 - 2 = 0 --> паддинг не добавляется\n",
        "\n",
        "        Необходимо умножить токен [ UNK ] на (n - 1 - len(prefix)) и прибавить префикс\n",
        "        \"\"\"\n",
        "        prefix = [ UNK ] * (self.n - 1 - len(prefix)) + prefix\n",
        "\n",
        "        return self.probs[tuple(prefix)]\n",
        "\n",
        "    def get_next_token_prob(self, prefix, next_token):\n",
        "        return self.get_possible_next_tokens(prefix).get(next_token, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "0ftnn4nmuzrup6c0vvhb8q",
        "id": "dJDSRmD4eCXQ"
      },
      "source": [
        "Создадим триграммную модель на основе 100 самых коротких текстов и протестируйте ее."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_lm = NGramLanguageModel(example_lines, n=3)\n",
        "assert example_lm.get_possible_next_tokens('')['a'] == 0.13\n",
        "assert example_lm.get_possible_next_tokens('')['using'] == 0.02\n",
        "assert round(sum(example_lm.get_possible_next_tokens('').values())) == 1"
      ],
      "metadata": {
        "id": "lVR50lAqfzwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_lm.get_possible_next_tokens('a')"
      ],
      "metadata": {
        "id": "Cu-1xq6HodNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert round(example_lm.get_next_token_prob('a', 'note'), 2) == 0.23\n",
        "assert round(example_lm.get_next_token_prob('a', 'study'), 2) == 0"
      ],
      "metadata": {
        "id": "F-22CxhupZcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_lm.get_next_token_prob('a', 'note')"
      ],
      "metadata": {
        "id": "dFkKJ6eVpq9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "oh8r9a41kuk4r51wra9",
        "id": "MRCDzpSNeCXR"
      },
      "source": [
        "Обучим языковую модель на всем наборе данных и посмотрим, какие последовательности она может генерировать."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellId": "f17xoejjppmooo2nopw4xo",
        "id": "POQdpQxGeCXR"
      },
      "outputs": [],
      "source": [
        "lm = NGramLanguageModel(lines, n=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Генерация осуществляется последовательно. Каждый следующий токен добавляется путем сэмплирования по вероятностям.\n",
        "\n",
        "$ X = [] $\n",
        "\n",
        "- $w_{next} \\sim P(w_{next} | X)$\n",
        "- $X = concat(X, w_{next})$\n",
        "\n",
        "Вместо выборки с вероятностями можно также попробовать всегда брать наиболее вероятный токен, выборку из $K$ наиболее вероятных токенов или выборку с температурой. При сэмплировании с температурой выполняется выборка из\n",
        "\n",
        "$$w_{next} \\sim {P(w_{next} | X) ^ {1 / \\tau} \\over \\sum_{\\hat w} P(\\hat w | X) ^ {1 / \\tau}}$$\n",
        "\n",
        "где $τ>0$ — температура модели.\n",
        "\n",
        "Функция `get_next_token` принимает на вход префикс и значение температуры, возвращает следующий токен после префикса.\n",
        "- если температура равна нулю:\n",
        "  - сортируем возможные продолжения последовательности по возрастанию\n",
        "  - берем последний элемент — наиболее вероятное продолжение\n",
        "- если температура больше нуля:\n",
        "  - создаем список `tokens` из всех возможных продолжений последовательности и список `probs` из их вероятностей\n",
        "  - значения вероятностей возводятся в степень $1 / \\tau$ с помощью `np.power`, при этом в числителе вероятность конкретного токена, в знаменателе — сумма вероятностей всех токенов"
      ],
      "metadata": {
        "id": "fQjj9JHz_nBM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellId": "sgbatlm9vzb4z889fho7",
        "id": "3z52JlyReCXR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_next_token(lm, prefix, temperature=1.0):\n",
        "\n",
        "    if temperature == 0.:\n",
        "        ptk = sorted (lm.get_possible_next_tokens(prefix), key = lm.get_possible_next_tokens(prefix).get)\n",
        "        return ptk[-1]\n",
        "    else:\n",
        "        tokens = list(lm.get_possible_next_tokens(prefix).keys())\n",
        "        probs = [ lm.get_next_token_prob(prefix, token) for token in tokens ]\n",
        "        probs = np.power(probs, 1.0/temperature) / sum(np.power(probs, 1.0/temperature))\n",
        "        return np.random.choice(np.array(tokens), p=probs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сгенерируем 10000 продолжений для префикса *there have* и проверим, какие будут самыми частотными."
      ],
      "metadata": {
        "id": "rlkMIdGft25h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "test_freqs = Counter([get_next_token(lm, 'there have') for _ in range(10000)])\n",
        "print(test_freqs)\n",
        "assert 250 < test_freqs['not'] < 450\n",
        "assert 8500 < test_freqs['been'] < 9500\n",
        "assert 1 < test_freqs['lately'] < 200"
      ],
      "metadata": {
        "id": "AJTiIY0zA86a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сгенерируем 10000 продолжений для префикса *deep* и протестируем, какое продолжение предложит модель при разных значениях температуры: 1.0, 0.5, 0.0."
      ],
      "metadata": {
        "id": "80gxi30Huhba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_freqs_t_1 = Counter([get_next_token(lm, 'deep', temperature=1.0) for _ in range(10000)])\n",
        "assert 1500 < test_freqs_t_1['learning'] < 3000\n",
        "print(f't=1.0: {test_freqs_t_1}')\n",
        "\n",
        "test_freqs_t_0_5 = Counter([get_next_token(lm, 'deep', temperature=0.5) for _ in range(10000)])\n",
        "assert 8000 < test_freqs_t_0_5['learning'] < 9000\n",
        "print(f't=0.5: {test_freqs_t_0_5}')\n",
        "\n",
        "test_freqs_t_0 = Counter([get_next_token(lm, 'deep', temperature=0.0) for _ in range(10000)])\n",
        "assert test_freqs_t_0['learning'] == 10000\n",
        "print(f't=0.0: {test_freqs_t_0}')"
      ],
      "metadata": {
        "id": "Fu9uVAjjtpCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "ux4n8iq523n4s3ftrelhxj",
        "id": "pwcsKFCheCXR"
      },
      "source": [
        "Протестируем, что сгенерирует модель при различных префиксах."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellId": "1nnnycga61rijt6nd8zai",
        "id": "WMv8UCvqeCXR"
      },
      "outputs": [],
      "source": [
        "prefix = 'artificial'\n",
        "\n",
        "for i in range(100):\n",
        "    prefix += ' ' + get_next_token(lm, prefix)\n",
        "    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n",
        "        break\n",
        "\n",
        "print(prefix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellId": "pxyjsv3b7r8thdfxlgitl",
        "id": "D9Z7-g4peCXR"
      },
      "outputs": [],
      "source": [
        "prefix = 'bridging the'\n",
        "\n",
        "for i in range(100):\n",
        "    prefix += ' ' + get_next_token(lm, prefix, temperature=0.5)\n",
        "    if prefix.endswith(EOS) or len(lm.get_possible_next_tokens(prefix)) == 0:\n",
        "        break\n",
        "\n",
        "print(prefix)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Оценка языковой модели"
      ],
      "metadata": {
        "id": "8P7pDDlDBmeI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для оценки языковых моделей используется перплексия. Она показывает, насколько хорошо модель аппроксимирует истинное распределение вероятностей, лежащее в основе данных. Чем меньше перплексия, тем лучше модель.\n",
        "\n",
        "Расчет перплексии для предложения:\n",
        "$$\n",
        "    {\\mathbb{P}}(w_1 \\dots w_N) = P(w_1, \\dots, w_N)^{-\\frac1N} = \\left( \\prod_t P(w_t \\mid w_{t - n}, \\dots, w_{t - 1})\\right)^{-\\frac1N},\n",
        "$$\n"
      ],
      "metadata": {
        "id": "ApJsIhBUBqe5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "На уровне корпуса, перплексия — это произведение вероятностей всех токенов во всех предложениях в степени $1/N$ , где $N$ — общая длина (в токенах) всех предложений в корпусе.\n",
        "\n",
        "Получившееся число может быстро стать слишком маленьким, поэтому сначала вычисляется логарифмическая перплексия (на основе логарифмических вероятностей), а затем берется экспонента."
      ],
      "metadata": {
        "id": "OmXEkHwZCC3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция `perplexity` принимает на вход список строк с токенами, разделенными пробелами (`lines`) и минимальную логарифмическую вероятность (`min_logprob`): если $log(P(w | ...))$ меньше, чем `min_logprop`, логариф вероятности приравнивается к `min_logprob`. Функция возвращает значение перплексии на уровне корпуса."
      ],
      "metadata": {
        "id": "bCDrW_sZChTc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellId": "5hp010xyzzb4vqewo1bhny",
        "id": "9Jv8M0U3eCXS"
      },
      "outputs": [],
      "source": [
        "def perplexity(lm, lines, min_logprob=np.log(10 ** -50.)):\n",
        "    perplexity = 0.0\n",
        "    n = 0\n",
        "    for line in lines:\n",
        "        l_line = line.split()\n",
        "\n",
        "        for k, tok_pref in enumerate(l_line):\n",
        "            # Пусть lm.n = 3, l_line = [ w_0, w_1, w_2, w_3, w_4]\n",
        "            if k == 0:\n",
        "                \"\"\"\n",
        "                Первый токен : префикс пуст\n",
        "                k = 0 --> вероятность w_0 в начале предложения\n",
        "                \"\"\"\n",
        "                perplexity += np.maximum(np.log(lm.get_next_token_prob('', tok_pref)), min_logprob)\n",
        "                n += 1\n",
        "            elif (k > 0) and (k < lm.n):\n",
        "                \"\"\"\n",
        "                k = 1 --> вероятность w_1 после [ w_0 ]\n",
        "                k = 2 --> вероятность w_2 после [ w_0, w_1 ]\n",
        "                \"\"\"\n",
        "                perplexity += np.maximum(np.log(lm.get_next_token_prob(' '.join(l_line[:k]), tok_pref)), min_logprob)\n",
        "                n += 1\n",
        "            else:\n",
        "                \"\"\"\n",
        "                k = 3 --> вероятность w_3 после [ w_1, w_2 ]\n",
        "                k = 4 --> вероятность w_4 после [ w_2, w_3 ]\n",
        "                \"\"\"\n",
        "                perplexity += np.maximum(np.log(lm.get_next_token_prob(' '.join(l_line[k-lm.n+1:k]), tok_pref)), min_logprob)\n",
        "                n += 1\n",
        "\n",
        "            \"\"\"\n",
        "            Для последнего токена в строке добавляется логарифмическая вероятность токена EOS\n",
        "            Вероятность EOS после [ w_3, w_4 ]\n",
        "            \"\"\"\n",
        "            if k == (len(l_line) - 1) :\n",
        "                perplexity += np.maximum(np.log(lm.get_next_token_prob(' '.join(l_line[k-lm.n+2:k+1]), EOS)), min_logprob)\n",
        "                n += 1\n",
        "    # делим сумму логарифмических вероятностей на количество токенов\n",
        "    perplexity = perplexity/n\n",
        "    return np.exp(-perplexity)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создадим языковые модели из предложений в `example_lines` при `n=1`, `n=3`, `n=10`. Посчитаем перплексию для них."
      ],
      "metadata": {
        "id": "k_I6WyJ_hkg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "lm1 = NGramLanguageModel(example_lines, n=1)\n",
        "lm3 = NGramLanguageModel(example_lines, n=3)\n",
        "lm10 = NGramLanguageModel(example_lines, n=10)\n",
        "\n",
        "ppx1 = perplexity(lm1, example_lines)\n",
        "ppx3 = perplexity(lm3, example_lines)\n",
        "ppx10 = perplexity(lm10, example_lines)\n",
        "\n",
        "print(f\"Perplexities: ppx1={round(ppx1,2)} ppx3={round(ppx3,2)} ppx10={round(ppx10,2)}\")\n",
        "\n",
        "assert all(0 < ppx < 500 for ppx in (ppx1, ppx3, ppx10))\n",
        "assert ppx1 > ppx3 > ppx10"
      ],
      "metadata": {
        "id": "gohbmm9Ob8zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cellId": "ypc4lks4vs1li908fqi8",
        "id": "CyZPxNi1eCXS"
      },
      "source": [
        "Разделим все данные на обучающую и тестовую выборку, оценим модель только на тестовых данных."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellId": "tjnehsem2lmijkg2lto4w",
        "id": "aEEjjTT2eCXS"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_lines, test_lines = train_test_split(lines, test_size=0.25, random_state=42)\n",
        "\n",
        "for n in (1, 2, 3):\n",
        "    lm = NGramLanguageModel(n=n, lines=train_lines)\n",
        "    ppx = perplexity(lm, test_lines)\n",
        "    print(f\"N = {n}, Perplexity = {round(ppx)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cглаживание"
      ],
      "metadata": {
        "id": "FQP98_fKD48R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проблема с нашей простой языковой моделью заключается в том, что всякий раз, когда она сталкивается с *n*-граммой, которую она никогда раньше не видела, она присваивает ей вероятность, равную 0. Каждый раз, когда это происходит, значение преплексии сильно возрастает.\n",
        "\n",
        "Реализуем аддитивное сглаживание или сглаживание Лапласа. Если значения для префикса невелики, сглаживание приведет к более равномерному распределению вероятностей.\n",
        "\n",
        "$$ P(w_t | prefix) = { Count(prefix, w_t) + \\delta \\over \\sum_{\\hat w} (Count(prefix, \\hat w) + \\delta) } $$"
      ],
      "metadata": {
        "id": "JAO4-1glD9Ls"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellId": "ioh26rlov6g8l2ssj1c8pm",
        "id": "duTdn6O4eCXT"
      },
      "outputs": [],
      "source": [
        "class LaplaceLanguageModel(NGramLanguageModel):\n",
        "    def __init__(self, lines, n, delta=1.0):\n",
        "        self.n = n\n",
        "        counts = count_ngrams(lines, self.n)\n",
        "        self.vocab = set(token for token_counts in counts.values() for token in token_counts)\n",
        "        self.probs = defaultdict(Counter)\n",
        "\n",
        "        for prefix in counts:\n",
        "            token_counts = counts[prefix]\n",
        "            total_count = sum(token_counts.values()) + delta * len(self.vocab)\n",
        "            self.probs[prefix] = {token: (token_counts[token] + delta) / total_count\n",
        "                                          for token in token_counts}\n",
        "    def get_possible_next_tokens(self, prefix):\n",
        "        token_probs = super().get_possible_next_tokens(prefix)\n",
        "        print(f'sum: {sum(token_probs.values())}')\n",
        "        print(f'len: {len(token_probs)}')\n",
        "        missing_prob_total = 1.0 - sum(token_probs.values())\n",
        "        missing_prob = missing_prob_total / max(1, len(self.vocab) - len(token_probs))\n",
        "        return {token: token_probs.get(token, missing_prob) for token in self.vocab}\n",
        "\n",
        "    def get_next_token_prob(self, prefix, next_token):\n",
        "        token_probs = super().get_possible_next_tokens(prefix)\n",
        "        if next_token in token_probs:\n",
        "            return token_probs[next_token]\n",
        "        else:\n",
        "            missing_prob_total = 1.0 - sum(token_probs.values())\n",
        "            missing_prob_total = max(0, missing_prob_total)\n",
        "            return missing_prob_total / max(1, len(self.vocab) - len(token_probs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellId": "j6zqa50koitjjri9ipd8ec",
        "id": "BTE-QrJweCXU"
      },
      "outputs": [],
      "source": [
        "for n in (1, 2, 3):\n",
        "    lm = LaplaceLanguageModel(train_lines, n=n, delta=0.1)\n",
        "    ppx = perplexity(lm, test_lines)\n",
        "    print(f\"N = {n}, Perplexity = {ppx}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Нейросетевые языковые модели"
      ],
      "metadata": {
        "id": "yXETL_wE70G1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мы рассмотрели, как можно построить счетные (*n*-граммные) языковые модели. Их основной недостаток в том, что они не способны учитывать длинный контекст предложения и генерировать связный текст.\n",
        "\n",
        "С этими проблемами справляются языковые модели на основе нейронных сетей. Мы рассмотрим архитектуру рекуррентных нейронных сетей и построим с помощью них языковую модель."
      ],
      "metadata": {
        "id": "-TyIhU5W76RE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Рекуррентные нейронные сети"
      ],
      "metadata": {
        "id": "bcA1N4Iz8t5b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Рекуррентные нейронные сети (recurrent neural networks, RNN)** — это класс нейронных сетей, которые применяются для обработки последовательных данных."
      ],
      "metadata": {
        "id": "oKEP6URL897M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Каждое слово $x_t$ в предложении последовательно обрабатывается с помощью **ячейки RNN**."
      ],
      "metadata": {
        "id": "S29ikHkIm7X2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src =\"https://i.ibb.co/f0hvFdv/RNN-cell.png\" width=\"850\"></center>"
      ],
      "metadata": {
        "id": "y7SKJNLMBc7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. На вход ячейки поступает входной вектор $x_t$ —  эмбеддинг слова с индексом $t$. Он имеет фиксированный размер $k$.\n",
        "\n",
        "2. Ячейка принимает еще один параметр $h_{t-1}$ — **скрытое состояние** или **память** (hidden state). Это вектор, хранящий информацию о предшествующем контексте. Он тоже имеет фиксированный размер $n$.\n",
        "\n",
        "3. Вектор $x_t$ умножается на матрицу $W^{nk}$ (размер $n \\times k$), которая содержит обучаемые веса: $W^{nk} \\cdot x_t$. Получаем новый вектор размера $n$.\n",
        "\n",
        "4. Вектор $h_{t-1}$ умножается на другую матрицу весов $W^{nn}$ (размер $n \\times n$): $ W^{nn} \\cdot h_{t-1} $. Получаем новый вектор размера $n$.\n",
        "\n",
        "5. Получившиеся векторы имеют одинаковый размер, их можно сложить: $W^{nk} \\cdot x_t + W^{nn} \\cdot h_{t-1}$.\n",
        "\n",
        "6. К получившемуся вектору применим функцию активации — гиперболический тангенс: $tanh(W^{nk} \\cdot x_t + W^{nn} \\cdot h_{t-1})$. Это и будет новым скрытым состоянием $h_t$. Оно зависит от предыдущего состояния $h_{t-1}$ и текущего элемента последовательности $x_t$.\n",
        "\n",
        "7. Рассчитанное скрытое состояние $h_t$ является представлением текущего слова $x_t$ с учетом предшествующего контекста и передается в качестве старого скрытого состояния для следующего слова $x_{t+1}$."
      ],
      "metadata": {
        "id": "_f-tGBizKKqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Воспользуемся реализацией ячейки [RNNCell](https://pytorch.org/docs/stable/generated/torch.nn.RNNCell.html) из pytorch."
      ],
      "metadata": {
        "id": "BSEmtO4jl9Rs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "x = torch.randn(5) # вектор 1 слова из 5 признаков\n",
        "print(f\"Входные данные: {x}\")\n",
        "print(f\"Размер входных данных: {x.shape}\")"
      ],
      "metadata": {
        "id": "AAgxpENQ9s-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "torch_rnn_cell = nn.RNNCell(input_size=5, hidden_size=3)\n",
        "h = torch_rnn_cell(x)\n",
        "print(f\"Выходные данные {h}\")\n",
        "print(f\"Размер выходных данных: {h.shape}\")"
      ],
      "metadata": {
        "id": "Oosh6SQ-k6MM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для обработки всей последовательности слов используется **слой RNN**, где ячейка применяется ко всем словам в цикле."
      ],
      "metadata": {
        "id": "Dfhx0soPnGgm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src =\"https://i.ibb.co/GdjZth0/RNN-layer.png\" width=\"700\"></center>"
      ],
      "metadata": {
        "id": "0f9bmrarBfgF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Когда первый токен $x_0$ подается в ячейку, вектор памяти $h$ инициализируется нулями.\n",
        "\n",
        "Вектор памяти $h_0$ для первого токена $x_0$ передается в следующую ячейку, обрабатывающую второй токен $x_1$. Вектор $h_0$ также является контекстуализированным представлением токена $x_0$ — $y_0$.\n",
        "\n",
        "Для каждого последующего токена вектор памяти учитывает контекст из предыдущих токенов.\n",
        "\n",
        "Вектор $y_t$ является не только контекстуализированным представлением для последнего токена $x_t$, но и вектором всего предложения, т.к. содержит информацию обо всех токенах.\n",
        "\n",
        "В зависимости от типа задачи, можно использовать контекстуализированные векторы для каждого слова или только вектор всего предложения (последнего слова)."
      ],
      "metadata": {
        "id": "q8vDA64YohLe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Воспользуемся реализацией слоя [RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) из pytorch."
      ],
      "metadata": {
        "id": "sTq5ZC6gB2h1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence = torch.randn(7, 5) # предложение из 7 токенов, у каждого 5 признаков\n",
        "print(f\"Входные данные:\\n{sequence}\")\n",
        "print(f\"Размер входных данных: {sequence.shape}\")"
      ],
      "metadata": {
        "id": "0j4jPrFj-Jit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnn = nn.RNN(input_size=5, hidden_size=3)\n",
        "out, h = rnn(sequence)\n",
        "\n",
        "print(f\"Выходные данные:\\n{out}\")\n",
        "print(f\"Размер выходных данных: {out.shape}\\n\")\n",
        "\n",
        "print(f\"Вектор предложения: {h}\")\n",
        "print(f\"Размер вектора предложения: {h.shape}\")"
      ],
      "metadata": {
        "id": "ZGRpFTyJAaFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Алгоритм обратного распространения ошибки сквозь время"
      ],
      "metadata": {
        "id": "hOAX39T6aF9C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучение RNN аналогично обучению обычной нейронной сети. Мы также используем алгоритм обратного распространения ошибки (backpropagation), но с небольшим изменением.\n",
        "\n",
        "Градиент на каждом выходе зависит не только от расчетов текущего шага, но и от предыдущих временных шагов. Вектор памяти для $h_3$ зависит от вектора памяти $h_2$, $h_1$ и $h_0$. Следовательно, чтобы вычислить градиент для $h_3$, нужно распространить ошибку на 3 шага и суммировать градиенты.\n",
        "\n",
        "Этот алгоритм называется «алгоритмом обратного распространения ошибки сквозь время» (Backpropagation Through Time, BPTT)."
      ],
      "metadata": {
        "id": "j90M1rdfaPoY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src =\"https://i.postimg.cc/RV29LpvM/bptt.png\" width=\"400\"></center>"
      ],
      "metadata": {
        "id": "NDCcIVfwmYaJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\\frac{\\partial L}{\\partial W} = \\sum\\limits_{i=0}^t\\frac{\\partial L_i}{\\partial W}$$\n",
        "$$\\frac{\\partial L_t}{\\partial W} = \\frac{\\partial L_t}{\\partial \\hat y} \\frac{\\partial \\hat y}{\\partial h_t} \\frac{\\partial h_t}{\\partial W} $$\n",
        "\n",
        "$$h_t = tanh(Vx_t + Wh_{t-1})$$\n",
        "\n",
        "$$\\frac{\\partial L_t}{\\partial W} = \\frac{\\partial L_t}{\\partial \\hat y_t} \\frac{\\partial \\hat y_t}{\\partial h_t} (\\frac{\\partial h_t}{\\partial W} + \\frac{\\partial h_t}{\\partial h_{t-1}} \\frac{\\partial h_{t-1}}{\\partial W} + \\cdots)  = \\frac{\\partial L_t}{\\partial \\hat y_t} \\frac{\\partial \\hat y_t}{\\partial h_t}\\sum\\limits_{k=0}^t (\\prod\\limits_{i = k+1}^t \\frac{\\partial h_i}{\\partial h_{i-1}}) \\frac{\\partial h_k}{\\partial W}$$"
      ],
      "metadata": {
        "id": "UaW4R7ROxxOK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\large \\| \\frac{\\partial h_i}{\\partial h_{i-1}} \\| \\lt 1 $ — затухающий градиент (vanishing gradient)\n",
        "\n",
        "$\\large \\| \\frac{\\partial h_i}{\\partial h_{i-1}} \\| \\gt 1 $ — взрывающийся градиент (exploding gradient)"
      ],
      "metadata": {
        "id": "jiPKRNCl2Ouq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Борьба с взрывающимся градиентом — **градиентное отсечение** (gradient clipping) — метод, который ограничивает максимально допустимое значение градиента, позволяя избежать градиентного взрыва."
      ],
      "metadata": {
        "id": "bBK8FUgjwbKi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src =\"https://i.postimg.cc/zvD4qK52/gradient-clipping.png\" width=\"600\"></center>"
      ],
      "metadata": {
        "id": "ru0n0Qj9mBgL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "От затухания и взрыва градиента может помочь **пропускание градиента по частям** (truncated gradient) на сколько-то шагов по времени назад, а не через всю нейросеть."
      ],
      "metadata": {
        "id": "7IbvCMAb49od"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src =\"https://i.postimg.cc/J7dps9Qz/truncated-gradient.png\" width=\"800\"></center>"
      ],
      "metadata": {
        "id": "ueZoyufYmQ1Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Минусы классических рекуррентных сетей:\n",
        "- информация о начальных словах затухает к концу предложения\n",
        "- затухающие и взрывающиеся градиенты\n"
      ],
      "metadata": {
        "id": "0OeSMKGGbsCg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Сети долгой краткосрочной памяти"
      ],
      "metadata": {
        "id": "w7i8MIQAaIit"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В обычной RNN в ячейке был только один путь передачи информации. На каждом шаге мы складывали информацию, накопленную с предыдущих шагов, с текущей. При этом информация о предыдущих токенах быстро затухает, и теряется общая информация о предложении.\n",
        "\n",
        "Эта проблема были решена в сети долгой краткосрочной памяти (Long short-term memory, LSTM). Структура ячейки LSTM намного сложнее. Здесь есть целых 4 линейных слоя, каждый из которых выполняет разные задачи.\n",
        "\n",
        "Главное нововведение: в LSTM добавлено состояние $c_t$, которое должно сохранять общий контекст. Помогает нейросети сохранять важную информацию, встретившуюся в какой-то момент в предложении, все время, пока эта информация требуется."
      ],
      "metadata": {
        "id": "D0bL_DlP4gzH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src =\"https://i.ibb.co/hfxC7SF/RNN-LSTM.png\" width=\"650\"></center>"
      ],
      "metadata": {
        "id": "L2s6QzxKBioP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Рассмотрим подробно структуру ячейки LSTM."
      ],
      "metadata": {
        "id": "G9_NaJrBlAy-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src =\"https://i.ibb.co/tDmV7X3/LSTM1.png\" width=\"700\"></center>\n",
        "<center><img src =\"https://i.ibb.co/b3H19Kp/LSTM2.png\" width=\"500\"></center>"
      ],
      "metadata": {
        "id": "MWC57NMNBjeI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$h_{t-1}$ — вектор краткосрочной памяти для предшествующего контекста;\n",
        "\n",
        "$c_{t-1}$ — вектор долгой памяти для предшествующего контекста;\n",
        "\n",
        "$x_t$ — вектор текущего токена;\n",
        "\n",
        "$h_t$ — вектор краткосрочной памяти для контекста с учетом текущего токена;\n",
        "\n",
        "$c_t$ — вектор долгой памяти для контекста с учетом текущего токена;\n",
        "\n",
        "$W_f, W_i, W_c, W_o$ — матрицы параметров;\n",
        "\n",
        "$σ,\\;tanh$ — функции активации."
      ],
      "metadata": {
        "id": "KwdmU1udjcjt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$ f_t = σ(W_f \\cdot [h_{t-1}, x_t])$ — вентиль забывания (forget gate), вес забывания старой информации."
      ],
      "metadata": {
        "id": "SMHNf3gClRBs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src =\"https://i.ibb.co/7tsb68R/LSTM3.png\" width=\"350\"></center>"
      ],
      "metadata": {
        "id": "GkbfQ_tjBlpm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$ i_t = σ(W_i \\cdot [h_{t-1}, x_t])$ — входной вентиль (input gate), вес запоминания новой информации;\n",
        "\n",
        "$ \\tilde c_t = \\tanh(W_c \\cdot [h_{t-1}, x_t])$ — предварительный вектор долгой памяти (candidate cell state), содержание новой информации."
      ],
      "metadata": {
        "id": "i46CEXt6nWjN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src =\"https://i.postimg.cc/5NV9vVg6/LSTM4.png\" width=\"350\"></center>"
      ],
      "metadata": {
        "id": "43HRU2XGBmrn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$ c_t = f_t\\otimes c_{t-1} + i_t \\otimes \\tilde c_t$ — обновленный вектор долгой памяти."
      ],
      "metadata": {
        "id": "PnKMEFi7oowO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src =\"https://i.ibb.co/6ZsZgp9/LSTM5.png\" width=\"350\"></center>"
      ],
      "metadata": {
        "id": "GJGiVxCwBn0G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$ o_t = σ(W_o \\cdot [h_{t-1}, x_t])$ — выходной вентиль для текущего токена (output gate);\n",
        "\n",
        "$ h_t = o_t\\otimes \\tanh(c_t)$ — обновленный вектор краткосрочной памяти."
      ],
      "metadata": {
        "id": "0jTxhs69pm2-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src =\"https://i.ibb.co/F7DX5T3/LSTM6.png\" width=\"350\"></center>"
      ],
      "metadata": {
        "id": "5rPpuqFIBotn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Воспользуемся реализацией слоя [LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) из pytorch."
      ],
      "metadata": {
        "id": "AbneGruR5Qfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence = torch.randn(4, 6) # предложение из 4 токенов, у каждого 6 признаков\n",
        "print(f\"Входные данные:\\n{sequence}\")\n",
        "print(f\"Размер входных данных: {sequence.shape}\")"
      ],
      "metadata": {
        "id": "R244dCX_5aFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm = nn.LSTM(input_size=6, hidden_size=5)\n",
        "out, (h, c) = lstm(sequence)\n",
        "\n",
        "print(f\"Выходные данные (краткосрочная память):\\n{out}\")\n",
        "print(f\"Размер выходных данных: {out.shape}\\n\")\n",
        "\n",
        "print(f\"Вектор краткосрочной памяти всего предложения:\\n{h}\")\n",
        "print(f\"Размер вектора краткосрочной памяти: {h.shape}\\n\")\n",
        "\n",
        "print(f\"Вектор долгой памяти всего предложения:\\n{c}\")\n",
        "print(f\"Размер вектора долгой памяти: {c.shape}\")"
      ],
      "metadata": {
        "id": "_aJseDacuBiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Типы задач"
      ],
      "metadata": {
        "id": "M_dqIxd17pdl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Один к одному: на вход подается один элемент, на выходе получаем вероятность класса — классификация изображений.\n",
        "\n",
        "- Один ко многим: на вход приходит один элемент, а на выходе получаем целую последовательность — генерация текста по слову, текстовой подписи по изображению.\n",
        "\n",
        "- Многие к одному: на вход сети подается последовательность, а в качестве выхода получаем один вектор вероятностей для классов — классификация текстов (анализ тональности, определение языка).\n",
        "\n",
        "- Многие ко многим: преобразовать последовательность в последовательность\n",
        "  - количество выходов равно количеству входов — тегирование (именованные сущности, части речи).\n",
        "  - количество выходов сети не обязательно равно количеству входов — машинный перевод, суммаризация."
      ],
      "metadata": {
        "id": "VYc2i7aY1P8b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src =\"https://i.postimg.cc/SRp4W6Ww/RNN-tasks.png\" width=\"800\"></center>"
      ],
      "metadata": {
        "id": "MDTlNYa1BpbW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Применение для языкового моделирования"
      ],
      "metadata": {
        "id": "vFyy1_dK-J4h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мы рассмотрим языковые модели на основе рекуррентных нейронных сетей. Они должны хорошо подходить для этой задачи, поскольку они проходят по последовательности и запоминают порядок элементов."
      ],
      "metadata": {
        "id": "6m9p0zgufxN3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src =\"https://i.ibb.co/RN1mHF6/LSTM-lm.png\" width=\"700\"></center>"
      ],
      "metadata": {
        "id": "XcN4HDa8BqON"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Embedding\n",
        "\n",
        "Есть входная последовательность из $n$ элементов. Мы пропускаем ее через слой эмбеддингов, который каждому элементу последовательности сопоставляет векторное представление.\n",
        "\n",
        "- LSTM1, LSTM2\n",
        "\n",
        "На вход ячейки LSTM поступает вектор заданной длины. Скрытое состояние (краткосрочная память, hidden state) элемента $w_i$ передается на следующую ячейку того же слоя вместе с элементом $w_{i+1}$. На рисунке представлено 2 слоя LSTM. Следовательно, скрытое состояние элемента $w_i$ на слое LSTM1 передается также на следующий слой LSTM2.\n",
        "\n",
        "- Softmax\n",
        "\n",
        "Выход каждого слоя — контекстуализированный эмбеддинг элемента последовательности. Однако в задаче языкового моделирования мы должны получать распределение  вероятностей для следующего элемента при условии текущей последовательности. Это должен быть вектор вероятностей, длина которого равна количеству элементов в словаре. Необходимо пропустить векторы элементов через линейный слой и применить функцию активации softmax.\n",
        "\n",
        "Для элемента $w_i$ предсказывается вероятность элемента $w_{i+1}$ с учетом предшествующей последовательности $w_1, \\cdots, w_i$ (по прошлому предсказываем будущее)."
      ],
      "metadata": {
        "id": "2oAFuX-5neE7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "После предсказания вероятностей необходимо оценить качество и посчитать размер ошибки. В качестве правильных ответов мы используем те же обучающие данные, но сдвинутые на 1 шаг. Это достигается за счет добавления тегов начала START и конца END последовательности.\n",
        "\n",
        "Для элемента START правильным ответом будет элемент R, для R — I, для I — G, и т.д."
      ],
      "metadata": {
        "id": "ZCIMlC7E3Isp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src =\"https://i.ibb.co/KGZ554x/rnn-generation.png\" width=\"600\"></center>"
      ],
      "metadata": {
        "id": "dvNzXUWqBI4P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "📌 В чем особенность обучающих данных для языкового моделирования по сравнению с задачей классификации?"
      ],
      "metadata": {
        "id": "NxkhwZEf3G6J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "После обучения рекуррентной сети для задачи языкового моделирования, она способна генерировать текст. Чем больше объем обучающих данных, тем более естественным будет сгенерированный текст.\n",
        "\n",
        "Этап генерации (inference) можно сравнить с тестированием при классификации. Если при обучении всегда есть правильные ответы, то при генерации их нет. Мы подаем какой-то символ (например, начало строки), по нему модель предсказывает следующий символ. Далее этот предсказанный символ снова подается на вход, по нему модель предсказывает следующий.\n",
        "\n",
        "Модель каждый раз вызывается заново, поэтому скрытые состояния не передаются автоматически. Их нужно сохранять и передавать в модель вручную."
      ],
      "metadata": {
        "id": "7H_huxNQ9gmw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "📌 Когда останавливается процесс генерации?"
      ],
      "metadata": {
        "id": "Ot3ZnTceQYdG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Если все время подавать модели на вход символ начала строки, то она будет предсказывать одну и ту же наиболее вероятную последовательность. Чтобы разнообразить генерацию, элементы последовательности берутся в соответствии с установленным распределением вероятностей."
      ],
      "metadata": {
        "id": "yCEO-u7a75p0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Генерация названий динозавров с помощью LSTM"
      ],
      "metadata": {
        "id": "pV5XiAsC-q02"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мы обучим языковую модель на уровне символов.\n",
        "\n",
        "В качестве обучающих данных будем использовать [корпус названий динозавров](https://www.kaggle.com/datasets/swimmingwhale/dinosaur-island).\n",
        "\n",
        "После обучения эта модель будет способна порождать новые имена динозавров, похожие на существующие."
      ],
      "metadata": {
        "id": "i1DMm1oK6fhI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src =\"https://i.ibb.co/TrtMyVH/dinos.png\" width=\"800\"></center>"
      ],
      "metadata": {
        "id": "pkizajOSBruG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Загрузка и подготовка данных"
      ],
      "metadata": {
        "id": "RzMYIDrIE5zX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузим данных и посмотрим на содержание файла."
      ],
      "metadata": {
        "id": "1YaA-J8T3cbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/Xeanst/NLP_course_FBB/main/data/dinos.txt"
      ],
      "metadata": {
        "id": "2XO_DoIJ9OrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head dinos.txt"
      ],
      "metadata": {
        "id": "xEiUgcLk94jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tail dinos.txt"
      ],
      "metadata": {
        "id": "gGZZ6HmA11aJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чтобы дальнейший код воспроизводился, зафиксируем случайные состояния:"
      ],
      "metadata": {
        "id": "kmP24OODzaa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def set_random_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "\n",
        "set_random_seed(42)"
      ],
      "metadata": {
        "id": "-cr7GKzPzZw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мы бы хотели иметь возможность обучать модель на аппаратном ускорителе, таком как GPU, если он доступен. Проверим, доступен ли нам ускоритель `torch.cuda`, иначе продолжим вычисления на CPU."
      ],
      "metadata": {
        "id": "dKjKGHAihFAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "id": "QZDIRZOXm-zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создадим класс `DinoDataset`, наследник класса `Dataset`."
      ],
      "metadata": {
        "id": "NsYNA29I3JbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class DinosDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    with open('dinos.txt') as f:\n",
        "      content = f.read().lower()\n",
        "      self.vocab = sorted(set(content)) + ['<','>'] # добавляем в словарь все буквы, а также спецсимволы начала и конца\n",
        "      self.vocab_size = len(self.vocab) # определяем размер словаря\n",
        "      self.lines = content.splitlines() # разбиваем по строкам\n",
        "    self.char2id = {char:id for id,char in enumerate(self.vocab)} # создаем словарь, каждому символу присваиваем индекс\n",
        "    self.id2char = {id:char for id,char in enumerate(self.vocab)}\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    line = self.lines[index]\n",
        "    \"\"\"\n",
        "    Входные данные x_str: символ начала + последовательность\n",
        "    Выходные данные y_str: последовательность + символ конца\n",
        "    \"\"\"\n",
        "    x_str = '<' + line\n",
        "    y_str = line + '>'\n",
        "    x = torch.empty(len(x_str), dtype=torch.long, device=device) # создаем пустой тензор для входных данных\n",
        "    y = torch.empty(len(y_str), dtype=torch.long, device=device) # создаем пустой тензор для выходных данных\n",
        "    for i, (x_ch, y_ch) in enumerate(zip(x_str, y_str)): # переводим символы в индексы по словарю char2id\n",
        "      x[i] = self.char2id[x_ch]\n",
        "      y[i] = self.char2id[y_ch]\n",
        "    return x,y\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.lines) # определяем размер датасета"
      ],
      "metadata": {
        "id": "uZF-rqVR-aBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dinos_dataset = DinosDataset()\n",
        "dinos_dataloader = DataLoader(dinos_dataset, shuffle=True)"
      ],
      "metadata": {
        "id": "2IWIlztUBc_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Убедимся, что входные и выходные данные различаются сдвигом на один шаг."
      ],
      "metadata": {
        "id": "CceJSrp14Rr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x,y = next(iter(dinos_dataloader))\n",
        "print(x.shape)\n",
        "print(x)\n",
        "print([dinos_dataset.id2char[int(i)] for i in x[0]])\n",
        "print(y.shape)\n",
        "print(y)\n",
        "print([dinos_dataset.id2char[int(i)] for i in y[0]])"
      ],
      "metadata": {
        "id": "eL7kwnSSDxLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотрим на размер обучающих данных и количество уникальных символов."
      ],
      "metadata": {
        "id": "iIJSgkgp4Ik8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dinos_dataset.lines))\n",
        "print(dinos_dataset.vocab_size)"
      ],
      "metadata": {
        "id": "lJ44a9KJN3G_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Создание модели"
      ],
      "metadata": {
        "id": "wzhyLlNeE8pe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Перейдем к построению модели. Она включает эмбеддинг слой, два слоя LSTM и линейный слой."
      ],
      "metadata": {
        "id": "nWFop0PT4weA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class LM(nn.Module):\n",
        "  def __init__(self, vocab_size):\n",
        "    super(LM, self).__init__()\n",
        "    self.lstm_size = 15 # размер скрытых состояний h и c (краткосрочная и долгая память)\n",
        "    self.embedding_dim = 10 # размер входных данных (длина эмбеддингов)\n",
        "    self.num_layers = 2 # количество слоев LSTM\n",
        "\n",
        "    # слой эмбеддингов\n",
        "    self.embedding = nn.Embedding(\n",
        "        num_embeddings=vocab_size,\n",
        "        embedding_dim=self.embedding_dim\n",
        "        )\n",
        "    # слой LSTM\n",
        "    self.lstm = nn.LSTM(\n",
        "        input_size=self.embedding_dim,\n",
        "        hidden_size=self.lstm_size,\n",
        "        num_layers=self.num_layers\n",
        "    )\n",
        "    # линейный слой\n",
        "    self.hid2out = nn.Linear(\n",
        "        in_features=self.lstm_size,\n",
        "        out_features=vocab_size\n",
        "        )\n",
        "\n",
        "  def forward(self, x, prev_state=None):\n",
        "    embedding = self.embedding(x)\n",
        "    if prev_state:\n",
        "      output, state = self.lstm(embedding)\n",
        "    else:\n",
        "      output, state = self.lstm(embedding, prev_state)\n",
        "    logits = self.hid2out(output)\n",
        "\n",
        "    return logits, state"
      ],
      "metadata": {
        "id": "fD2pXvj9pE3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LM(len(dinos_dataset.char2id)).to(device)"
      ],
      "metadata": {
        "id": "VlPKO7HJJeFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Применим модель к одному батчу и посмотрим размер получившихся данных.\n",
        "\n",
        "📌 Почему он именно такой?"
      ],
      "metadata": {
        "id": "Af5E7qG86MiG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred, (state_h, state_c) = model(x)\n",
        "print(y_pred.shape)\n",
        "print(state_h.shape)\n",
        "print(state_c.shape)"
      ],
      "metadata": {
        "id": "CfDzSSbRKHnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Случайная выборка из элементов массива `np.random.choice`"
      ],
      "metadata": {
        "id": "fKlrITPX7Vll"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Метод `np.random.choice` выбирает из списка элемент случайным образом. Если мы не задаем распределение вероятностей, то каждый элемент будет выбран примерно одинаковое количество раз."
      ],
      "metadata": {
        "id": "3ghi1CrN8MVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "for i in range(20):\n",
        "  print(np.random.choice([1,2,3]))"
      ],
      "metadata": {
        "id": "ef4BNen7e3FJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Распределение вероятностей может быть задано эксплицитно. Если это ohe-hot вектор, то всегда будет выбираться номер позиции, на которой стоит 1."
      ],
      "metadata": {
        "id": "oFgvM1er8qRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "p1 = torch.nn.functional.one_hot(torch.tensor(2), 29)\n",
        "print(p1)"
      ],
      "metadata": {
        "id": "MRFhy-J7fIPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(20):\n",
        "  pred_id = np.random.choice(np.arange(len(dinos_dataset.char2id)), p=p1)\n",
        "  print(pred_id)"
      ],
      "metadata": {
        "id": "Kha53Ke-f1qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Можно использовать распределение вероятностей, которое было получено при обучении сети. Для этого необходимо к выходным данным применить функцию активации softmax."
      ],
      "metadata": {
        "id": "mVV8jHtl9DOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logits = y_pred[:, -1, :].unsqueeze(1)\n",
        "print(logits.shape)\n",
        "print(logits)\n",
        "y_softmax_scores = torch.softmax(logits, dim=2)\n",
        "print(y_softmax_scores.shape)\n",
        "print(y_softmax_scores)\n",
        "torch.sum(y_softmax_scores)"
      ],
      "metadata": {
        "id": "HN_hKFq2dnbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p2 = y_softmax_scores.detach().cpu().numpy().ravel()\n",
        "print(p2)"
      ],
      "metadata": {
        "id": "gVUCR9A0eKI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(20):\n",
        "  pred_id = np.random.choice(np.arange(len(dinos_dataset.char2id)), p=p2)\n",
        "  print(pred_id)"
      ],
      "metadata": {
        "id": "HZsqarh0eeZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Тестирование модели"
      ],
      "metadata": {
        "id": "_P4V9PSTQ4Je"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Напишем функцию для генерации названий динозавров."
      ],
      "metadata": {
        "id": "6qbikrOB-OrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(dataset, model):\n",
        "  model.eval() # переводим модель в состояние тестирования\n",
        "  newline_id = dataset.char2id['>'] # записываем индекс символа конца последовательности\n",
        "  word_size = 0 # будем контролировать длину порождаемой последовательности\n",
        "  with torch.no_grad():\n",
        "    state_h, state_c = (None, None) # скрытые состояния будут передаваться вручную, поэтому их надо хранить\n",
        "    start_id = dataset.char2id['<'] # генерация начинается с символа начала последовательности\n",
        "    indices = [start_id] # создаем список, где будем хранить предсказания\n",
        "    word_size += 1 # увеличиваем длину последовательности\n",
        "    pred_id = start_id # записываем символ начала последовательности как первое предсказание\n",
        "    x = torch.tensor([[pred_id]]).to(device) # преобразуем в тензор\n",
        "\n",
        "    \"\"\"\n",
        "    Будем использовать два условия для продолжения генерации в цикле while:\n",
        "    1) сгенерированный символ не является символом конца последовательности '>'\n",
        "    и\n",
        "    2) длина сгенерированной последовательности меньше 20\n",
        "    \"\"\"\n",
        "    while pred_id != newline_id and word_size < 20:\n",
        "      logits, (state_h, state_c) = model(x, (state_h, state_c)) # передаем в модель тензор с текущим символом x, предыдущие состояния h и c\n",
        "      y_softmax_scores = torch.softmax(logits, dim=2)\n",
        "      pred_id = np.random.choice( # осуществляем случайную выборку значений из заданного массива\n",
        "          np.arange(len(dinos_dataset.char2id)), # с некоторой вероятностью получим один из 29 индексов\n",
        "          p=y_softmax_scores.detach().cpu().numpy().ravel() # вероятность определяется в зависимости от обучающих данных\n",
        "          )\n",
        "      indices.append(pred_id) # добавляем предсказанный индекс в список предсказаний\n",
        "      x = torch.tensor([[pred_id]]).to(device) # преобразуем в тензор\n",
        "      word_size += 1 # увеличиваем длину последовательности\n",
        "\n",
        "      if word_size == 20 and indices[-1] != newline_id:\n",
        "        indices.append(newline_id)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    return ''.join([dinos_dataset.id2char[i] for i in indices]) # возвращаем индексы, переведенные в символы"
      ],
      "metadata": {
        "id": "RBU6fhxQQ7UF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Обучение модели"
      ],
      "metadata": {
        "id": "x7HutTATh0TP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataset, dataloader, model, criterion, optimizer, max_epochs):\n",
        "  model.train() # переводим модель в состояние обучения\n",
        "  losses = []\n",
        "  for epoch in range(max_epochs):\n",
        "    print(f'\\nEpoch {epoch+1}')\n",
        "    epoch_loss = 0\n",
        "    for batch, (x,y) in enumerate(dataloader):\n",
        "      optimizer.zero_grad()\n",
        "      y_pred, (state_h, state_c) = model(x) # передаем данные в модель, записываем предсказание\n",
        "      loss = criterion(y_pred.transpose(1,2), y) # считаем ошибку\n",
        "      epoch_loss += loss.item()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      if (batch+1) % 100 == 0:\n",
        "        print(inference(dataset, model))\n",
        "\n",
        "    print(f'Loss {epoch_loss/(batch+1)}')\n",
        "    losses.append(epoch_loss/(batch+1))\n",
        "  return losses"
      ],
      "metadata": {
        "id": "zymkKTtSiF3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "\n",
        "model = LM(len(dinos_dataset.char2id)).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "max_epochs = 15"
      ],
      "metadata": {
        "id": "i7N7shjWi3Fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = train(dinos_dataset,dinos_dataloader, model, criterion, optimizer, max_epochs)"
      ],
      "metadata": {
        "id": "j0gsVIQVngqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(losses)\n",
        "plt.title('Cross Entropy Loss value')\n",
        "plt.ylabel('Cross Entropy Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uc0Xa1PNrJOC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "notebookId": "53997d2d-afb8-4477-8874-b6d46299f06c",
    "notebookPath": "seminar.ipynb",
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}