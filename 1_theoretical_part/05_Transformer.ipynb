{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–†–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω—ã–µ —Å–µ—Ç–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏–º–µ—é—Ç —Ä—è–¥ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–æ–≤:\n",
    "- –Ω–µ —Å–ø–æ—Å–æ–±–Ω—ã –∑–∞–ø–æ–º–∏–Ω–∞—Ç—å –¥–∏—Å—Ç–∞–Ω—Ç–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏,\n",
    "- –ø–æ–¥–≤–µ—Ä–∂–µ–Ω—ã –ø—Ä–æ–±–ª–µ–º–∞–º –≤–∑—Ä—ã–≤–∞ –∏ –∑–∞—Ç—É—Ö–∞–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤,\n",
    "- –≤—ã–ø–æ–ª–Ω—è—é—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, —á—Ç–æ –∑–∞–Ω–∏–º–∞–µ—Ç –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏.\n",
    "\n",
    "–í –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–µ sequence-to-sequence –¥–ª—è –±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –±—ã–ª –¥–æ–±–∞–≤–ª–µ–Ω –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è, –æ–¥–Ω–∞–∫–æ —ç—Ç–æ –Ω–µ —Ä–µ—à–∞–µ—Ç –≤—Å–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –ø—Ä–æ–±–ª–µ–º—ã.\n",
    "\n",
    "–ê —á—Ç–æ –µ—Å–ª–∏ –æ—Ç–∫–∞–∑–∞—Ç—å—Å—è –æ—Ç —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω–∏–º–∞–Ω–∏—è?\n",
    "–ü–æ–¥–æ–±–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø–æ–ª—É—á–∏–ª–∞ –Ω–∞–∑–≤–∞–Ω–∏–µ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä.\n",
    "\n",
    "[[paper] üéì Vaswani A. et al. (2017).Attention Is All You Need](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö–∞–∫ –∏ –≤ —Å–ª—É—á–∞–µ —Å —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ–π —Å–µ—Ç—å—é seq2seq, –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –±–ª–æ–∫–∞ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞ –∏ –±–ª–æ–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞.\n",
    "- –ö–æ–¥–∏—Ä–æ–≤—â–∏–∫ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∏—Å—Ö–æ–¥–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –∫–æ–¥–∏—Ä—É–µ—Ç –µ–µ.\n",
    "- –î–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ü–µ–ª–µ–≤—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å —É—á–µ—Ç–æ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞. –í—ã—Ö–æ–¥ –∏–∑ –¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞ —è–≤–ª—è–µ—Ç—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "–ë–ª–æ–∫ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ 6 –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–æ–≤, —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–Ω—ã—Ö –¥—Ä—É–≥ –∑–∞ –¥—Ä—É–≥–æ–º. –ë–ª–æ–∫ –¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞ ‚Äì —ç—Ç–æ —Å—Ç–µ–∫ –¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–æ–≤, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –≤ —Ç–æ–º –∂–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L05/transformer.jpg\" width=\"860\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í—Å–µ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∏ –∏–¥–µ–Ω—Ç–∏—á–Ω—ã –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ, —Ö–æ—Ç—å –∏ –∏–º–µ—é—Ç —Ä–∞–∑–Ω—ã–µ –≤–µ—Å–∞. –ö–∞–∂–¥—ã–π –º–æ–∂–Ω–æ —Ä–∞–∑–¥–µ–ª–∏—Ç—å –Ω–∞ –¥–≤–∞ –ø–æ–¥—Å–ª–æ—è.\n",
    "- –í—Ö–æ–¥–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å, –ø–æ—Å—Ç—É–ø–∞—é—â–∞—è –≤ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫, —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ—Ö–æ–¥–∏—Ç —á–µ—Ä–µ–∑ —Å–ª–æ–π –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è (self-attention), –ø–æ–º–æ–≥–∞—é—â–∏–π –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫—É –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ –¥—Ä—É–≥–∏–µ —Å–ª–æ–≤–∞ –≤–æ –≤—Ö–æ–¥–Ω–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏ –≤–æ –≤—Ä–µ–º—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å–ª–æ–≤–∞.\n",
    "- –í—ã—Ö–æ–¥ —Å–ª–æ—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è –≤ –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å –ø—Ä—è–º–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è (feed-forward neural network).\n",
    "\n",
    "–î–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫ —Ç–∞–∫–∂–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —ç—Ç–∏ –¥–≤–∞ —Å–ª–æ—è, –Ω–æ –º–µ–∂–¥—É –Ω–∏–º–∏ –µ—Å—Ç—å —Å–ª–æ–π –≤–Ω–∏–º–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–æ–≥–∞–µ—Ç –¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫—É —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞—Å—Ç—è—Ö –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ –º–µ—Ö–∞–Ω–∏–∑–º—É –≤–Ω–∏–º–∞–Ω–∏—è –≤ –º–æ–¥–µ–ª—è—Ö seq2seq)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L05/enc_dec_tr.png\" width=\"650\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö–∞–∫ –∏ –≤ —Å–ª—É—á–∞–µ –¥—Ä—É–≥–∏—Ö –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤, –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –≤—Ö–æ–¥–Ω–æ–π –∏ —Ü–µ–ª–µ–≤–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç—Å—è –≤ –≤–µ–∫—Ç–æ—Ä.\n",
    "\n",
    "–ü–æ—Å–ª–µ —Ç–æ–≥–æ –∫–∞–∫ —Å–ª–æ–≤–∞ –≤—Ö–æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–ª–∏—Å—å –≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏, –∫–∞–∂–¥—ã–π –∏–∑ –Ω–∏—Ö –ø–æ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏ –ø—Ä–æ—Ö–æ–¥–∏—Ç —á–µ—Ä–µ–∑ —Å–ª–æ–∏ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞. –û–¥–Ω–∞ –∏–∑ –æ—Å–Ω–æ–≤–Ω—ã—Ö –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –∏–¥–µ—Ç –ø–æ —Å–≤–æ–µ–π —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–π —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L05/encoder.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú—ã –ø–æ–ø—Ä–æ–±—É–µ–º —Å–æ–∑–¥–∞—Ç—å –∏ –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä, –æ—Å–Ω–æ–≤—ã–≤–∞—è—Å—å –Ω–∞ —Ç—å—é—Ç–æ—Ä–∏–∞–ª–µ:\n",
    "\n",
    "[[doc] üõ†Ô∏è The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)\n",
    "\n",
    "**–î–∏—Å–∫–ª–µ–π–º–µ—Ä:** –æ—Ç –≤–∞—Å –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è –ø–æ–ª–Ω–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ –∫–æ–¥–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏ –æ—Å–æ–±–µ–Ω–Ω–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏. –û—Å–Ω–æ–≤–Ω–∞—è —Ü–µ–ª—å ‚Äì –ø–æ–∫–∞–∑–∞—Ç—å, —á—Ç–æ –æ–±—É—á–∏—Ç—å —Å–≤–æ–π –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –Ω–µ –æ—á–µ–Ω—å –ª–µ–≥–∫–æ –∏ –æ—á–µ–Ω—å –¥–æ–ª–≥–æ, —á—Ç–æ –æ–±—ä—è—Å–Ω—è–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å –≤ –¥–∞–ª—å–Ω–µ–π—à–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—É—Å—Ç—å –º—ã —Ö–æ—Ç–∏–º –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ: *The animal didn't cross the street because it was too tired*. –ú–µ—Å—Ç–æ–∏–º–µ–Ω–∏–µ *it* –º–æ–∂–µ—Ç –æ—Ç–Ω–æ—Å–∏—Ç—å—Å—è –∫ —É–ª–∏—Ü–µ (*street*) –∏–ª–∏ –∫ –∂–∏–≤–æ—Ç–Ω–æ–º—É (*animal*). –ö–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–ª–æ–≤–æ *it*, —Å–ª–æ–π –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –ø–æ–º–æ–≥–∞–µ—Ç –ø–æ–Ω—è—Ç—å, —á—Ç–æ *it* –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∫ *animal*.\n",
    "\n",
    "–ü–æ –º–µ—Ä–µ —Ç–æ–≥–æ –∫–∞–∫ –º–æ–¥–µ–ª—å –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∫–∞–∂–¥–æ–µ —Å–ª–æ–≤–æ –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç \"–≤–∑–≥–ª—è–Ω—É—Ç—å\" –Ω–∞ –¥—Ä—É–≥–∏–µ —Å–ª–æ–≤–∞ –∏ –ª—É—á—à–µ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –¥–∞–Ω–Ω–æ–µ —Å–ª–æ–≤–æ. –ú–µ—Ö–∞–Ω–∏–∑–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è ‚Äì —ç—Ç–æ –º–µ—Ç–æ–¥, –∫–æ—Ç–æ—Ä—ã–π –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –∏—Å–ø–æ–ª—å–∑—É–µ—Ç, —á—Ç–æ–±—ã —Å–º–æ–¥–µ–ª–∏—Ä–æ–≤–∞—Ç—å \"–ø–æ–Ω–∏–º–∞–Ω–∏–µ\" –¥—Ä—É–≥–∏—Ö —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Å–ª–æ–≤ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å–ª–æ–≤–∞.\n",
    "\n",
    "–í–æ –≤—Ä–µ–º—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è *it* –≤ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–µ ‚Ññ5 —á–∞—Å—Ç—å –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è –Ω–∞ *The animal* –∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ñ—Ä–∞–≥–º–µ–Ω—Ç –µ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è *it*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L05/self_attention.png\" width=\"400\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ù–∞ –ø—Ä–∏–º–µ—Ä–µ –≤–µ–∫—Ç–æ—Ä–æ–≤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è: –≤–µ–∫—Ç–æ—Ä –∑–∞–ø—Ä–æ—Å–∞ $\\text{query}$, –≤–µ–∫—Ç–æ—Ä –∫–ª—é—á–∞ $\\text{key}$ –∏ –≤–µ–∫—Ç–æ—Ä –∑–Ω–∞—á–µ–Ω–∏—è $\\text{value}$. –û–Ω–∏ —Å–æ–∑–¥–∞—é—Ç—Å—è —Å –ø–æ–º–æ—â—å—é —É–º–Ω–æ–∂–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ —Å–ª–æ–≤–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ —Ç—Ä–∏ –º–∞—Ç—Ä–∏—Ü—ã –≤–µ—Å–æ–≤ (–ª–∏–Ω–µ–π–Ω—ã—Ö —Å–ª–æ—è) $W^Q, W^K, W^V$. –†–∞–∑–º–µ—Ä –Ω–æ–≤—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ ‚Äì 64, —Ä–∞–∑–º–µ—Ä –∏—Å—Ö–æ–¥–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ ‚Äì 512.\n",
    "\n",
    "$q_i=x_iW^Q$\n",
    "\n",
    "$k_i=x_iW^K$\n",
    "\n",
    "$v_i=x_iW^V$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L05/vector_attention.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–∞–ª–µ–µ –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è $\\text{score}$ –¥–ª—è $i$-–≥–æ —Å–ª–æ–≤–∞ –ø–æ –æ—Ç–Ω–æ—à–µ–Ω–∏—é –∫ –∫–∞–∂–¥–æ–º—É —Å–ª–æ–≤—É –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏. –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ –Ω—É–∂–Ω–æ —Å—Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ –¥—Ä—É–≥–∏—Ö —á–∞—Å—Ç—è—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –≤–æ –≤—Ä–µ–º—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è —Å–ª–æ–≤–∞ –≤ $i$-–π –ø–æ–∑–∏—Ü–∏–∏. –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –ø–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é —Å–∫–∞–ª—è—Ä–Ω–æ–≥–æ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–∞ –∑–∞–ø—Ä–æ—Å–∞ $q$ $i$-–≥–æ —Å–ª–æ–≤–∞ –∏ –≤–µ–∫—Ç–æ—Ä–∞ –∫–ª—é—á–∞ $k$ –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞.\n",
    "\n",
    "$$\\text{score}_{ij}=q_i \\cdot k_j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–∞ —Å–ª–µ–¥—É—é—â–µ–º —à–∞–≥–µ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –¥–µ–ª—è—Ç—Å—è –Ω–∞ –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã–π –∫–æ—Ä–µ–Ω—å –∏–∑ $d_k$ ‚Äì —Ä–∞–∑–º–µ—Ä–∞ –≤–µ–∫—Ç–æ—Ä–æ–≤ –∫–ª—é—á–∞ $k$. –ö –ø–æ–ª—É—á–∏–≤—à–∏–º—Å—è –∑–Ω–∞—á–µ–Ω–∏—è–º –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —Ñ—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ softmax, —á—Ç–æ–±—ã –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã –≤ —Å—É–º–º–µ –¥–∞–≤–∞–ª–∏ 1. –ü–æ–ª—É—á–µ–Ω–Ω—ã–π softmax-–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç, –≤ –∫–∞–∫–æ–π –º–µ—Ä–µ –∫–∞–∂–¥–æ–µ –∏–∑ —Å–ª–æ–≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è \"—Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è\" –Ω–∞ –¥—Ä—É–≥–æ–º —Å–ª–æ–≤–µ.\n",
    "\n",
    "$$\\text{softmax.score}_{ij}=\\text{softmax}(\\dfrac{\\text{score}_i}{\\sqrt d_k})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ—Å–ª–µ —ç—Ç–æ–≥–æ –∫–∞–∂–¥—ã–π –≤–µ–∫—Ç–æ—Ä –∑–Ω–∞—á–µ–Ω–∏—è $v$ —É–º–Ω–æ–∂–∞–µ—Ç—Å—è –Ω–∞ softmax-–∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç, –ø–æ–ª—É—á–∞–µ–º –≤–∑–≤–µ—à–µ–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã. –ò–¥–µ—è –≤ —Ç–æ–º, —á—Ç–æ –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π –∑–Ω–∞—á–µ–Ω–∏—è —Å–ª–æ–≤, –Ω–∞ –∫–æ—Ç–æ—Ä—ã—Ö –º—ã —Ñ–æ–∫—É—Å–∏—Ä—É–µ–º—Å—è, –∏ –æ—Ç–≤–µ—Å—Ç–∏ –Ω–∞ –≤—Ç–æ—Ä–æ–π –ø–ª–∞–Ω –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Å–ª–æ–≤–∞ (—É–º–Ω–æ–∂–∏–≤ –∏—Ö –Ω–∞ –Ω–µ–±–æ–ª—å—à–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è, –Ω–∞–ø—Ä–∏–º–µ—Ä, 0.001). –ó–∞—Ç–µ–º  –≤–∑–≤–µ—à–µ–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã —Å–∫–ª–∞–¥—ã–≤–∞—é—Ç—Å—è. –†–µ–∑—É–ª—å—Ç–∞—Ç (–≤–∑–≤–µ—à–µ–Ω–Ω–∞—è —Å—É–º–º–∞) –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –≤—ã—Ö–æ–¥ —Å–ª–æ—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è $i$-–≥–æ —Å–ª–æ–≤–∞.\n",
    "\n",
    "$$\\text{sum}_i= \\sum_{j=1}^n v_j \\cdot \\text{softmax.score}_{ij}$$\n",
    "\n",
    "–ü–æ–ª—É—á–µ–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è –¥–∞–ª—å—à–µ –≤ –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å –ø—Ä—è–º–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L05/attention_example.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ù–∞ –ø—Ä–∏–º–µ—Ä–µ –º–∞—Ç—Ä–∏—Ü"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ, –º–∞—Ç—Ä–∏—Ü—ã –∑–∞–ø—Ä–æ—Å–∞ $Q$, –∫–ª—é—á–∞ $K$ –∏ –∑–Ω–∞—á–µ–Ω–∏—è $V$ –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è —Å –ø–æ–º–æ—â—å—é —É–º–Ω–æ–∂–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –º–∞—Ç—Ä–∏—Ü—ã $X$ –Ω–∞ –º–∞—Ç—Ä–∏—Ü—ã –≤–µ—Å–æ–≤ (–ª–∏–Ω–µ–π–Ω—ã–µ —Å–ª–æ–∏) $W^Q, W^K, W^V$. –ö–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ –≤ –º–∞—Ç—Ä–∏—Ü–µ $X$ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Å–ª–æ–≤—É –≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L05/matrix_attention.png\" width=\"450\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ—Å–ª–µ–¥—É—é—â–∏–µ —ç—Ç–∞–ø—ã –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤—ã—Ö–æ–¥–∞ —Å–ª–æ—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –º–æ–≥—É—Ç –±—ã—Ç—å –æ—Ç—Ä–∞–∂–µ–Ω—ã –≤ –æ–¥–Ω–æ–π —Ñ–æ—Ä–º—É–ª–µ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L05/attention_score.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–∞–ø–∏—à–µ–º —Ñ—É–Ω–∫—Ü–∏—é `attention` –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è.\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}(\\dfrac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "–î–ª—è —É–º–Ω–æ–∂–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü $Q, K, V$ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–µ—Ç–æ–¥ `torch.matmul()`.\n",
    "\n",
    "–¢—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Å—É—â–µ—Å—Ç–≤–ª—è–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–∞ `.transpose`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"\"\"\n",
    "    Compute 'Scaled Dot Product Attention'\n",
    "    \"\"\"\n",
    "    d_k = query.size(-1)\n",
    "    # compute the attention scores by using torch.matmul\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = torch.softmax(scores, dim=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    # compute the result as the values weighted by attention probabilities (again, using torch.matmul)\n",
    "    result = torch.matmul(p_attn, value)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "query = torch.tensor([[0, 0], [0, 1], [1, 1]], dtype=torch.float)\n",
    "key = torch.tensor([[100, 0], [0, 100], [0, 0]], dtype=torch.float)\n",
    "value = torch.tensor([[1, 0], [0, 1], [0, 0]], dtype=torch.float)\n",
    "results = attention(query, key, value)\n",
    "print(f\"Query:\\n{query}\")\n",
    "print(f\"Key:\\n{key}\")\n",
    "print(f\"Value:\\n{value}\")\n",
    "print(f\"Results:\\n{results}\")\n",
    "\n",
    "assert np.allclose(\n",
    "    results[0].numpy(), [1 / 3, 1 / 3]\n",
    ")  # the first query attends to all keys equally\n",
    "assert np.allclose(\n",
    "    results[1].numpy(), [0, 1]\n",
    ")  # the second query attends only to the second key\n",
    "assert np.allclose(\n",
    "    results[2].numpy(), [1 / 2, 1 / 2]\n",
    ")  # the third query attends to the first and second key equally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–µ –º–µ—Ö–∞–Ω–∏–∑–º –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è —Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤—É–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è (multi-headed attention). –≠—Ç–æ –ø–æ–≤—ã—à–∞–µ—Ç —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —Å–ª–æ–≤–∞—Ö. –í —Å–ª—É—á–∞–µ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –º—ã —Ä–∞—Å–ø–æ–ª–∞–≥–∞–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–º–∏ –º–∞—Ç—Ä–∏—Ü–∞–º–∏ –≤–µ—Å–æ–≤ $W^Q, W^K, W^V$  –¥–ª—è –∫–∞–∂–¥–æ–π \"–≥–æ–ª–æ–≤—ã\", —á—Ç–æ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –¥–∞–µ—Ç —Ä–∞–∑–Ω—ã–µ –º–∞—Ç—Ä–∏—Ü—ã $Q, K, V$. –ö–∞–∫ –∏ —Ä–∞–Ω–µ–µ, –º–∞—Ç—Ä–∏—Ü–∞ $X$ —É–º–Ω–æ–∂–∞–µ—Ç—Å—è –Ω–∞ –≤–µ—Å–∞ $W^Q, W^K, W^V$ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü $Q, K, V$.\n",
    "\n",
    "–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 8 \"–≥–æ–ª–æ–≤\" –≤–Ω–∏–º–∞–Ω–∏—è, —Ç–∞–∫ —á—Ç–æ –≤ –∏—Ç–æ–≥–µ —É –Ω–∞—Å –ø–æ–ª—É—á–∞–µ—Ç—Å—è 8 –Ω–∞–±–æ—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞ –∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞. –°–¥–µ–ª–∞–≤ —Ç–µ –∂–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è 8 —Ä–∞–∑ —Å —Ä–∞–∑–Ω—ã–º–∏ –º–∞—Ç—Ä–∏—Ü–∞–º–∏ –≤–µ—Å–æ–≤, –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –ø–æ–ª—É—á–∏–º 8 —Ä–∞–∑–Ω—ã—Ö –º–∞—Ç—Ä–∏—Ü."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L05/multihead_attention.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–¥–Ω–∞–∫–æ —Å–ª–æ–π —Å–µ—Ç–∏ –ø—Ä—è–º–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –Ω–µ –æ–∂–∏–¥–∞–µ—Ç, —á—Ç–æ –∫ –Ω–µ–º—É –ø–æ—Å—Ç—É–ø–∏—Ç 8 –º–∞—Ç—Ä–∏—Ü ‚Äî –æ–Ω –∂–¥–µ—Ç –≤—Å–µ–≥–æ –æ–¥–Ω—É, –≤ –∫–æ—Ç–æ—Ä—É—é –Ω–∞–º –∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Å–∂–∞—Ç—å –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –º–∞—Ç—Ä–∏—Ü—ã. –ß—Ç–æ–±—ã —ç—Ç–æ —Å–¥–µ–ª–∞—Ç—å, –º–æ–∂–Ω–æ –∫–æ–Ω–∫–∞—Ç–µ–Ω–∏—Ä–æ–≤–∞—Ç—å –∏ –∑–∞—Ç–µ–º —É–º–Ω–æ–∂–∏—Ç—å –∏—Ö –Ω–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–µ—Å–∞ –º–∞—Ç—Ä–∏—Ü—ã $W^O$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L05/concatenate.png\" width=\"800\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú—ã –º–æ–∂–µ–º –æ—Ç–æ–±—Ä–∞–∑–∏—Ç—å –≤—Å–µ \"–≥–æ–ª–æ–≤—ã\" –≤–Ω–∏–º–∞–Ω–∏—è –Ω–∞ –æ–¥–Ω–æ–π –∫–∞—Ä—Ç–∏–Ω–∫–µ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L05/multihead_viz.png\" width=\"400\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–ª—è —Å–æ–∑–¥–∞–Ω–∏–µ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –Ω–µ–æ–±—Ö–æ–¥–∏–º –∫–ª–∞—Å—Å `MultiHeadedAttention`.\n",
    "\n",
    "–ö–∞–∂–¥–∞—è –≥–æ–ª–æ–≤–∞ –≤–Ω–∏–º–∞–Ω–∏—è –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –≤—ã—á–∏—Å–ª—è–µ—Ç –≤–Ω–∏–º–∞–Ω–∏–µ (—Å–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ) –¥–ª—è —Å–≤–æ–∏—Ö –º–∞—Ç—Ä–∏—Ü $Q,K,V$. –°–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, –∫–∞–∂–¥–∞—è –≥–æ–ª–æ–≤–∞ –º–æ–∂–µ—Ç \"–æ–±—Ä–∞—â–∞—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ\" –Ω–∞ —Ä–∞–∑–Ω—ã–µ —á–∞—Å—Ç–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{concat}(\\text{head}_1, ..., \\text{head}_h)W^O$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "\n",
    "def clones(module, N):\n",
    "    \"Produce N identical layers\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"Take in model size and number of heads\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [\n",
    "            l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "            for l, x in zip(self.linears, (query, key, value))\n",
    "        ]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch\n",
    "        x = attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear\n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò–∑-–∑–∞ —Ç–æ–≥–æ, —á—Ç–æ –º—ã –∏–∑–±–∞–≤–∏–ª–∏—Å—å –æ—Ç —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏, –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ—Ä—è–¥–∫–µ —Å–ª–æ–≤ –ø–µ—Ä–µ—Å—Ç–∞–ª–∞ —É—á–∏—Ç—ã–≤–∞—Ç—å—Å—è –º–æ–¥–µ–ª—å—é. –í –∫–ª–∞—Å—Å–µ `MultiHeadedAttention` –æ–ø–µ—Ä–∞—Ü–∏–∏ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –∫ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ 2 (–ø—Ä–∏–∑–Ω–∞–∫–∏ —Å–ª–æ–≤), –Ω–æ –Ω–µ –∫ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ 1 (—Å–ª–æ–≤–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏).\n",
    "\n",
    "–î–ª—è —Ç–æ–≥–æ —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –ø–æ–Ω–∏–º–∞–ª–∞ –ø–æ—Ä—è–¥–æ–∫ —Å–ª–æ–≤, –≤–≤–æ–¥—è—Ç—Å—è –≤–µ–∫—Ç–æ—Ä—ã –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–∑–∏—Ü–∏–∏ (positional encoding). –û–Ω–∏ —Å—É–º–º–∏—Ä—É—é—Ç—Å—è —Å –≤–µ–∫—Ç–æ—Ä–∞–º–∏ –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L05/positional_encoding.png\" width=\"850\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ–∑–∏—Ü–∏–∏ –∫–æ–¥–∏—Ä—É–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é —Ç—Ä–∏–≥–æ–Ω–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö —Ñ—É–Ω–∫—Ü–∏–π: —Å–∏–Ω—É—Å–∞ –∏ –∫–æ—Å–∏–Ω—É—Å–∞.\n",
    "\n",
    "–ü—É—Å—Ç—å –µ—Å—Ç—å –≤—Ö–æ–¥–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª–∏–Ω—ã $L$, –Ω—É–∂–Ω–æ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –ø–æ–∑–∏—Ü–∏—é –Ω–µ–∫–æ—Ç–æ—Ä–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ $\\text{pos}$.\n",
    "\n",
    "$\\text{pos}$ ‚Äî –∏–Ω–¥–µ–∫—Å —Å–ª–æ–≤–∞ –≤ –∏—Å—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, $0 \\leq \\text{pos} < L$\n",
    "\n",
    "$d$ ‚Äî —Ä–∞–∑–º–µ—Ä —ç–º–µ–¥–¥–∏–Ω–≥–æ–≤ –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è\n",
    "\n",
    "$i$ ‚Äî –Ω–æ–º–µ—Ä –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è, $0 \\leq i < \\large \\frac{d}{2}$\n",
    "\n",
    "$p_0$ ‚Äî –ø–µ—Ä–≤–æ–µ —Å–ª–æ–≤–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, $d$ ‚Äî –¥–ª–∏–Ω–∞ –≤–µ–∫—Ç–æ—Ä–∞, $i$ ‚Äî –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã –≤–µ–∫—Ç–æ—Ä–∞."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L05/pe1.png\" width=\"500\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://www.youtube.com/watch?v=dichIcUZfOw\">Visual Guide to Transformer Neural Networks</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–ª–æ–≤–∞ —Å —Ä–∞–∑–Ω—ã–º –Ω–æ–º–µ—Ä–æ–º –ø–æ–∑–∏—Ü–∏–∏ –±—É–¥—É—Ç –∏–º–µ—Ç—å —Ä–∞–∑–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ –æ—Å–∏ $y$. –û–¥–Ω–∞–∫–æ –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –ø–æ–∑–∏—Ü–∏–π ($p_0$ –∏ $p_6$) –∑–Ω–∞—á–µ–Ω–∏—è —Å–æ–≤–ø–∞–¥–∞—é—Ç, –ø–æ—Å–∫–æ–ª—å–∫—É —Å–∏–Ω—É—Å ‚Äî –ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è —Ñ—É–Ω–∫—Ü–∏—è."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L05/pe2.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://www.youtube.com/watch?v=dichIcUZfOw\">Visual Guide to Transformer Neural Networks</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ–ª—É—á–∏—Ç—å —Ä–∞–∑–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –º–æ–∂–Ω–æ –±–ª–∞–≥–æ–¥–∞—Ä—è –ø–∞—Ä–∞–º–µ—Ç—Ä—É $i$. –ü—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –∑–Ω–∞—á–µ–Ω–∏—è $i$ –º–µ–Ω—è–µ—Ç—Å—è —á–∞—Å—Ç–æ—Ç–∞, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –ø–æ–ª—É—á–µ–Ω–∏—é —Ä–∞–∑–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –¥–ª—è $p_0$ –∏ $p_6$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L05/pe3.png\" width=\"800\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://www.youtube.com/watch?v=dichIcUZfOw\">Visual Guide to Transformer Neural Networks</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–æ–∑–¥–∞–¥–∏–º –∫–ª–∞—Å—Å `PositionalEncoding` –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–∑–∏—Ü–∏–∏.\n",
    "\n",
    "$PE_{(\\text{pos}, 2i)} = \\sin(\\dfrac{\\text{pos}}{10000^{2i / d_\\text{model}}})$\n",
    "\n",
    "$PE_{(\\text{pos}, 2i+1)} = \\cos(\\dfrac{\\text{pos}}{10000^{2i / d_\\text{model}}})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = 10000 ** (torch.arange(0, d_model, 2) / d_model)\n",
    "        pe[:, 0::2] = torch.sin(position / div_term)\n",
    "        pe[:, 1::2] = torch.cos(position / div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.pe[:, : x.size(1)], requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "pe = PositionalEncoding(20, 0)\n",
    "y = pe.forward(Variable(torch.zeros(1, 100, 20)))\n",
    "plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())\n",
    "plt.legend([\"dim %d\" % p for p in [4, 5, 6, 7]])\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –°–ª–æ–∏ Embedding –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ –¥—Ä—É–≥–∏–º –º–æ–¥–µ–ª—è–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π, –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—É—á–µ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –∏ –≤—ã—Ö–æ–¥–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –≤ –≤–µ–∫—Ç–æ—Ä—ã —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ $d_{\\text{model}}$. –ú—ã —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ª–∏–Ω–µ–π–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∏ —Ñ—É–Ω–∫—Ü–∏—é softmax –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞ –≤ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Å–ª–µ–¥—É—é—â–µ–≥–æ —Ç–æ–∫–µ–Ω–∞. –í –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏ –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–¥–Ω—É –∏ —Ç—É –∂–µ –≤–µ—Å–æ–≤—É—é –º–∞—Ç—Ä–∏—Ü—É –º–µ–∂–¥—É —Å–ª–æ—è–º–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ –ª–∏–Ω–µ–π–Ω—ã–º –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ–º, –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω—ã–º –¥–æ softmax. –í —Å–ª–æ—è—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –º—ã —É–º–Ω–æ–∂–∞–µ–º —ç—Ç–∏ –≤–µ—Å–∞ –Ω–∞ $\\sqrt{d_{\\text{model}}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C–µ—Ç—å –ø—Ä—è–º–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ó–∞ —Å–ª–æ–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –≤ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–µ –∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–µ —Å–ª–µ–¥—É–µ—Ç —Å–µ—Ç—å –ø—Ä—è–º–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è. –û–Ω–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –¥–≤—É—Ö –ª–∏–Ω–µ–π–Ω—ã—Ö —Å–ª–æ–µ–≤ –∏ —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ ReLU –º–µ–∂–¥—É –Ω–∏–º–∏. –†–∞–∑–º–µ—Ä –≤—Ö–æ–¥–Ω—ã—Ö –∏ –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö ‚Äî 512, –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π —Ä–∞–∑–º–µ—Ä ‚Äî 2048 (–≤ 4 —Ä–∞–∑–∞ –±–æ–ª—å—à–µ)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ö–æ–¥–∏—Ä–æ–≤—â–∏–∫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–æ–∂–µ–º –ø–µ—Ä–µ–π—Ç–∏ –∫ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –±–ª–æ–∫–∞ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞. –û–Ω —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Å—Ç–µ–∫–∞ $N=6$ –∏–¥–µ–Ω—Ç–∏—á–Ω—ã—Ö —Å–ª–æ–µ–≤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L05/enc_details.png\" width=\"400\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"Pass the input (and mask) through each layer in turn\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö–∞–∂–¥—ã–π –ø–æ–¥—Å–ª–æ–π (–≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ –≤–Ω–∏–º–∞–Ω–∏–µ, –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π) –≤–∫–ª—é—á–∞–µ—Ç —Å–∫–≤–æ–∑–Ω—É—é —Å–≤—è–∑—å (residual connection), –∑–∞ –∫–æ—Ç–æ—Ä–æ–π —Å–ª–µ–¥—É–µ—Ç —ç—Ç–∞–ø –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–ª–æ—è ([layer normalization üõ†Ô∏è[doc]](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.htm)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L05/connection_normalization.png\" width=\"500\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–≠–º–±–µ–¥–¥–∏–Ω–≥ –∫–∞–∂–¥–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥—É–±–ª–∏—Ä—É–µ—Ç—Å—è: –æ–¥–Ω–∞ –∫–æ–ø–∏—è –ø–æ—Å—Ç—É–ø–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ —Å–ª–æ—è (–≤–Ω–∏–º–∞–Ω–∏—è –∏–ª–∏ —Å–µ—Ç–∏ –ø—Ä—è–º–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è), –¥—Ä—É–≥–∞—è –∫–æ–ø–∏—è –Ω–µ –º–µ–Ω—è–µ—Ç—Å—è –∏ –ø—Ä–∏–±–∞–≤–ª—è–µ—Ç—Å—è –∫ –≤—ã—Ö–æ–¥—É —Å–ª–æ—è. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –∑–∞—Ç—É—Ö–∞–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞.\n",
    "\n",
    "–ü–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è. –í–µ—Å–∞ –º–æ–≥—É—Ç –ø—Ä–∏–Ω–∏–º–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Ä–∞–∑–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ, –Ω–æ —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –æ–¥–∏–Ω–∞–∫–æ–≤–∞—è. –ò–∑-–∑–∞ —ç—Ç–æ–≥–æ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç —Å–ª–æ–∂–Ω–æ—Å—Ç—å —Å –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º –≤–µ—Å–æ–≤.\n",
    "\n",
    "–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –ø–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º: –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫ –ø–µ—Ä–≤–æ–º—É –ø—Ä–∏–∑–Ω–∞–∫—É –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ –≤ –ø–µ—Ä–≤–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏, –∑–∞—Ç–µ–º –∫–æ –≤—Ç–æ—Ä–æ–º—É –ø—Ä–∏–∑–Ω–∞–∫—É –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ –≤ –ø–µ—Ä–≤–æ–º –ø—Ä–µ–¥–æ–∂–µ–Ω–∏–∏ –∏ —Ç.–¥.\n",
    "\n",
    "–°—Ä–µ–¥–Ω–µ–µ: $\\mu_i =\\dfrac{1}{m}\\sum\\limits_{j=1}^mx_{ij}$\n",
    "\n",
    "–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ: $\\sigma_i = \\dfrac{1}{m}\\sum\\limits_{j=1}^m(x_{ij}-\\mu_i)$\n",
    "\n",
    "–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: $\\hat x_{ij} = \\dfrac {x_{ij}-\\mu_i}{\\sigma_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-content/L05/out/layer_normalization.png\" width=\"400\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–∫–≤–æ–∑–Ω—É—é —Å–≤—è–∑—å –≤–æ–∫—Ä—É–≥ –∫–∞–∂–¥–æ–≥–æ –∏–∑ –¥–≤—É—Ö –ø–æ–¥—Å–ª–æ–µ–≤, –¥–∞–ª–µ–µ —Å–ª–µ–¥—É–µ—Ç –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å–ª–æ—è."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Return a2 * x_normalized + b2,\n",
    "        where x_normalized is calculated by subtracting row-wise means from x and dividing the result by row-wise standard deviation + eps.\n",
    "        standard deviation is calculated with Bessel's correction (the default in Pytorch)\n",
    "        \"\"\"\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ln = LayerNorm(2, eps=0)\n",
    "with torch.no_grad():\n",
    "    result = ln(torch.tensor([[0.0, 1], [100, 101], [100, 200]])).numpy()\n",
    "\n",
    "# becasue of Bessel's correction, standard deviation is pulled to 0. Here we un-pull it back.\n",
    "print(f\"Result of layer normalization:\\n{result}\")\n",
    "result_unnormalized = result / np.sqrt(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–æ –µ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫–∞–∂–¥–æ–≥–æ –ø–æ–¥—Å–ª–æ—è:\n",
    "$$\\mathrm{LayerNorm}(x + \\mathrm{Sublayer}(x)),$$\n",
    "\n",
    "–≥–¥–µ $\\mathrm{Sublayer}(x)$ ‚Äî —ç—Ç–æ —Ñ—É–Ω–∫—Ü–∏—è, —Ä–µ–∞–ª–∏–∑—É–µ–º–∞—è —Å–∞–º–∏–º –ø–æ–¥—Å–ª–æ–µ–º.\n",
    "\n",
    "–ú—ã –ø—Ä–∏–º–µ–Ω—è–µ–º `Dropout` –∫ –≤—ã—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º –∫–∞–∂–¥–æ–≥–æ –ø–æ–¥—Å–ª–æ—è, –ø—Ä–µ–∂–¥–µ —á–µ–º –æ–Ω–∏ —Å–∫–ª–∞–¥—ã–≤–∞—é—Ç—Å—è –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É—é—Ç—Å—è.\n",
    "\n",
    "–ß—Ç–æ–±—ã —É–ø—Ä–æ—Å—Ç–∏—Ç—å —ç—Ç–∏ —Å–∫–≤–æ–∑–Ω—ã–µ —Å–≤—è–∑–∏, –≤—Å–µ –ø–æ–¥—Å–ª–æ–∏ –≤ –º–æ–¥–µ–ª–∏, –∞ —Ç–∞–∫–∂–µ —Å–ª–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –≤—ã–¥–∞—é—Ç –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é $d_{\\text{model}}=512$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö–∞–∂–¥—ã–π —Å–ª–æ–π —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –¥–≤—É—Ö –ø–æ–¥—Å–ª–æ–µ–≤. –ü–µ—Ä–≤—ã–π –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –º–µ—Ö–∞–Ω–∏–∑–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è, –∞ –≤—Ç–æ—Ä–æ–π ‚Äî –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—É—é —Å–µ—Ç—å –ø—Ä—è–º–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward\"\n",
    "\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –î–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ë–ª–æ–∫ –¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞ —É—Å—Ç—Ä–æ–µ–Ω –æ—á–µ–Ω—å –ø–æ—Ö–æ–∂–µ, –Ω–æ –µ—Å—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –æ—Ç–ª–∏—á–∏–π.\n",
    "- –î–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –¥–≤–∞ –∞—Ä–≥—É–º–µ–Ω—Ç–∞: —Ü–µ–ª–µ–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –∏ –≤—ã—Ö–æ–¥ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞.\n",
    "- –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–≤–∞ —Å–ª–æ—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è:\n",
    "  - –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ü–µ–ª–µ–≤–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è,\n",
    "  - –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –º–µ–∂–¥—É –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–æ–º –∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–æ–º –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Ü–µ–ª–µ–≤–æ–≥–æ –∏ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.\n",
    "- –í—Ç–æ—Ä–æ–π —Å–ª–æ–π –≤ –∫–∞—á–µ—Å—Ç–≤–µ –º–∞—Ç—Ä–∏—Ü $K$ –∏ $V$ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤—ã—Ö–æ–¥ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞.\n",
    "\n",
    "–î–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫ –≤—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–¥–∞—á—É —è–∑—ã–∫–æ–≤–æ–≥–æ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–ª–æ–≤–∞. –°–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ, –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ü–µ–ª–µ–≤–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –æ–Ω –Ω–µ –º–æ–∂–µ—Ç \"–∑–∞–≥–ª—è–¥—ã–≤–∞—Ç—å\" –≤–ø–µ—Ä–µ–¥ –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π –µ—â–µ –Ω–µ –±—ã–ª —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω. –ü–æ—ç—Ç–æ–º—É –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –ø–æ–∑–∏—Ü–∏–π –ø–æ—Å–ª–µ —Ç–µ–∫—É—â–µ–π, –∏—Ö –≤–µ–∫—Ç–æ—Ä—ã –∑–∞–ø–æ–ª–Ω—è—é—Ç—Å—è –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ `-inf`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L05/enc_dec_details.png\" width=\"800\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://jalammar.github.io/illustrated-transformer/\">The Illustrated Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫ —Ç–∞–∫–∂–µ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Å—Ç–µ–∫–∞ –∏–∑ $N=6$ –∏–¥–µ–Ω—Ç–∏—á–Ω—ã—Ö —Å–ª–æ–µ–≤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –∫ –¥–≤—É–º –ø–æ–¥—Å–ª–æ—è–º –Ω–∞ –∫–∞–∂–¥–æ–º —Å–ª–æ–µ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞, –¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç—Ä–µ—Ç–∏–π –ø–æ–¥—Å–ª–æ–π, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–∏–º–µ–Ω—è–µ—Ç –º–µ—Ö–∞–Ω–∏–∑–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –∫ –≤—ã—Ö–æ–¥–Ω—ã–º –¥–∞–Ω–Ω—ã–º –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞. –ö–∞–∫ –∏ –≤ —Å–ª—É—á–∞–µ —Å –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–æ–º, –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–∫–≤–æ–∑–Ω—É—é —Å–≤—è–∑—å –≤–æ–∫—Ä—É–≥ –∫–∞–∂–¥–æ–≥–æ –∏–∑ –ø–æ–¥—Å–ª–æ–µ–≤ —Å –ø–æ—Å–ª–µ–¥—É—é—â–µ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π —Å–ª–æ—è."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
    "\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú—ã —Ç–∞–∫–∂–µ –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –ø–æ–¥—Å–ª–æ–π –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è –≤ —Å—Ç–µ–∫–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞, –¥–æ–±–∞–≤–ª—è—è –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ. –≠—Ç–æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ, —á—Ç–æ–±—ã –Ω–µ —É—á–∏—Ç—ã–≤–∞—Ç—å —Ç–æ–∫–µ–Ω—ã –≤ —Å–ª–µ–¥—É—é—â–∏—Ö –ø–æ–∑–∏—Ü–∏—è—Ö –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Ç–µ–∫—É—â–µ–π –∏ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –ø–æ–∑–∏—Ü–∏–π. –ú–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ—Ç, —á—Ç–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –ø–æ–∑–∏—Ü–∏–∏ $i$ –º–æ–≥—É—Ç –∑–∞–≤–∏—Å–µ—Ç—å —Ç–æ–ª—å–∫–æ –æ—Ç –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –ø–æ–∑–∏—Ü–∏—è—Ö, –º–µ–Ω—å—à–∏—Ö, —á–µ–º $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"Mask out subsequent positions\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype(\"uint8\")\n",
    "    return torch.from_numpy(subsequent_mask) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–∞—Å–∫–∞ –≤–Ω–∏–º–∞–Ω–∏—è –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø–æ–∑–∏—Ü–∏—é (—Å—Ç—Ä–æ–∫–∞), –Ω–∞ –∫–æ—Ç–æ—Ä—É—é —Ä–∞–∑—Ä–µ—à–µ–Ω–æ —Å–º–æ—Ç—Ä–µ—Ç—å –∫–∞–∂–¥–æ–º—É —Å–ª–æ–≤—É (—Å—Ç–æ–ª–±–µ—Ü). –°–ª–æ–≤–∞ –±–ª–æ–∫–∏—Ä—É—é—Ç—Å—è –¥–ª—è —Ç–æ–≥–æ, —á—Ç–æ–±—ã –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –º–æ–∂–Ω–æ –±—ã–ª–æ –æ–±—Ä–∞—Ç–∏—Ç—å –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –±—É–¥—É—â–∏–µ —Å–ª–æ–≤–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "data = subsequent_mask(6)[0]\n",
    "ax.imshow(data)\n",
    "\n",
    "ticks = [0, 1, 2, 3, 4, 5]\n",
    "ticklabels = [\"I\", \"am\", \"a\", \"student\", \"at\", \"MSU\"]\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_xticklabels(ticklabels)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_yticklabels(ticklabels)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–ø—Ä–µ–¥–µ–ª–∏–º —Ñ—É–Ω–∫—Ü–∏—é –æ—Ç –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –ø–æ–ª–Ω–æ–π –º–æ–¥–µ–ª–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L05/transformer_full.png\" width=\"1000\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html\">Sequence to Sequence and Attention</a></em></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    A standard Encoder-Decoder architecture.\n",
    "    Base for this and many other models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"Take in and process masked src and target sequences.\"\n",
    "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "\n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"Construct a model from hyperparameters\"\n",
    "    c = copy.deepcopy  # use it for attn, ffn, and position in the model layers\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    # insert correct arguments into the EncoderDecoder constructor.\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab),\n",
    "    )\n",
    "\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ—Å—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small example model.\n",
    "tmp_model = make_model(10, 30, 2)\n",
    "(\n",
    "    sum(p.numel() for p in tmp_model.encoder.parameters())\n",
    "    + sum(p.numel() for p in tmp_model.decoder.parameters())\n",
    "    + sum(p.numel() for p in tmp_model.tgt_embed.parameters())\n",
    "    + sum(p.numel() for p in tmp_model.src_embed.parameters())\n",
    "    + sum(p.numel() for p in tmp_model.generator.parameters())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ –æ–ø–∏—Å—ã–≤–∞–µ—Ç—Å—è —Ä–µ–∂–∏–º –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –Ω–∞—à–µ–π –º–æ–¥–µ–ª–∏. –ß–∞—Å—Ç—å —Ñ—É–Ω–∫—Ü–∏–π –º—ã –Ω–µ –±—É–¥–µ–º —Ä–∞–∑–±–∏—Ä–∞—Ç—å –ø–æ–¥—Ä–æ–±–Ω–æ –∏ –∏–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∏—Ö –∏–∑ —Ñ–∞–π–ª–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L05/transformer_code.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –î–µ–ª–µ–Ω–∏–µ –Ω–∞ –±–∞—Ç—á–∏ –∏ –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–Ω–∞—á–∞–ª–µ –æ–ø—Ä–µ–¥–µ–ª–∏–º —Ñ–æ—Ä–º–∞—Ç –±–∞—Ç—á–∞, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–¥–µ—Ä–∂–∏—Ç –∏—Å—Ö–æ–¥–Ω–æ–µ –∏ —Ü–µ–ª–µ–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, –∞ —Ç–∞–∫–∂–µ —Å–æ–∑–¥–∞–Ω–∏–µ –º–∞—Å–æ–∫."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    \"Object for holding a batch of data with mask during training.\"\n",
    "\n",
    "    def __init__(self, src, trg=None, pad=0):\n",
    "        self.src = src\n",
    "        self.src_mask = (src != pad).unsqueeze(-2)\n",
    "        if trg is not None:\n",
    "            self.trg = trg[:, :-1]\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            self.trg_mask = self.make_std_mask(self.trg, pad)\n",
    "            self.ntokens = (self.trg_y != pad).data.sum()\n",
    "\n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "        tgt_mask = tgt_mask & Variable(\n",
    "            subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data)\n",
    "        )\n",
    "        return tgt_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ó–∞—Ç–µ–º —Å–æ–∑–¥–∞–¥–∏–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –æ–±—É—á–µ–Ω–∏—è, –ø–æ–¥—Å—á–µ—Ç–∞ –∑–Ω–∞—á–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def run_epoch(data_iter, model, loss_compute):\n",
    "    \"Standard Training and Logging Function\"\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        out = model.forward(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\n",
    "        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "        if i % 50 == 1:\n",
    "            elapsed = time.time() - start\n",
    "            print(\n",
    "                \"Epoch Step: %d Loss: %f Tokens per Sec: %f\"\n",
    "                % (i, loss / batch.ntokens, tokens / elapsed)\n",
    "            )\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "    return total_loss / total_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ë—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä `Adam` [üõ†Ô∏è[doc]](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ $\\beta_1=0.9$, $\\beta_2=0.98$ –∏ $\\epsilon=10^{-9}$.  –°–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –±—É–¥–µ—Ç –≤–∞—Ä—å–∏—Ä–æ–≤–∞—Ç—å—Å—è –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å —Ñ–æ—Ä–º—É–ª–æ–π:\n",
    "$$\n",
    "\\text{lrate} = d_{\\text{model}}^{-0.5} \\cdot\n",
    "  \\min(\\text{step_num}^{-0.5},\n",
    "    \\text{step_num} \\cdot \\text{warmup_steps}^{-1.5})\n",
    "$$\n",
    "\n",
    "–≠—Ç–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ª–∏–Ω–µ–π–Ω–æ–º—É —É–≤–µ–ª–∏—á–µ–Ω–∏—é —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –ø–µ—Ä–≤—ã—Ö —ç—Ç–∞–ø–∞—Ö $\\text{warmup_steps}$ –∏ –ø–æ—Å–ª–µ–¥—É—é—â–µ–º—É –µ–µ —É–º–µ–Ω—å—à–µ–Ω–∏—é –ø—Ä–æ–ø–æ—Ä—Ü–∏–æ–Ω–∞–ª—å–Ω–æ –æ–±—Ä–∞—Ç–Ω–æ–º—É –∫–≤–∞–¥—Ä–∞—Ç–Ω–æ–º—É –∫–æ—Ä–Ω—é –∏–∑ –Ω–æ–º–µ—Ä–∞ —à–∞–≥–∞. –ë—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ $\\text{warmup_steps}=4000$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω—É–∂–Ω—ã–π –∫–ª–∞—Å—Å –∏–∑ —Ñ–∞–π–ª–∞ –∏ –ø–æ—Å—Ç—Ä–æ–∏–º –ø—Ä–∏–º–µ—Ä –≥—Ä–∞—Ñ–∏–∫–æ–≤ –¥–ª—è —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤ –º–æ–¥–µ–ª–∏ –∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from transformer_code import NoamOpt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Three settings of the lrate hyperparameters.\n",
    "opts = [\n",
    "    NoamOpt(512, 1, 4000, None),\n",
    "    NoamOpt(512, 1, 8000, None),\n",
    "    NoamOpt(256, 1, 4000, None),\n",
    "]\n",
    "plt.plot(np.arange(1, 20000), [[opt.rate(i) for opt in opts] for i in range(1, 20000)])\n",
    "plt.legend([\"512:4000\", \"512:8000\", \"256:4000\"])\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –†–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –º–µ—Ç–æ–∫ (label smoothing) ‚Äî —ç—Ç–æ –º–µ—Ç–æ–¥, –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏. –û–Ω –ø–æ–º–æ–≥–∞–µ—Ç –∏–∑–±–µ–∂–∞—Ç—å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –∑–∞ —Å—á–µ—Ç —É–º–µ–Ω—å—à–µ–Ω–∏—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –æ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –º–µ—Ç–æ–∫ –≤–æ –≤—Ä–µ–º—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è. –≠—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –∑–∞ —Å—á–µ—Ç –ø–æ–≤—ã—à–µ–Ω–∏—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ –≤ ¬´–Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö¬ª —è—Ä–ª—ã–∫–∞—Ö.\n",
    "\n",
    "–í–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –º–µ—Ç–æ–∫ —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–º $\\large \\varepsilon_{ls}$ $=0.1$. –í–º–µ—Å—Ç–æ one-hot —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ –æ–¥–Ω–æ–π —Ü–µ–ª–µ–≤–æ–π –≥—Ä—É–ø–ø–µ –º—ã —Å–æ–∑–¥–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ, –≤ –∫–æ—Ç–æ—Ä–æ–º –Ω–∞–∏–±–æ–ª—å—à—É—é –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∏–º–µ–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ, –∞ –æ—Å—Ç–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è—é—Ç—Å—è –ø–æ –≤—Å–µ–º—É —Å–ª–æ–≤–∞—Ä—é."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "q_i =\n",
    "\\left\\{\n",
    "    \\begin {aligned}\n",
    "         & 1 - \\varepsilon \\quad & \\text{if } i=y, \\\\\n",
    "         & \\varepsilon/(K-1) \\quad & \\text{otherwise}                  \n",
    "    \\end{aligned}\n",
    "\\right.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L05/label_smoothing.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://arxiv.org/abs/2011.12562\">Delving Deep into Label Smoothing</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ü—Ä–∏–º–µ—Ä –Ω–∞ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ–ø—Ä–æ–±—É–µ–º –ø—Ä–∏–º–µ–Ω–∏—Ç—å –º–æ–¥–µ–ª—å –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ—Å—Ç–æ–π –∑–∞–¥–∞—á–∏ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è. –ó–∞–¥–∞–Ω —Å–ª—É—á–∞–π–Ω—ã–π –Ω–∞–±–æ—Ä –≤—Ö–æ–¥–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –∏–∑ –Ω–µ–±–æ–ª—å—à–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è. –¶–µ–ª—å —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ–±—ã —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–µ –∂–µ —Å–∏–º–≤–æ–ª—ã."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ß—Ç–æ–±—ã –¥–∞–ª—å–Ω–µ–π—à–∏–π –∫–æ–¥ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–ª—Å—è, –∑–∞—Ñ–∏–∫—Å–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    random.seed(seed)\n",
    "\n",
    "\n",
    "set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–∞–ø–∏—à–µ–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen(V, batch, nbatches):\n",
    "    \"Generate random data for a src-tgt copy task.\"\n",
    "    for i in range(nbatches):\n",
    "        data = torch.from_numpy(np.random.randint(1, V, size=(batch, 10)))\n",
    "        data[:, 0] = 1\n",
    "        src = Variable(data, requires_grad=False)\n",
    "        tgt = Variable(data, requires_grad=False)\n",
    "        yield Batch(src, tgt, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–ø—Ä–µ–¥–µ–ª–∏–º —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"A simple loss compute and train function.\"\n",
    "\n",
    "    def __init__(self, generator, criterion, opt=None):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "\n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        loss = (\n",
    "            self.criterion(x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1))\n",
    "            / norm\n",
    "        )\n",
    "        loss.backward()\n",
    "        if self.opt is not None:\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad()\n",
    "        return loss.item() * norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ó–∞–ø—É—Å—Ç–∏–º –æ–±—É—á–µ–Ω–∏–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_code import LabelSmoothing\n",
    "\n",
    "# Train the simple copy task\n",
    "V = 11\n",
    "criterion = LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)\n",
    "model = make_model(V, V, N=2)\n",
    "model_opt = NoamOpt(\n",
    "    model.src_embed[0].d_model,\n",
    "    1,\n",
    "    400,\n",
    "    torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9),\n",
    ")\n",
    "\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    run_epoch(\n",
    "        data_gen(V, 20, 30),\n",
    "        model,\n",
    "        SimpleLossCompute(model.generator, criterion, model_opt),\n",
    "    )\n",
    "    model.eval()\n",
    "    print(\n",
    "        run_epoch(\n",
    "            data_gen(V, 20, 5),\n",
    "            model,\n",
    "            SimpleLossCompute(model.generator, criterion, None),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–∏–º–µ–Ω–∏–º –º–æ–¥–µ–ª—å –∏ –¥–µ–∫–æ–¥–∏—Ä—É–µ–º –µ—ë –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(model, src, src_mask, max_len, start_symbol):\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    for i in range(max_len - 1):\n",
    "        out = model.decode(\n",
    "            memory,\n",
    "            src_mask,\n",
    "            Variable(ys),\n",
    "            Variable(subsequent_mask(ys.size(1)).type_as(src.data)),\n",
    "        )\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "    return ys\n",
    "\n",
    "\n",
    "model.eval()\n",
    "src = Variable(torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]))\n",
    "print(f\"Input:\\n{src}\")\n",
    "src_mask = Variable(torch.ones(1, 1, 10))\n",
    "print(f\"Output:\\n{decode(model, src, src_mask, max_len=10, start_symbol=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ü—Ä–∏–º–µ—Ä –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–µ–ø–µ—Ä—å –º—ã —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º —Ä–µ–∞–ª—å–Ω—ã–π –ø—Ä–∏–º–µ—Ä —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä—É—Å—Å–∫–æ-–∞–Ω–≥–ª–∏–π—Å–∫–∏—Ö –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø—Ä–æ–µ–∫—Ç–∞ [Tatoeba üõ†Ô∏è[doc]](https://tatoeba.org/ru). –≠—Ç–∞ –∑–∞–¥–∞—á–∞ –Ω–∞–º–Ω–æ–≥–æ –º–µ–Ω—å—à–µ, —á–µ–º —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–Ω–∞—è –≤ –∏—Å—Ö–æ–¥–Ω–æ–π —Å—Ç–∞—Ç—å–µ, –Ω–æ –æ–Ω–∞ –∏–ª–ª—é—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—Å—é —Å–∏—Å—Ç–µ–º—É –≤ —Ü–µ–ª–æ–º."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è –±–∏–±–ª–∏–æ—Ç–µ–∫–æ–π [spaCy üõ†Ô∏è[doc]](https://spacy.io/) –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "!python -m spacy download en_core_web_sm -q\n",
    "!python -m spacy download ru_core_news_sm -q\n",
    "!pip install -q datasets==3.6.0\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "spacy_ru = spacy.load(\"ru_core_news_sm\")\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def tokenize_ru(text):\n",
    "    return [tok.text for tok in spacy_ru.tokenizer(text)]\n",
    "\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "\n",
    "BOS_WORD = \"<s>\"\n",
    "EOS_WORD = \"</s>\"\n",
    "BLANK_WORD = \"<blank>\"\n",
    "UNK_WORD = \"<unk>\"\n",
    "\n",
    "MAX_LEN = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ó–∞–≥—Ä—É–∑–∏–º –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ–µ–∫—Ç–∞ [Tatoeba üõ†Ô∏è[doc]](https://huggingface.co/datasets/tatoeba) –∏–∑ —Ä–∞–∑–¥–µ–ª–∞ [Datasets üõ†Ô∏è[doc]](https://huggingface.co/datasets) —Å–∞–π—Ç–∞ HuggingFace. –° —ç—Ç–∏–º –ø–æ–ª–µ–∑–Ω—ã–º —Ä–µ—Å—É—Ä—Å–æ–º –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –º—ã –ø–æ–∑–Ω–∞–∫–æ–º–∏–º—Å—è –ø–æ–¥—Ä–æ–±–Ω–µ–µ –Ω–∞ —Å–ª–µ–¥—É—é—â–∏—Ö –∑–∞–Ω—è—Ç–∏—è—Ö.\n",
    "\n",
    "–î–∞—Ç–∞—Å–µ—Ç Tatoeba ‚Äî —ç—Ç–æ –Ω–∞–±–æ—Ä –∏–∑ –ø–∞—Ä –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ. –ó–∞–≥—Ä—É–∑–∏–º –ø—Ä–∏–º–µ—Ä—ã –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º –∏ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–∞—Ö, —É–∫–∞–∑–∞–≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è `lang1` –∏ `lang2`. –î–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–µ—Ç–æ–¥ `load_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"tatoeba\", lang1=\"en\", lang2=\"ru\", trust_remote_code=True)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —è–≤–ª—è–µ—Ç—Å—è –æ–±—ä–µ–∫—Ç–æ–º –∫–ª–∞—Å—Å–∞ `DatasetDict`. –û–Ω –∏–º–µ–µ—Ç –∫–ª—é—á–∏ –∏ –∑–Ω–∞—á–µ–Ω–∏—è."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –æ–±—ä–µ–∫—Ç –¥–∞—Ç–∞—Å–µ—Ç–∞ –º–æ–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –ø–æ –∏–Ω–¥–µ–∫—Å—É."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–£–¥–∞–ª–∏–º –ø—Ä–∏–º–µ—Ä—ã –¥–ª–∏–Ω–µ–µ 100 —Ç–æ–∫–µ–Ω–æ–≤ –∏ —Ä–∞–∑–¥–µ–ª–∏–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = (\n",
    "    data[\"train\"]\n",
    "    .filter(\n",
    "        lambda x: max(len(x[\"translation\"][\"ru\"]), len(x[\"translation\"][\"en\"]))\n",
    "        <= MAX_LEN\n",
    "    )\n",
    "    .train_test_split(test_size=1000, shuffle=True, seed=2)\n",
    ")\n",
    "data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ë—É–¥–µ–º —Ä–µ—à–∞—Ç—å –∑–∞–¥–∞—á—É –ø–µ—Ä–µ–≤–æ–¥–∞ —Å —Ä—É—Å—Å–∫–æ–≥–æ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —è–∑—ã–∫.\n",
    "\n",
    "–ó–∞–ø–∏—à–µ–º –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞ –∏—Å—Ö–æ–¥–Ω–æ–º —è–∑—ã–∫–µ (source) –∏ –Ω–∞ —Ü–µ–ª–µ–≤–æ–º —è–∑—ã–∫–µ (target)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_src = [d[\"ru\"] for d in data2[\"train\"][\"translation\"]]\n",
    "train_trg = [d[\"en\"] for d in data2[\"train\"][\"translation\"]]\n",
    "\n",
    "val_src = [d[\"ru\"] for d in data2[\"test\"][\"translation\"]]\n",
    "val_trg = [d[\"en\"] for d in data2[\"test\"][\"translation\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of source sentences in train set: {len(train_src)} \")\n",
    "print(f\"Number of target sentences in train set: {len(train_trg)} \")\n",
    "print(f\"Example of source sentence in train set:\\n'{train_src[0]}'\")\n",
    "print(f\"Example of target sentence in train set:\\n'{train_trg[0]}'\\n\")\n",
    "print(f\"Number of source sentences in test set: {len(val_src)} \")\n",
    "print(f\"Number of target sentences in test set: {len(val_trg)} \")\n",
    "print(f\"Example of source sentence in test set:\\n'{val_src[0]}'\")\n",
    "print(f\"Example of target sentence in test set:\\n'{val_trg[0]}'\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–∞–ø–∏—à–µ–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Å–ª–æ–≤–∞—Ä—è —Ç–æ–∫–µ–Ω–æ–≤ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∏ —Ü–µ–ª–µ–≤–æ–≥–æ —è–∑—ã–∫–æ–≤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "def build_vocab(\n",
    "    texts,\n",
    "    tokenize,\n",
    "    min_freq=3,\n",
    "    init_token=BOS_WORD,\n",
    "    eos_token=EOS_WORD,\n",
    "    pad_token=BLANK_WORD,\n",
    "    unk_token=UNK_WORD,\n",
    "):\n",
    "    cnt = Counter()\n",
    "    for text in tqdm(texts):\n",
    "        cnt.update(tokenize(text))\n",
    "    vocab = [pad_token, init_token, eos_token, unk_token]\n",
    "    for w, c in cnt.most_common():\n",
    "        if c < min_freq:\n",
    "            break\n",
    "        vocab.append(w)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "src_vocab = build_vocab(train_src, tokenize_ru)\n",
    "tgt_vocab = build_vocab(train_trg, tokenize_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Size of source language vocabulary: {len(src_vocab)}\")\n",
    "print(f\"First 50 tokens in source language vocabulary:\\n{src_vocab[:50]}\\n\")\n",
    "print(f\"Size of target language vocabulary: {len(tgt_vocab)}\")\n",
    "print(f\"First 50 tokens in target language vocabulary:\\n{tgt_vocab[:50]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–æ–∑–¥–∞–¥–∏–º —Å–ª–æ–≤–∞—Ä—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –∏ –∏–Ω–¥–µ–∫—Å–æ–≤ –¥–ª—è –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –∏ —Ü–µ–ª–µ–≤–æ–≥–æ —è–∑—ã–∫–æ–≤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_voc_src = {w: i for i, w in enumerate(src_vocab)}\n",
    "inv_voc_tgt = {w: i for i, w in enumerate(tgt_vocab)}\n",
    "\n",
    "print(f\"Mapping word2id dictionary for source language: {inv_voc_src}\")\n",
    "print(f\"Mapping word2id dictionary for target language: {inv_voc_tgt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –∑–∞–º–µ–Ω–∏–≤ —Ç–æ–∫–µ–Ω—ã –Ω–∞ –∏–Ω–¥–µ–∫—Å—ã –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –Ω—É–∂–Ω—ã–º —Å–ª–æ–≤–∞—Ä–µ–º."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, tokenize_fn, inv_vocab, bos_id=1, eos_id=2, unk_id=3):\n",
    "    result = [bos_id]\n",
    "    for word in tokenize_fn(text):\n",
    "        if word in inv_vocab:\n",
    "            result.append(inv_vocab[word])\n",
    "        else:\n",
    "            result.append(unk_id)\n",
    "    result.append(eos_id)\n",
    "    return result\n",
    "\n",
    "\n",
    "train_src_tokenized = [tokenize(t, tokenize_ru, inv_voc_src) for t in tqdm(train_src)]\n",
    "train_tgt_tokenized = [tokenize(t, tokenize_en, inv_voc_tgt) for t in tqdm(train_trg)]\n",
    "\n",
    "val_src_tokenized = [tokenize(t, tokenize_ru, inv_voc_src) for t in tqdm(val_src)]\n",
    "val_tgt_tokenized = [tokenize(t, tokenize_en, inv_voc_tgt) for t in tqdm(val_trg)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"First sentence in source language train set after tokenization:\\n{train_src_tokenized[0]}\"\n",
    ")\n",
    "print(\n",
    "    f\"First sentence in target language train set after tokenization:\\n{train_tgt_tokenized[0]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–ø—Ä–µ–¥–µ–ª–∏–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –¥–ª–∏–Ω—ã –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –ø—É—Ç—ë–º –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(sequences, pad_id=0):\n",
    "    max_len = max(len(s) for s in sequences)\n",
    "    return [s + [pad_id] * (max_len - len(s)) for s in sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Example sentences in source language train set after tokenization:\\n\\\n",
    "{padding(train_src_tokenized[0:3])}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–µ–º–Ω–æ–≥–æ –ø–æ–º–µ–Ω—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –±–∞—Ç—á–∞ –∏ —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏—è."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    def __init__(self, src, trg, src_mask, trg_mask, ntokens):\n",
    "        self.src = src\n",
    "        self.trg = trg\n",
    "        self.src_mask = src_mask\n",
    "        self.trg_mask = trg_mask\n",
    "        self.ntokens = ntokens\n",
    "\n",
    "\n",
    "def make_std_mask(src, tgt, pad):\n",
    "    src_mask = (src != pad).unsqueeze(-2)\n",
    "    tgt_mask = (tgt != pad).unsqueeze(-2)\n",
    "    tgt_mask = tgt_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "    return src_mask, tgt_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–ø—Ä–µ–¥–µ–ª–∏–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è —Ä–∞–Ω–¥–æ–º–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm.auto import trange\n",
    "\n",
    "\n",
    "def data_iterator(srcs, tgts, batch_size=128, shuffle=True):\n",
    "    if shuffle:\n",
    "        pairs = list(zip(srcs, tgts))\n",
    "        random.shuffle(pairs)\n",
    "        srcs, tgts = [list(t) for t in zip(*pairs)]\n",
    "\n",
    "    for i in trange(0, len(srcs), batch_size):\n",
    "        x = torch.tensor(padding(srcs[i : i + batch_size]))\n",
    "        y = torch.tensor(padding(tgts[i : i + batch_size]))\n",
    "        src = Variable(x, requires_grad=False)\n",
    "        tgt = Variable(y, requires_grad=False)\n",
    "        src_mask, tgt_mask = make_std_mask(src, tgt, 0)\n",
    "        yield Batch(src, tgt, src_mask, tgt_mask, (tgt[1:] != 0).data.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –û–±—É—á–µ–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–æ–≤–µ—Ä–∏–º, –¥–æ—Å—Ç—É–ø–µ–Ω –ª–∏ –Ω–∞–º —É—Å–∫–æ—Ä–∏—Ç–µ–ª—å `torch.cuda` [üõ†Ô∏è[doc]](https://pytorch.org/docs/stable/notes/cuda.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–æ–∑–¥–∞–¥–∏–º –º–æ–¥–µ–ª—å —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –∏ –≤—ã–≤–µ–¥–µ–º –µ–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—É."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_code import get_std_opt\n",
    "\n",
    "# Create the model an load it onto our GPU\n",
    "pad_idx = tgt_vocab.index(\"<blank>\")\n",
    "model = make_model(len(src_vocab), len(tgt_vocab), N=6)\n",
    "model_opt = get_std_opt(model)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ—Å—á–∏—Ç–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model: nn.Module):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–£—Å—Ç–∞–Ω–æ–≤–∏–º —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –∏ –æ–ø—Ä–µ–¥–µ–ª–∏–º —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "criterion = LabelSmoothing(size=len(tgt_vocab), padding_idx=pad_idx, smoothing=0.1)\n",
    "criterion.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ß—Ç–æ–±—ã –æ–±—É—á–∏—Ç—å –Ω–∞—à—É –¥–æ–≤–æ–ª—å–Ω–æ –±–æ–ª—å—à—É—é –º–æ–¥–µ–ª—å, –æ–ø—Ä–µ–¥–µ–ª–∏–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ –æ—à–∏–±–∫–∏, –∫–æ—Ç–æ—Ä–∞—è –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –æ–±—É—á–µ–Ω–∏—è."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_backprop(generator, criterion, out, targets, normalize):\n",
    "    \"\"\"\n",
    "    Memory optmization. Compute each timestep separately and sum grads.\n",
    "    \"\"\"\n",
    "    assert out.size(1) == targets.size(1)\n",
    "    total = 0.0\n",
    "    out_grad = []\n",
    "    for i in range(out.size(1)):\n",
    "        out_column = Variable(out[:, i].data, requires_grad=True)\n",
    "        gen = generator(out_column)\n",
    "        loss = criterion(gen, targets[:, i]) / normalize\n",
    "        total += loss.item()  # .data[0]\n",
    "        loss.backward()\n",
    "        out_grad.append(out_column.grad.data.clone())\n",
    "    out_grad = torch.stack(out_grad, dim=1)\n",
    "    out.backward(gradient=out_grad)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–ø—Ä–µ–¥–µ–ª–∏–º —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(train_iter, model, criterion, opt, transpose=False):\n",
    "    model.train()\n",
    "    for i, batch in enumerate(train_iter):\n",
    "        src, trg, src_mask, trg_mask = (\n",
    "            batch.src.to(device),\n",
    "            batch.trg.to(device),\n",
    "            batch.src_mask.to(device),\n",
    "            batch.trg_mask.to(device),\n",
    "        )\n",
    "        out = model.forward(src, trg[:, :-1], src_mask, trg_mask[:, :-1, :-1])\n",
    "        loss = loss_backprop(model.generator, criterion, out, trg[:, 1:], batch.ntokens)\n",
    "\n",
    "        model_opt.step()\n",
    "        model_opt.optimizer.zero_grad()\n",
    "        if i % 10 == 1:\n",
    "            print(i, loss, model_opt._rate)\n",
    "\n",
    "\n",
    "def valid_epoch(valid_iter, model, criterion, transpose=False):\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    for batch in valid_iter:\n",
    "        src, trg, src_mask, trg_mask = (\n",
    "            batch.src.to(device),\n",
    "            batch.trg.to(device),\n",
    "            batch.src_mask.to(device),\n",
    "            batch.trg_mask.to(device),\n",
    "        )\n",
    "        out = model.forward(src, trg[:, :-1], src_mask, trg_mask[:, :-1, :-1])\n",
    "        loss = loss_backprop(model.generator, criterion, out, trg[:, 1:], batch.ntokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∑–∞–Ω–∏–º–∞–µ—Ç –æ–∫–æ–ª–æ —á–∞—Å–∞. –ï–≥–æ –º–æ–∂–Ω–æ –æ—Å—É—â–µ—Å—Ç–≤–∏—Ç—å —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:\n",
    "\n",
    "```python\n",
    "for epoch in trange(3):\n",
    "    train_epoch(data_iterator(train_src_tokenized, train_tgt_tokenized, batch_size=BATCH_SIZE), model, criterion, model_opt)\n",
    "    valid_epoch(data_iterator(val_src_tokenized, val_tgt_tokenized, batch_size=BATCH_SIZE), model, criterion)\n",
    "\n",
    "torch.save(model.state_dict(), \"model_weights.pth\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í —Ü–µ–ª—è—Ö —ç–∫–æ–Ω–æ–º–∏–∏ –≤—Ä–µ–º–µ–Ω–∏ –∏ —Ä–µ—Å—É—Ä—Å–æ–≤ –∑–∞–≥—Ä—É–∑–∏–º –≤–µ—Å–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/datasets/ru_en_model.zip -P /content/weights\n",
    "!unzip -q /content/weights/ru_en_model.zip -d ./weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model(len(src_vocab), len(tgt_vocab), N=6)\n",
    "model.load_state_dict(torch.load(\"weights/model_weights.pth\"))\n",
    "model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–∏–º–µ–Ω–∏–º –º–æ–¥–µ–ª—å –∫ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω–æ–º—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—é."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "sent = \"–í—ã –º–æ–∂–µ—Ç–µ –ø–µ—Ä–µ–≤–µ—Å—Ç–∏ —ç—Ç–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ?\"\n",
    "src = Variable(torch.LongTensor([tokenize(sent, tokenize_ru, inv_voc_src)]))\n",
    "src_mask = (src != src_vocab.index(\"<blank>\")).unsqueeze(-2)\n",
    "print(f\"Tokenized source sentence:\\n{src}\")\n",
    "print(f\"Mask for tokenized source sentence:\\n{src_mask}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = decode(\n",
    "    model,\n",
    "    src.to(device),\n",
    "    src_mask.to(device),\n",
    "    max_len=60,\n",
    "    start_symbol=tgt_vocab.index(\"<s>\"),\n",
    ")\n",
    "print(f\"Tokenized translated sentence:\\n{out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subword_punctuation = [\",\", \".\", \"!\", \"?\", \"n't\", \"'s\", \"'ve\", \"'re\", \"'m\", \"'d\", \"'ll\"]\n",
    "trans = \"\"\n",
    "for i in range(1, out.size(1)):\n",
    "    sym = tgt_vocab[out[0, i]]\n",
    "    if sym == \"</s>\":\n",
    "        break\n",
    "    if i != 1 and sym not in subword_punctuation:\n",
    "        trans += \" \"\n",
    "    trans += sym\n",
    "print(f\"Translation: {trans}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ß—Ç–æ–±—ã –æ—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏, –Ω—É–∂–Ω–æ –∑–∞–ø–∏—Å–∞—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è –≤—Å–µ—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–∏.\n",
    "\n",
    "–ü–æ–ø—Ä–æ–±—É–µ–º —Å–¥–µ–ª–∞—Ç—å —ç—Ç–æ –¥–ª—è 20 –ø–µ—Ä–≤—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(source_sentences, subword_punctuation):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    for sentence in source_sentences:\n",
    "        src = torch.LongTensor(sentence).unsqueeze(0)\n",
    "        src_mask = (src != src_vocab.index(\"<blank>\")).unsqueeze(-2)\n",
    "        out = decode(\n",
    "            model,\n",
    "            src.to(device),\n",
    "            src_mask.to(device),\n",
    "            max_len=60,\n",
    "            start_symbol=tgt_vocab.index(\"<s>\"),\n",
    "        )\n",
    "        trans = \"\"\n",
    "        for i in range(1, out.size(1)):\n",
    "            sym = tgt_vocab[out[0, i]]\n",
    "            if sym == \"</s>\":\n",
    "                break\n",
    "            if i != 1 and sym not in subword_punctuation:\n",
    "                trans += \" \"\n",
    "            trans += sym\n",
    "        predictions.append(trans)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = inference(val_src_tokenized[:20], subword_punctuation)\n",
    "for i in range(20):\n",
    "    print(f\"Source sentence: {val_src[i]}\")\n",
    "    print(f\"Translation: {predictions[i]}\")\n",
    "    print(f\"Target sentence: {val_trg[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–∏–º–µ–Ω–∏—Ç—å –º–æ–¥–µ–ª—å –∫–æ –≤—Å–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º –º–æ–∂–Ω–æ —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:\n",
    "\n",
    "```python\n",
    "val_predictions = inference(val_src_tokenized, subword_punctuation)\n",
    "with open(r\"val_predictions.txt\", \"w\") as file:\n",
    "    for line in val_predictions:\n",
    "        file.write(line + '\\n')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–¥–Ω–∞–∫–æ —ç—Ç–æ –∑–∞–π–º–µ—Ç –æ–∫–æ–ª–æ 20 –º–∏–Ω—É—Ç, –ø–æ—ç—Ç–æ–º—É –∑–∞–≥—Ä—É–∑–∏–º –≥–æ—Ç–æ–≤—ã–π —Ñ–∞–π–ª —Å –ø–µ—Ä–µ–≤–æ–¥–∞–º–∏:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/datasets/val_predictions.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions = []\n",
    "with open(\"val_predictions.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        if line != \"\":\n",
    "            val_predictions.append(line.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ—Å—á–∏—Ç–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ BLEU –ø—Ä–∏ —Ä–∞–∑–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö $n$-–≥—Ä–∞–º–º."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torcheval -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torcheval\n",
    "from torcheval.metrics.functional import bleu_score\n",
    "\n",
    "for n in range(2, 5):\n",
    "    bleu = bleu_score(val_predictions, val_trg, n_gram=n)\n",
    "    print(f\"BLEU ({n}-gram)= {bleu.item():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–µ—Ç—Ä–∏–∫–∞ BLEU –±—ã–ª–∞ –ø–æ—Å—á–∏—Ç–∞–Ω–∞ –¥–ª—è –º–æ–¥–µ–ª–µ–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –∏ –º–æ–¥–µ–ª–µ–π –ø—Ä–µ–¥—à–µ—Å—Ç–≤—É—é—â–µ–≥–æ –ø–æ–∫–æ–ª–µ–Ω–∏—è –ø—Ä–∏ –ø–µ—Ä–µ–≤–æ–¥–µ —Å –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –Ω–∞ —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π (EN-FR) –∏ —Å –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –Ω–∞ –Ω–µ–º–µ—Ü–∫–∏–π (EN-DE). –î–ª—è –æ—Ü–µ–Ω–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –¥–∞–Ω–Ω—ã–µ –≤–æ—Ä–∫—à–æ–ø–∞ –ø–æ –º–∞—à–∏–Ω–Ω–æ–º—É –ø–µ—Ä–µ–≤–æ–¥—É [WMT üõ†Ô∏è[doc]](https://paperswithcode.com/dataset/wmt-2014) (Workshop on Statistical Machine Translation) 2014 –≥–æ–¥–∞.\n",
    "\n",
    "–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ –ø—Ä–∏ –ø–µ—Ä–µ–≤–æ–¥–µ —Å –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –Ω–∞ –Ω–µ–º–µ—Ü–∫–∏–π –º–æ–¥–µ–ª—å Transformer (big) –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –ª—É—á—à–∏–µ —Ä–∞–Ω–µ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ (–≤–∫–ª—é—á–∞—è –∞–Ω—Å–∞–º–±–ª–∏) –±–æ–ª–µ–µ —á–µ–º –Ω–∞ 2,0 –±–∞–ª–ª–∞ –ø–æ —à–∫–∞–ª–µ BLEU, —á—Ç–æ –¥–∞–µ—Ç –Ω–æ–≤—ã–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å BLEU –≤ 28,4 –±–∞–ª–ª–∞. –û–±—É—á–µ–Ω–∏–µ –Ω–∞ 8 –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞—Ö P100 –∑–∞–Ω—è–ª–æ 3,5 –¥–Ω—è. –î–∞–∂–µ Transformer (base model) –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç –≤—Å–µ —Ä–∞–Ω–µ–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏ –∞–Ω—Å–∞–º–±–ª–∏, –ø—Ä–∏ —ç—Ç–æ–º —Å—Ç–æ–∏–º–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –Ω–∏–∂–µ, —á–µ–º —É –ª—é–±–æ–π –∏–∑ –∫–æ–Ω–∫—É—Ä–∏—Ä—É—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π. –ü—Ä–∏ –ø–µ—Ä–µ–≤–æ–¥–µ —Å –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ –Ω–∞ —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏–π –º–æ–¥–µ–ª—å Transformer (big) –Ω–∞–±—Ä–∞–ª–∞ 41,0 –±–∞–ª–ª –ø–æ —à–∫–∞–ª–µ BLEU, –ø—Ä–µ–≤–∑–æ–π–¥—è –≤—Å–µ —Ä–∞–Ω–µ–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L05/wmt_table.jpg\" width=\"650\"><center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://arxiv.org/pdf/1706.03762\">Tensor2Tensor for Neural Machine Translation</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú—ã –º–æ–∂–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç, —á—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å, —á—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –Ω–∞ –∫–∞–∂–¥–æ–º —É—Ä–æ–≤–Ω–µ –≤–Ω–∏–º–∞–Ω–∏—è. –î–ª—è —ç—Ç–æ–≥–æ –Ω—É–∂–Ω–æ –ø–æ–ª—É—á–∏—Ç—å –≤–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è, –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –∏–∑ —ç–Ω–∫–æ–¥–µ—Ä–∞ –∏ –¥–µ–∫–æ–¥–µ—Ä–∞.\n",
    "\n",
    "–í—Å–ø–æ–º–Ω–∏–º, —á—Ç–æ –±–ª–æ–∫ —ç–Ω–∫–æ–¥–µ—Ä–∞ –∏ –±–ª–æ–∫ –¥–µ–∫–æ–¥–µ—Ä–∞ —Å–æ—Å—Ç–æ—è—Ç –∏–∑ 6 —Å–ª–æ–µ–≤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Encoder layers:\\n{model.encoder.layers}\")\n",
    "print(f\"Decoder layers:\\n{model.decoder.layers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–æ–∂–µ–º –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞ –ø–æ–ª—É—á–∏—Ç—å –≤–µ—Å–∞ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ—è."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Encoder Layer 0:\\n \\\n",
    "{model.encoder.layers[0].self_attn}\"\n",
    ")\n",
    "print(\n",
    "    f\"Decoder Layer 0:\\n \\\n",
    "{model.decoder.layers[0].self_attn}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ—Å–∫–æ–ª—å–∫—É –º—ã –∏—Å–ø–æ–ª—å–∑—É–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ, –∫–∞–∂–¥—ã–π —Å–ª–æ–π —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∞—Ç—Ä–∏—Ü –≤–Ω–∏–º–∞–Ω–∏—è. –ü–æ–ø—Ä–æ–±—É–µ–º –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ –ø–µ—Ä–≤–æ–π –≥–æ–ª–æ–≤–µ –≤–Ω–∏–º–∞–Ω–∏—è."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Encoder Layer 0 Attention Head 0:\\n \\\n",
    "{model.encoder.layers[0].self_attn.linears[0]}\"\n",
    ")\n",
    "print(\n",
    "    f\"Decoder Layer 0 Attention Head 0:\\n \\\n",
    "{model.decoder.layers[0].self_attn.linears[0]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –Ω—É–∂–Ω–æ –æ–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ –∞—Ç—Ä–∏–±—É—Ç—É `weight`. –ß—Ç–æ–±—ã –æ—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è, –Ω—É–∂–Ω–æ –≤—ã–∑–≤–∞—Ç—å –∞—Ç—Ä–∏–±—É—Ç `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Encoder Layer 0 Attention Head 0 attention weights:\\n \\\n",
    "{model.encoder.layers[0].self_attn.linears[0].weight.data}\"\n",
    ")\n",
    "print(\n",
    "    f\"Decoder Layer 0 Attention Head 0 attention weights:\\n \\\n",
    "{model.decoder.layers[0].self_attn.linears[0].weight.data}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –Ω–∞ —Ç–µ–ø–ª–æ–≤–æ–π –∫–∞—Ä—Ç–µ –Ω–∞–º –Ω—É–∂–Ω—ã —Ç–æ–ª—å–∫–æ –≤–µ—Å–∞, —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Ç–æ–∫–µ–Ω–∞–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –ø–æ—ç—Ç–æ–º—É –º—ã –æ–≥—Ä–∞–Ω–∏—á–∏–º —Ä–∞–∑–º–µ—Ä –º–∞—Ç—Ä–∏—Ü.\n",
    "\n",
    "–í—ã–≤–µ–¥–µ–º –º–∞—Ç—Ä–∏—Ü—ã –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ—è —ç–Ω–∫–æ–¥–µ—Ä–∞ –∏ –¥–µ–∫–æ–¥–µ—Ä–∞ –¥–ª—è –≤—Å–µ—Ö –≥–æ–ª–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è –≤ —Ç—Ä–µ—Ö –≤–∞—Ä–∏–∞–Ω—Ç–∞—Ö:\n",
    "- –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ –≤–Ω–∏–º–∞–Ω–∏–µ —ç–Ω–∫–æ–¥–µ—Ä–∞\n",
    "- –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –¥–µ–∫–æ–¥–µ—Ä–∞\n",
    "- –≤–Ω–∏–º–∞–Ω–∏–µ –∏–∑ —ç–Ω–∫–æ–¥–µ—Ä–∞ –≤ –¥–µ–∫–æ–¥–µ—Ä"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn\n",
    "\n",
    "seaborn.set_context(context=\"talk\")\n",
    "\n",
    "src_sent, tgt_sent = sent.split(), trans.split()\n",
    "\n",
    "\n",
    "def draw(data, x, y, ax):\n",
    "    seaborn.heatmap(data, xticklabels=x, square=True, yticklabels=y, cbar=False, ax=ax)\n",
    "\n",
    "\n",
    "layer = 0\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "print(\"Encoder Layer\", layer + 1)\n",
    "for h in range(4):\n",
    "    draw(\n",
    "        model.encoder.layers[layer]\n",
    "        .self_attn.linears[h]\n",
    "        .weight.data[: len(src_sent), : len(src_sent)]\n",
    "        .cpu(),\n",
    "        src_sent,\n",
    "        src_sent if h == 0 else [],\n",
    "        ax=axs[h],\n",
    "    )\n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "print(\"Decoder Self Layer\", layer + 1)\n",
    "for h in range(4):\n",
    "    draw(\n",
    "        model.decoder.layers[layer]\n",
    "        .self_attn.linears[h]\n",
    "        .weight.data[: len(tgt_sent), : len(tgt_sent)]\n",
    "        .cpu(),\n",
    "        tgt_sent,\n",
    "        tgt_sent if h == 0 else [],\n",
    "        ax=axs[h],\n",
    "    )\n",
    "plt.show()\n",
    "print(\"Decoder Src Layer\", layer + 1)\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "for h in range(4):\n",
    "    draw(\n",
    "        model.decoder.layers[layer]\n",
    "        .self_attn.linears[h]\n",
    "        .weight.data[: len(tgt_sent), : len(src_sent)]\n",
    "        .cpu(),\n",
    "        src_sent,\n",
    "        tgt_sent if h == 0 else [],\n",
    "        ax=axs[h],\n",
    "    )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –†–∞–∑–≤–∏—Ç–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–≥–æ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞ —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞ –∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞. –≠—Ç–∏ –±–ª–æ–∫–∏ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ –æ—Ç–¥–µ–ª—å–Ω–æ—Å—Ç–∏:\n",
    "- –ú–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –º–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ (Generative Pre-trained Transformers, GPT)\n",
    "- –ú–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è –¥–ª—è –¥—Ä—É–≥–∏—Ö –∑–∞–¥–∞—á: –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –æ–¥–Ω–æ–≥–æ –∏–ª–∏ –ø–∞—Ä—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, —Ç–µ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –ø–æ–∏—Å–∫–∞ –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å (Bidirectional Encoder Representations from Transformers, BERT)\n",
    "\n",
    "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —ç—Ç–∏—Ö –º–æ–¥–µ–ª–µ–π –º—ã –ø–æ–¥—Ä–æ–±–Ω–æ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –Ω–∞ —Å–ª–µ–¥—É—é—â–∏—Ö –¥–≤—É—Ö –∑–∞–Ω—è—Ç–∏—è—Ö."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L05/transformer_models.PNG\" width=\"800\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://medium.com/@reyhaneh.esmailbeigi/bert-gpt-and-bart-a-short-comparison-5d6a57175fca\">BERT, GPT and BART: a short comparison</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞ –∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –∑–∞–¥–∞—á–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞ ‚Äî –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è –æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –≤ –¥—Ä—É–≥—É—é, –ø—Ä–∏ —ç—Ö—Ç–æ–º –¥–ª–∏–Ω–∞ –≤—Ö–æ–¥–Ω–æ–π –∏ –≤—ã—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –º–æ–∂–µ—Ç –Ω–µ —Å–æ–≤–ø–∞–¥–∞—Ç—å (sequence-to-sequence).\n",
    "\n",
    "–ü–æ–º–∏–º–æ –º–∞—à–∏–Ω–Ω–æ–≥–æ –ø–µ—Ä–µ–≤–æ–¥–∞, –∫ –∑–∞–¥–∞—á–∞–º sequence-to-sequence –æ—Ç–Ω–æ—Å—è—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–µ:\n",
    "- –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è\n",
    "- –ü–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
    "- –ü–µ—Ä–µ–Ω–æ—Å —Å—Ç–∏–ª—è —Ç–µ–∫—Å—Ç–∞\n",
    "- –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤ –≤ –¥–∏–∞–ª–æ–≥–µ\n",
    "- –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ–ø–µ—á–∞—Ç–æ–∫\n",
    "- –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ —Ç–µ–∫—Å—Ç (—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö, –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π, –∑–∞–ø–∏—Å–µ–π —Ä–µ—á–∏).\n",
    "\n",
    "–í —ç—Ç–æ–º —Ä–∞–∑–¥–µ–ª–µ —Ä–∞—Å—Å–º–æ—Ç—Ä–∏–º –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞ –∏ –¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–∞: BART –∏ T5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BART"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BART ‚Äî sequence-to-sequence –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä, –∫–æ—Ç–æ—Ä—ã–π –ø—Ä–µ–¥–æ–±—É—á–∞–µ—Ç—Å—è —Ä–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏—Å–ø–æ—Ä—á–µ–Ω–Ω–æ–≥–æ –∑–∞—à—É–º–ª—ë–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞. –ù–∞ –≤—Ö–æ–¥–µ —É –º–æ–¥–µ–ª–∏ –∫–∞–∫–∏–º-—Ç–æ –æ–±—Ä–∞–∑–æ–º –∏—Å–ø–æ—Ä—á–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç, –∞ –Ω–∞ –≤—ã—Ö–æ–¥–µ –µ–π –Ω–∞–¥–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –µ–≥–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –≤–µ—Ä—Å–∏—é.\n",
    "\n",
    "[[paper] üéì Lewis M. et al. (2019). BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)\n",
    "\n",
    "–°–ø–æ—Å–æ–±—ã –∑–∞—à—É–º–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞:\n",
    "\n",
    "- –ú–∞—Å–∫–∏—Ä–æ–≤–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤\n",
    "- –£–¥–∞–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤\n",
    "- –ú–∞—Å–∫–∏—Ä–æ–≤–∫–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–æ–¥—Ä—è–¥ –∏–¥—É—â–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤ –æ–¥–Ω–∏–º —Å–ø–µ—Ü—Ç–æ–∫–µ–Ω–æ–º [MASK]\n",
    "- –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π\n",
    "- \"–í—Ä–∞—â–µ–Ω–∏–µ\" –¥–æ–∫—É–º–µ–Ω—Ç–∞ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Å–ª—É—á–∞–π–Ω–æ–≥–æ —Ç–æ–∫–µ–Ω–∞\n",
    "  - –¥–µ–ª–∞–µ–º —ç—Ç–æ—Ç —Ç–æ–∫–µ–Ω –Ω–∞—á–∞–ª–æ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞, –∞ –≤—Å—ë, —á—Ç–æ –±—ã–ª–æ –¥–æ –Ω–µ–≥–æ, –ø–µ—Ä–µ–Ω–æ—Å–∏–º –≤ –∫–æ–Ω–µ—Ü"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L05/bart.png\" width=\"500\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://arxiv.org/abs/1910.13461\">BART: Denoising Sequence-to-Sequence Pre-training</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö—Ä–æ–º–µ –∞–Ω–≥–ª–∏–π—Å–∫–æ–π –≤–µ—Ä—Å–∏–∏ BART –±—ã–ª–∞ –æ–±—É—á–µ–Ω–∞ –µ—â—ë –∏ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –≤–µ—Ä—Å–∏—è —ç—Ç–æ–π –º–æ–¥–µ–ª–∏, mBART. –ü—Ä–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –æ–¥–Ω–æ—è–∑—ã—á–Ω—ã–µ —á–∞—Å—Ç–∏ –∫–æ—Ä–ø—É—Å–∞, —Ç–æ –µ—Å—Ç—å –Ω–∏–∫–∞–∫–∏—Ö –ø–µ—Ä–µ–≤–æ–¥–æ–≤ –º–æ–¥–µ–ª—å –Ω–µ –≤–∏–¥–µ–ª–∞. –†—É—Å—Å–∫–∏–π —è–∑—ã–∫ ‚Äî –≤—Ç–æ—Ä–æ–π –ø–æ —Å—Ç–µ–ø–µ–Ω–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ—Å—Ç–∏ –ø–æ—Å–ª–µ –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç—É –º–æ–¥–µ–ª—å –∏ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ —Ç–æ–∂–µ.\n",
    "\n",
    "[[paper] üéì Liu Y. et al. (2020). Multilingual Denoising Pre-training for Neural Machine Translation](https://arxiv.org/abs/2001.08210)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "T5 ‚Äî –µ—â—ë –æ–¥–Ω–∞ sequence-to-sequence –º–æ–¥–µ–ª—å. –ï—ë –Ω–∞–∑–≤–∞–Ω–∏–µ —Ä–∞—Å—à–∏—Ñ—Ä–æ–≤—ã–≤–∞–µ—Ç—Å—è  –∫–∞–∫ text-to-text transfer transformer.\n",
    "\n",
    "[[paper] üéì Raffel C. (2019). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683)\n",
    "\n",
    "Text-to-text –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –º–æ–¥–µ–ª—å T5 –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ —Ç–µ–∫—Å—Ç—ã –∏ \"—á–∏—Ç–∞–µ—Ç\" –∏—Ö –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–æ–º, –∞ –ø–æ—Ç–æ–º \"–ø–∏—à–µ—Ç\" –¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫–æ–º –Ω–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã –∏ –æ—Ç–¥–∞—ë—Ç –Ω–∞ –≤—ã—Ö–æ–¥. –°–ª–æ–≤–æ transfer –≥–æ–≤–æ—Ä–∏—Ç –æ —Ü–µ–ª–∏ —ç—Ç–æ–π –º–æ–¥–µ–ª–∏: –æ–Ω–∞ –ø—Ä–µ–¥–æ–±—É—á–∞–ª–∞—Å—å –≤–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Ç–µ–∫—Å—Ç–∞. –ü—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –ø—Ä–æ–º–µ–∂—É—Ç–∫–∏ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å–∫—Ä—ã–≤–∞—é—Ç—Å—è, –∏ –∑–∞–¥–∞—á–∞ –º–æ–¥–µ–ª–∏ ‚Äî –∏—Ö —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å. –í –æ—Ç–ª–∏—á–∏–µ –æ—Ç BART, –≥–¥–µ —Ç–µ–∫—Å—Ç –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç—Å—è —Ü–µ–ª–∏–∫–æ–º, T5 –Ω—É–∂–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ —Å–∞–º–∏ —Å–∫—Ä—ã—Ç—ã–µ –ø—Ä–æ–º–µ–∂—É—Ç–∫–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L05/t5.png\" width=\"400\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://arxiv.org/abs/1910.10683\">Text-to-Text Transformer</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ mBART, –∫—Ä–æ–º–µ –∞–Ω–≥–ª–∏–π—Å–∫–æ–π –≤–µ—Ä—Å–∏–∏ T5 –±—ã–ª–∞ –æ–±—É—á–µ–Ω–∞ –µ—â—ë –∏ –º–Ω–æ–≥–æ—è–∑—ã—á–Ω–∞—è –≤–µ—Ä—Å–∏—è —ç—Ç–æ–π –º–æ–¥–µ–ª–∏, mT5.\n",
    "\n",
    "[[paper] üéì Xue L. (2020). mT5: A massively multilingual pre-trained text-to-text transformer](https://arxiv.org/abs/2010.11934)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–¥–∏–Ω –∏–∑ –∫–ª—é—á–µ–≤—ã—Ö —ç—Ç–∞–ø–æ–≤ –≤ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–∫—Å—Ç–∞ ‚Äî **—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è**. –ù–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –µ–¥–∏–Ω–∏—Ü—ã ‚Äî –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ —Å–ª–æ–≤–∞. –ó–∞—Ç–µ–º —Å–æ–∑–¥–∞–µ—Ç—Å—è —Å–ª–æ–≤–∞—Ä—å, –≤ –∫–æ—Ç–æ—Ä—ã–π –∑–∞–Ω–æ—Å—è—Ç—Å—è —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ª–µ–∫—Å–µ–º—ã, –≤—Å—Ç—Ä–µ—Ç–∏–≤—à–∏–µ—Å—è –≤ –∫–æ—Ä–ø—É—Å–µ –∏–ª–∏ —Ç–µ–∫—Å—Ç–µ. –ù–∞ —ç—Ç–∏—Ö —ç—Ç–∞–ø–∞—Ö –º–æ–∂–Ω–æ —Å—Ç–æ–ª–∫–Ω—É—Ç—å—Å—è —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –ø—Ä–æ–±–ª–µ–º–∞–º–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–ü—Ä–æ–±–ª–µ–º–∞ 1. –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è**\n",
    "\n",
    "–°–∞–º—ã–π –ø—Ä–æ—Å—Ç–æ–π —Å–ø–æ—Å–æ–± —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ ‚Äî –Ω–∞–∑–Ω–∞—á–∏—Ç—å –∫–∞–∂–¥–æ–º—É —É–Ω–∏–∫–∞–ª—å–Ω–æ–º—É —Å–ª–æ–≤—É —Å–≤–æ—ë —á–∏—Å–ª–æ. –ù–æ –µ—Å—Ç—å –ø—Ä–æ–±–ª–µ–º–∞: —Å–ª–æ–≤ –∏ –∏—Ö —Ñ–æ—Ä–º –º–∏–ª–ª–∏–æ–Ω—ã, –∏ –ø–æ—ç—Ç–æ–º—É —Å–ª–æ–≤–∞—Ä—å —Ç–∞–∫–∏—Ö —Å–ª–æ–≤ –ø–æ–ª—É—á–∏—Ç—Å—è —á–µ—Ä–µ—Å—á—É—Ä –±–æ–ª—å—à–∏–º, –∞ —ç—Ç–æ –±—É–¥–µ—Ç –∑–∞—Ç—Ä—É–¥–Ω—è—Ç—å –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "–ú–æ–∂–Ω–æ —Ä–∞–∑–±–∏–≤–∞—Ç—å —Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞ —Å–ª–æ–≤–∞, –∞ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –±—É–∫–≤—ã (char-level tokenization), —Ç–æ–≥–¥–∞ –≤ —Å–ª–æ–≤–∞—Ä–µ –±—É–¥–µ—Ç –≤—Å–µ–≥–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–µ—Å—è—Ç–∫–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤, –ù–û –≤ —Ç–∞–∫–æ–º —Å–ª—É—á–∞–µ —É–∂–µ —Å–∞–º —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –±—É–¥–µ—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–º, –∞ —ç—Ç–æ —Ç–æ–∂–µ –∑–∞—Ç—Ä—É–¥–Ω—è–µ—Ç –æ–±—É—á–µ–Ω–∏–µ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–ü—Ä–æ–±–ª–µ–º–∞ 2. –ë–æ–≥–∞—Ç–∞—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—è**\n",
    "\n",
    "\"–ù–µ–π—Ä–æ—Å–µ—Ç—å\", \"–Ω–µ–π—Ä–æ—Å–µ—Ç—å—é\", \"–Ω–µ–π—Ä–æ—Å–µ—Ç—è–º–∏\" —è–≤–ª—è—é—Ç—Å—è —Ä–∞–∑–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏, –Ω–æ –∏–º–µ—é—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—ã–π —Å–º—ã—Å–ª. –≠—Ç—É –ø—Ä–æ–±–ª–µ–º—É –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏ –≤—Å–µ–≥–¥–∞ —Ä–µ—à–∞–ª —ç—Ç–∞–ø **—Å—Ç–µ–º–º–∏–Ω–≥–∞** (—É–¥–∞–ª–µ–Ω–∏–µ —Å—É—Ñ—Ñ–∏–∫—Å–∞ –∏ –æ–∫–æ–Ω—á–∞–Ω–∏—è) –∏–ª–∏ **–ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏–∏** (–ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ —Å–ª–æ–≤–∞ –∫ –∫–∞–Ω–æ–Ω–∏—á–µ—Å–∫–æ–π —Ñ–æ—Ä–º–µ)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–ü—Ä–æ–±–ª–µ–º–∞ 3. –°–ª–æ–∂–Ω—ã–µ —Å–ª–æ–≤–∞**\n",
    "\n",
    "–ù–æ –≤—Å–µ –ø—Ä–æ–±–ª–µ–º—ã —ç—Ç–∏ —ç—Ç–∞–ø—ã –Ω–µ —Ä–µ—à–∞—é—Ç. –í –≥–µ—Ä–º–∞–Ω—Å–∫–∏—Ö —è–∑—ã–∫–∞—Ö (–≤ –Ω–µ–º–µ—Ü–∫–æ–º, —à–≤–µ–¥—Å–∫–æ–º, –¥–∞—Ç—Å–∫–æ–º) –æ—á–µ–Ω—å –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ –æ–±—Ä–∞–∑–æ–≤—ã–≤–∞—é—Ç—Å—è –Ω–æ–≤—ã–µ —Å–ª–æ–∂–Ω—ã–µ —Å–ª–æ–≤–∞. –ó–Ω–∞—á–µ–Ω–∏—è —Ç–∞–∫–∏—Ö —Å–ª–æ–≤ –≤—ã–≤–æ–¥—è—Ç—Å—è –∏–∑ –∑–Ω–∞—á–µ–Ω–∏—è –∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤. –ò—Ö –º–æ–∂–Ω–æ —Å–æ–∑–¥–∞–≤–∞—Ç—å –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ –¥–æ–ª–≥–æ, –∏ –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –∏–∑ –Ω–∏—Ö –Ω–µ –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–æ –≤ ¬´–±—É–º–∞–∂–Ω–æ–º¬ª —Å–ª–æ–≤–∞—Ä–µ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L05/swedish_word_example.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em>–ü—Ä–∏–º–µ—Ä —à–≤–µ–¥—Å–∫–æ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è –≥–∞–µ—á–Ω–æ–≥–æ –∫–ª—é—á–∞ –¥–ª—è –∫–æ–ª–µ—Å–∞ –º–æ—Ç–æ—Ü–∏–∫–ª–∞</a></em></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://sysblok.ru/nlp/7250/\">–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å —ç—Ç–∏–º–∏ —è–∑—ã–∫–∞–º–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç—å —Ç–∞–∫–∂–µ –≤–æ–∑–Ω–∏–∫–∞–µ—Ç –Ω–∞ —ç—Ç–∞–ø–µ —Å–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å–ª–æ–≤–∞—Ä—è. –ü—Ä–∏ —Å–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–∏ —Å–ª–æ–≤–∞—Ä—è –º–æ–¥–µ–ª–∏ –æ—Ä–∏–µ–Ω—Ç–∏—Ä—É—é—Ç—Å—è –Ω–∞ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ª–æ–≤–æ, –µ—Å–ª–∏ –æ–Ω–æ –≤—Å—Ç—Ä–µ—Ç–∏–ª–æ—Å—å —á–∞—â–µ –ø—è—Ç–∏ —Ä–∞–∑), –ø–æ—ç—Ç–æ–º—É –Ω–µ –±—É–¥—É—Ç –∑–∞–ø–æ–º–∏–Ω–∞—Ç—å —Ç–∞–∫–æ–µ –¥–ª–∏–Ω–Ω–æ–µ –∏ —Å–ª–æ–∂–Ω–æ–µ —Å–ª–æ–≤–æ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–ü—Ä–æ–±–ª–µ–º–∞ 4: –ì—Ä–∞–Ω–∏—Ü—ã —Å–ª–æ–≤–∞**\n",
    "\n",
    "–î–ª—è –Ω–∞—Å, –ø—Ä–∏–≤—ã–∫—à–∏—Ö –∫ —è–∑—ã–∫–∞–º –µ–≤—Ä–æ–ø–µ–π—Å–∫–æ–≥–æ —Ç–∏–ø–∞, —Å–ª–æ–≤–æ ‚Äî —ç—Ç–æ –Ω–∞–±–æ—Ä –±—É–∫–≤ –º–µ–∂–¥—É –ø—Ä–æ–±–µ–ª–∞–º–∏ –∏ –∑–Ω–∞–∫–∞–º–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è. –ù–æ –≤ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ –º–Ω–æ–≥–∏–µ —Å–ª–æ–∂–Ω—ã–µ —Å–ª–æ–≤–∞ –ø–∏—à—É—Ç—Å—è —Ä–∞–∑–¥–µ–ª—å–Ω–æ, –∞ –≤ —è–ø–æ–Ω—Å–∫–æ–º, –Ω–∞–æ–±–æ—Ä–æ—Ç, –º–µ–∂–¥—É —Å–ª–æ–≤–∞–º–∏ –≤–æ–æ–±—â–µ –Ω–µ—Ç –ø—Ä–æ–±–µ–ª–æ–≤. –ü–æ—ç—Ç–æ–º—É —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä —Å–æ–∑–¥–∞—Ç—å –±—ã–ª–æ –Ω–µ–ª–µ–≥–∫–æ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE-—Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Byte Pair Encoding (BPE) ‚Äî –∞–ª–≥–æ—Ä–∏—Ç–º –¥–ª—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –Ω–∞ –ø–æ–¥—Å–ª–æ–≤–∞. –û–Ω –ø–æ–∑–≤–æ–ª—è–µ—Ç —ç–∫—Å–ø–ª–∏—Ü–∏—Ç–Ω–æ –∑–∞–¥–∞–≤–∞—Ç—å —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è. –≠—Ç–æ—Ç —Ç–∏–ø —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤–æ –º–Ω–æ–≥–∏—Ö –º–æ–¥–µ–ª—è—Ö —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤, –≤–∫–ª—é—á–∞—è GPT, GPT-2, RoBERTa, BART.\n",
    "\n",
    "–ï—Å—Ç—å –¥–≤–∞ –ø–æ–ª—è—Ä–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–∞: —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –Ω–∞ —Å–ª–æ–≤–∞ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –Ω–∞ —Å–∏–º–≤–æ–ª—ã. –ë—É–¥–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ –¥–≤–∏–≥–∞—Ç—å—Å—è –æ—Ç –ø–æ—Å–∏–º–≤–æ–ª—å–Ω–æ–π —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –∫ –ø–æ—Å–ª–æ–≤–Ω–æ–π. –û—Å—Ç–∞–Ω–æ–≤–∏–º—Å—è –≤ —Ç–æ—Ç –º–æ–º–µ–Ω—Ç, –∫–æ–≥–¥–∞ –±—É–¥–µ—Ç –¥–æ—Å—Ç–∏–≥–Ω—É—Ç –Ω—É–∂–Ω—ã–π —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è.\n",
    "\n",
    "–î–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ –ø–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è —á–∞—Å—Ç–æ—Ç–∞ –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏ –≤ –∫–æ—Ä–ø—É—Å–µ. –í–Ω–∞—á–∞–ª–µ –≤—Å–µ —Å–ª–æ–≤–∞ –≤ –∫–æ—Ä–ø—É—Å–µ —Ä–∞–∑–±–∏–≤–∞—é—Ç—Å—è –Ω–∞ —Å–∏–º–≤–æ–ª—ã, –∫ –ø–æ—Å–ª–µ–¥–Ω–∏–º —Å–∏–º–≤–æ–ª–∞–º –ø—Ä–∏–±–∞–≤–ª—è–µ—Ç—Å—è —Å–∏–º–≤–æ–ª –∫–æ–Ω—Ü–∞ —Å–ª–æ–≤–∞."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L05/BPE1.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://wikidocs.net/166315\">Tokenizers: How machines read </a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–µ–ø–µ—Ä—å –Ω–∞—à —Å–ª–æ–≤–∞—Ä—å —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ —Å–∏–º–≤–æ–ª–æ–≤. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–¥—Å—á–∏—Ç–∞–Ω–∞ —á–∞—Å—Ç–æ—Ç–∞ –≤—Å—Ç—Ä–µ—á–∞–µ–º–æ—Å—Ç–∏ –≤ –∫–æ—Ä–ø—É—Å–µ. –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è —Ä–∞–≤–µ–Ω 27."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L05/BPE2.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://wikidocs.net/166315\">Tokenizers: How machines read </a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –º—ã —Ö–æ—Ç–∏–º –æ–±—ä–µ–¥–∏–Ω—è—Ç—å —Å–∏–º–≤–æ–ª—ã, –∫–æ—Ç–æ—Ä—ã–µ —á–∞—â–µ –≤—Å–µ–≥–æ –≤—Å—Ç—Ä–µ—á–∞—é—Ç—Å—è –≤–º–µ—Å—Ç–µ.\n",
    "\n",
    "–ü–æ—Å—á–∏—Ç–∞–µ–º —á–∞—Å—Ç–æ—Ç—É –±–∏–≥—Ä–∞–º–º. –í –Ω–∞—à–µ–º –∫–æ—Ä–ø—É—Å–µ —Å–∞–º–æ–π —á–∞—Å—Ç–æ—Ç–Ω–æ–π –æ–∫–∞–∑—ã–≤–∞–µ—Ç—Å—è –±–∏–≥—Ä–∞–º–º–∞ *de*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L05/BPE3.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://wikidocs.net/166315\">Tokenizers: How machines read </a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–µ–ø–µ—Ä—å –≤ —Å–ª–æ–≤–∞—Ä–µ –ø–æ—è–≤–∏–ª–∞—Å—å –æ–¥–Ω–∞ –±–∏–≥—Ä–∞–º–º–∞. –†–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è —Ä–∞–≤–µ–Ω 28. –ü—Ä–∏ —ç—Ç–æ–º —á–∞—Å—Ç–æ—Ç–∞ —Å–∏–º–≤–æ–ª–æ–≤ *d* –∏ *e* –ø–æ–Ω–∏–∑–∏–ª–∞—Å—å, –æ–Ω–∏ —Å—Ç–∞–ª–∏ –±–æ–ª–µ–µ —Ä–µ–¥–∫–∏–º–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L05/BPE4.png\" width=\"700\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://wikidocs.net/166315\">Tokenizers: How machines read </a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ –ø–æ–≤—Ç–æ—Ä—è–µ—Ç—Å—è –º–Ω–æ–≥–æ —Ä–∞–∑. –ù–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ –≤ —Å–ª–æ–≤–∞—Ä—å –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –Ω–æ–≤—ã–π —Ç–æ–∫–µ–Ω (n-–≥—Ä–∞–º–º–∞).\n",
    "\n",
    "–í –∫–æ–Ω—Ü–µ —Ä–∞–±–æ—Ç—ã –∞–ª–≥–æ—Ä–∏—Ç–º–∞ —Å–ª–æ–≤–∞—Ä—å –±—É–¥–µ—Ç –≤–∫–ª—é—á–∞—Ç—å –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–æ—Ç–Ω—ã–µ —Å–ª–æ–≤–∞, –∞ –±–æ–ª–µ–µ —Ä–µ–¥–∫–∏–µ –±—É–¥—É—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –ø–æ–¥—Å–ª–æ–≤–∞–º–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–≠—Ç–æ—Ç –∂–µ —Å–ø–æ—Å–æ–± –ø–æ–º–æ–≥–∞–µ—Ç —Ä–µ—à–∏—Ç—å **–ø—Ä–æ–±–ª–µ–º—É** **OOV (out of vocabulary)**. –í –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ –º–æ–∂–µ—Ç –Ω–µ –±—ã—Ç—å —Å–ª–æ–≤–∞ *Unfriendly*, –Ω–æ –ø–æ—Å–∫–æ–ª—å–∫—É **Unfriendly** = **Un** + **friend** + **ly**, –º—ã –º–æ–∂–µ–º —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞—Ç—å, —á—Ç–æ —Å–µ—Ç—å –±—É–¥–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å / –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏ —Å–ª–æ–≤–æ —Ü–µ–ª–∏–∫–æ–º."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/L05/token_unfriendly.png\" width=\"600\"></center>\n",
    "\n",
    "<center><em>Source: <a href=\"https://www.thoughtvector.io/blog/subword-tokenization/\">Subword Tokenization ‚Äî Handling Misspellings and Multilingual Data</a></em></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 6> –ó–∞–∫–ª—é—á–µ–Ω–∏–µ </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- –ë—ã–ª–∞ —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä, –ø–æ–∑–≤–æ–ª—è—é—â–∞—è —É—á–∏—Ç—ã–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –ø–æ–º–æ—â—å—é –º–µ—Ö–∞–Ω–∏–∑–º–∞ –≤–Ω–∏–º–∞–Ω–∏—è.\n",
    "- –í–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∞ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–≥–æ –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞.\n",
    "- –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –Ω–∞ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Ü–∏—Ñ—Ä) –∏ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö Tatoeba –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ —Å —Ä—É—Å—Å–∫–æ–≥–æ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π.\n",
    "- –†–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π-—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–æ–≤ –¥–ª—è –¥—Ä—É–≥–∏—Ö –∑–∞–¥–∞—á.\n",
    "- –û–ø–∏—Å–∞–Ω –º–µ—Ç–æ–¥ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ Byte-Pair Encoding, –∫–æ—Ç–æ—Ä—ã–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
