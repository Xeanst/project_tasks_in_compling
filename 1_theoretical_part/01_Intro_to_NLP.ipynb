{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<font size=\"6\">Введение в обработку текстов</font>"
      ],
      "metadata": {
        "id": "26_yE4WKz30z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Два пути"
      ],
      "metadata": {
        "id": "dM6Gc0Sw2eyi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Допустим, перед вами стоит задача автоматически выделить новости, относящиеся к разным тематикам: политические, спортивные, экономические и научные. К её решению можно подойти двумя способами."
      ],
      "metadata": {
        "id": "a-UasU2gzyX0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://i.postimg.cc/wBBXW9gW/News-classification.png\" width=\"450\"></center>"
      ],
      "metadata": {
        "id": "xAw7E9p0yEzw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Вариант №1: подход на основе правил (rule-based)**\n",
        "\n",
        "Будем использовать **вручную** заданные **правила** классификации и тематически размеченные **словари**. Эти правила определяют класс текста на основе ключевых слов. Однако чем больше примеров мы будем анализировать, тем больше исключений будет появляться.\n",
        "\n",
        "Следовательно, потребуется добавлять новые правила и увеличивать размер словаря. Алгоритмическая сложность программы будет расти, ее будет сложнее поддерживать."
      ],
      "metadata": {
        "id": "YC7CsDlS0Thm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Вариант №2: машинное обучение (machine learning)**\n",
        "\n",
        "С появлением **машинного обучения** мы можем применить принципиально другой подход. Он заключается в том, чтобы обучить модель, которая сама будет извлекать шаблоны из данных.\n",
        "\n",
        "Соберем некоторое количество новостей за определенное время и вручную разметим их тематики. Загрузим данные в модель, и она обучится на этих данных. При достаточном количестве данных и адекватно подобранной модели мы сможем научить ее решать конкретные задачи."
      ],
      "metadata": {
        "id": "ntzMFt4m2FhB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://i.postimg.cc/PJ1YLxQq/rule-based-vs-machine-learning.png\" width=\"650\"></center>"
      ],
      "metadata": {
        "id": "jgKzINlQ14S4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В ходе данного курса мы рассмотрим, как применять методы машинного обучения для работы с текстовыми данными и решать различные задачи."
      ],
      "metadata": {
        "id": "f-im5-6f2Slb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# План исследования"
      ],
      "metadata": {
        "id": "D2yyjcPE3EJJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Допустим, мы решили заняться разработкой классификатора новостей. Как будет выглядеть план исследования?"
      ],
      "metadata": {
        "id": "lnJAJxCj3GWb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://i.postimg.cc/N0VHpTNM/pipeline.png\" width=\"750\"></center>"
      ],
      "metadata": {
        "id": "ZF-Yy8TV7_ro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Инструменты"
      ],
      "metadata": {
        "id": "oxKsCr057NIg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В ходе курса мы будем опираться на существующие методы, реализованные в следующих основных библиотеках:\n",
        "\n",
        "* [NLTK](https://www.nltk.org/) — инструменты для предобработки текстов.\n",
        "- [RNNmorph](https://github.com/IlyaGusev/rnnmorph) — морфологический парсер для русского и английского языков.\n",
        "- [UDPipe](https://ufal.mff.cuni.cz/udpipe) — морфологический и синтаксический парсер для различных языков.\n",
        "* [NumPy](https://numpy.org/) — поддержка больших многомерных массивов и быстрых математических функций для операций с этими массивами.\n",
        "* [Scikit-learn](https://scikit-learn.org/stable/) — ML алгоритмы, \"toy\"-датасеты.\n",
        "* [Pandas](https://pandas.pydata.org/) — удобная работа с табличными данными.\n",
        "* [PyTorch](https://pytorch.org/) — основной фреймворк машинного обучения, который будет использоваться на протяжении всего курса.\n",
        "* [Matplotlib](https://matplotlib.org/) — основная библиотека для визуализации. Вывод различных графиков.\n",
        "* [Seaborn](https://seaborn.pydata.org/) — удобная библиотека для визуализации статистик. Прямо из коробки вызываются и гистограммы, и тепловые карты, и визуализация статистик по датасету, и многое другое."
      ],
      "metadata": {
        "id": "i7H0smoX7MXI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-wqq07f25M5"
      },
      "source": [
        "## Сбор данных"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Где можно добыть данные?\n",
        "\n",
        "* [HuggingFace](https://huggingface.co/datasets)\n",
        "* [Соревнования Kaggle](https://www.kaggle.com/datasets)\n",
        "* [Google Datasets](https://datasetsearch.research.google.com/)\n",
        "* [Papers with Code](https://paperswithcode.com/)"
      ],
      "metadata": {
        "id": "m3x6yQXSC_MJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Если вы используете данные, скачанные из сети, проверьте, откуда они. Описаны ли они в статье? Если да, посмотрите на документ, убедитесь, что он был опубликован в авторитетном месте, и проверьте, упоминают ли авторы какие-либо ограничения на использованные датасеты.\n",
        "\n",
        "Если данные использовались в ряде работ, это еще не гарантирует высокое качество датасета. **Иногда данные используются только потому, что их легко достать**."
      ],
      "metadata": {
        "id": "WzwZMLtU9QOY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9jYxYy925M5"
      },
      "source": [
        "## Разведочный анализ данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_YoZYbC25M6"
      },
      "source": [
        "**Разведочный анализ данных/ Exploratory data analysis (EDA)** — анализ основных свойств данных, нахождение в них общих закономерностей, зачастую с использованием инструментов визуализации."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X45ZYPbO25M6"
      },
      "source": [
        "<center><img src =\"https://i.postimg.cc/3NrYGfgD/eda.png\" width=\"450\"></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Предобработка текста"
      ],
      "metadata": {
        "id": "A26BRNZHtURX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Текстовые данные содержат различный «шум»: знаки препинания, чередование строчных и прописных символов, служебные слова. Для работы с текстовыми данными их необходимо упростить и привести к стандартной форме, которая подходит для используемого алгоритма.\n",
        "\n",
        "Результатом предобработки станет набор содержательных слов нижнего регистра в начальной форме без знаков препинания.\n",
        "\n",
        "Стандартная процедура предварительной обработки включает следующие этапы:\n",
        "- токенизация текста;\n",
        "- удаление знаков препинания и чисел;\n",
        "- приведение к нижнему регистру;\n",
        "- удаление стоп-слов;\n",
        "- стемминг/ лемматизация."
      ],
      "metadata": {
        "id": "8YGnDuN1ME7z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src =\"https://i.postimg.cc/6qQxq2Xr/text-preprocessing.png\" width=\"400\" ></center>"
      ],
      "metadata": {
        "id": "B7jD4nRjIhr1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чтобы унифицировать и структурировать текст, нужно разделить его на более мелкие единицы анализа. Процесс разделения больших блоков текста (предложений, абзацев) на более маленькие единицы называется **токенизацией**."
      ],
      "metadata": {
        "id": "2sMWeoOpJRVR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src =\"https://i.postimg.cc/CKC34HMC/tokenizer.png\" width=\"600\" ></center>"
      ],
      "metadata": {
        "id": "LnpUJdsudzH1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Hey, did you know that the summer break is coming? \\\n",
        "Amazing right!! It's only 5 more days!!\"\n",
        "tokenized_text = word_tokenize(text)\n",
        "print(f'Source text: {text}')\n",
        "print(f'Tokenized text: {tokenized_text}')"
      ],
      "metadata": {
        "id": "ofNTm24FSUjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Необходимо удалить элементы предложения, которые не являются словами: знаки препинания, специальные символы, числа. Они не содержат важной информации о смысловой характеристики текста и не будут полезны при классификации.\n",
        "\n",
        "Для обеспечения непротиворечивости при обработке текста необходимо привести все слова к нижнему регистру. Этот этап повозлит уменьшить размер словаря текстовых данных."
      ],
      "metadata": {
        "id": "_ZINddcoeaPb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src =\"https://i.postimg.cc/Xqqt3Rfx/normalization.png\" width=\"700\" ></center>"
      ],
      "metadata": {
        "id": "goDQ_AMsgXtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lowercased_text = [word.lower() for word in tokenized_text if word.isalpha()]\n",
        "print(f'Tokenized text: {tokenized_text}')\n",
        "print(f'Lowercased text: {lowercased_text}')"
      ],
      "metadata": {
        "id": "-wa2L8AeftOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Необходимо удалить стоп-слова — это часто используемые слова, которые не вносят никакой дополнительной информации в текст. В билиотеке [NLTK](https://www.nltk.org/) есть встроенный список стоп-слов, которым мы воспользуемся."
      ],
      "metadata": {
        "id": "TUeO0llnUl2Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "print(f\"Stopwords in English: {stopwords.words('english')}\")"
      ],
      "metadata": {
        "id": "cl-PdkfkT8Qy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "STOPWORDS = stopwords.words('english')\n",
        "text_without_stopwords = [word for word in lowercased_text if word not in STOPWORDS]\n",
        "print(f'Lowercased text: {lowercased_text}')\n",
        "print(f'Text without stopwords: {text_without_stopwords}')"
      ],
      "metadata": {
        "id": "vFZRGNgthHtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "В тексте одно и то же слово может встретиться в разной грамматической форме. Чтобы привести все токены к их словарной форме и сократить размер словаря, используется один из двух подходов для нормализации текста:\n",
        "\n",
        "- Лемматизация — процесс приведения словоформы к лемме, то есть её нормальной (словарной) форме. Для его применения необходима информация о части речи и морфологической форме анализируемого слова, что требует анализа контекста.\n",
        "- Стемминг — эвристический процесс нахождения основы для заданного слова. Он осуществляется на основе списка наиболее распространенных окончаний и словообразовательных суффиксов."
      ],
      "metadata": {
        "id": "jZLSnGrCiPkM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src =\"https://i.postimg.cc/PfDRbQ6C/stem-lemmatize.png\" width=\"700\" ></center>"
      ],
      "metadata": {
        "id": "rGc_ZMRxSUBD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_text = [stemmer.stem(word) for word in text_without_stopwords]\n",
        "print(f'Text without stopwords: {text_without_stopwords}')\n",
        "print(f'Stemmed text: {stemmed_text}')"
      ],
      "metadata": {
        "id": "-WKH1vUNpIgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "По аналогии со стеммером, создадим лемматизатор — объект класса `WordNetLemmatizer`и осуществим лемматизацию с помощью метода `lemmatize()`."
      ],
      "metadata": {
        "id": "sz1FrmFIl0py"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_text = [lemmatizer.lemmatize(word) for word in text_without_stopwords]\n",
        "print(f'Text without stopwords: {text_without_stopwords}')\n",
        "print(f'Lemmatized text: {lemmatized_text}')"
      ],
      "metadata": {
        "id": "sCbSINsFqA9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Можно заметить, что лемматизация применилась только к существительному \"days\". По умолчанию параметр `pos` (part of speech) лемматизатора `WordNetLemmatizer` имеет значение `\"n\"` (noun). Другие возможные значения: `\"v\"` (verb) для глаголов, `\"a\"` (adjective) для прилагательных и `\"r\"` (adverb) для наречий."
      ],
      "metadata": {
        "id": "WTwYzf6s_s33"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Lemma for word \"broke\" is {lemmatizer.lemmatize(\"broke\", \"v\")}.')\n",
        "print(f'Lemma for word \"better\" is {lemmatizer.lemmatize(\"better\", \"a\")}.')"
      ],
      "metadata": {
        "id": "iKSDaMIBhHZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Метод `pos_tag()` позволяет автоматически определять часть речи для всех слов предложения, а также содержит дополнительную грамматическую информацию."
      ],
      "metadata": {
        "id": "mIT4Q1aeh7Qa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.pos_tag(text_without_stopwords)"
      ],
      "metadata": {
        "id": "BJZb9Ggt98PE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Можно заметить, что частеречные теги лемматизатора и pos-теггера различаются. Напишем функцию, которая будет заменять теги pos-тэггера на теги лемматизатора по словарю соответствия и возвращать для каждого токена аналогичный кортеж `(слово, тег)`."
      ],
      "metadata": {
        "id": "pmWKLJraVCsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "def pos_mapping(text):\n",
        "    word_tag_array = []\n",
        "    for word_tag in nltk.pos_tag(text):\n",
        "        tag = word_tag[1][0]\n",
        "        tag_dict = {\"J\": \"a\",\n",
        "                    \"N\": \"n\",\n",
        "                    \"V\": \"v\",\n",
        "                    \"R\": \"r\"}\n",
        "        word_tag_array.append((word_tag[0], tag_dict[tag]))\n",
        "    return word_tag_array\n",
        "\n",
        "pos_mapping(text_without_stopwords)"
      ],
      "metadata": {
        "id": "qut-O7UkUxqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь можем осуществить лемматизацию всех частей речи."
      ],
      "metadata": {
        "id": "eJXDUx_BbUvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatized_text = [lemmatizer.lemmatize(word, tag) for word, tag in pos_mapping(text_without_stopwords)]\n",
        "print(f'Text without stopwords: {text_without_stopwords}')\n",
        "print(f'Lemmatized text: {lemmatized_text}')"
      ],
      "metadata": {
        "id": "zBuKhV7IbNBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "В завершение необходимо объединить список токенов обратно в строку через пробел."
      ],
      "metadata": {
        "id": "Q_4naKMPrPhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_text = ' '.join(stemmed_text)\n",
        "print(f'Stemmed text: {stemmed_text}')\n",
        "print(f'Final text: {final_text}')"
      ],
      "metadata": {
        "id": "E1zvNlrZrPI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X76zw7wg25Mx"
      },
      "source": [
        "## Векторизация текста"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ee47x8Xd25My"
      },
      "source": [
        "Методы машинного обучения работают с числами. Нам нужно найти способы, которые позволят представлять тексты в виде числовых данных.\n",
        "\n",
        "Если объектом является текст, в качестве признаков выступают слова, которые он содержит. Процесс преобразования текста в числа называется **векторизацией**."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src =\"https://i.postimg.cc/QtT0bjJ2/vectorization.webp\" width=\"500\" ></center>"
      ],
      "metadata": {
        "id": "ZHc_uXGjvbKX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-InMCBl025My"
      },
      "source": [
        "Рассмотрим два способа векторизации предложений из библиотеки [scikit-learn](https://scikit-learn.org/stable/).\n",
        "\n",
        "Для наглядности будем использовать небольшой корпус из трех предложений."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQgL8HN825My"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "corpus = pd.Series(['She loves pizza, pizza is delicious.',\n",
        "                     'She is good person.',\n",
        "                     'Good people are the best.'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biMcQHlU25My"
      },
      "source": [
        "### Мешок слов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8FVni2Y25My"
      },
      "source": [
        "<center><img src =\"https://i.postimg.cc/VNSRfzpJ/bag-of-words.png\" width=\"600\" ></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96lLhab225My"
      },
      "source": [
        "Мешок слов (bag of words) — представление текста, которое описывает вхождение слова в документ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HijWTK525My"
      },
      "source": [
        "Реализуем векторизацию мешком слов с помощью класса [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html). Метод `fit` собирает словарь, метод `transform` преобразует тексты в векторы на основе собранного словаря. Метод `fit_transform` выполняет все это сразу."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNd-d8H225Mz"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "bow = CountVectorizer()\n",
        "#bow.fit(corpus)\n",
        "#corpus_bow = bow.transform(corpus)\n",
        "corpus_bow = bow.fit_transform(corpus)\n",
        "corpus_bow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xXyj5k925Mz"
      },
      "source": [
        "В результате мы получаем разрежённую (sparse) матрицу — это матрица с преимущественно нулевыми элементами. Если бо́льшая часть элементов матрицы ненулевая, она считается плотной (dense). Особенностью разреженных матриц является их компактность. Подробнее про [разреженные матрицы](https://python-school.ru/blog/python/sparse-matrix/).\n",
        "\n",
        "Выведем результат для всех предложений."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0hwyjX925Mz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "bow_df = pd.DataFrame(corpus_bow.toarray(),\n",
        "                      columns = bow.get_feature_names_out(),\n",
        "                      index=corpus)\n",
        "bow_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXCHGxSV25Mz"
      },
      "source": [
        "По умолчанию в качестве признаков используются слова (униграммы). С помощью параметра `ngram_range` можно считать частоту встречаемости для *n*-грамм. Необходимо задать значения `min_n` и `max_n` (`default=(1, 1)`)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создадим векторизатор, использующий биграммы и триграммы в качестве признаков (*n* от 2 до 3). Применим векторизатор к текстам корпуса."
      ],
      "metadata": {
        "id": "I6_JlxqKsXWP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jCSy0Ju25Mz"
      },
      "outputs": [],
      "source": [
        "bow1 = CountVectorizer(ngram_range=(2,3))\n",
        "corpus_bow1 = bow1.fit_transform(corpus)\n",
        "corpus_bow1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gK9gKtjB25Mz"
      },
      "outputs": [],
      "source": [
        "bow1_df = pd.DataFrame(corpus_bow1.toarray(),\n",
        "                      columns = bow1.get_feature_names_out(),\n",
        "                      index=corpus)\n",
        "bow1_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfLjhasD25Mz"
      },
      "source": [
        "Параметр `analyzer` определяет, какая единица предложения является признаком — целое слово или подслово. По умолчанию он принимает значение `'word'`. Для использования символьных *n*-грамм нужно установить значение `'char'` (границы слов включаются в *n*-граммы) или `'char_wb'` (создает *n*-граммы символов только из текста внутри границ слов)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ijoX9kyh25Mz"
      },
      "outputs": [],
      "source": [
        "bow2 = CountVectorizer(ngram_range=(3,4), analyzer='char_wb')\n",
        "corpus_bow2 = bow2.fit_transform(corpus)\n",
        "corpus_bow2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4m6j86C25Mz"
      },
      "outputs": [],
      "source": [
        "bow2_df = pd.DataFrame(corpus_bow2.toarray(),\n",
        "                      columns = bow2.get_feature_names_out(),\n",
        "                      index=corpus)\n",
        "bow2_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bow2.get_feature_names_out()"
      ],
      "metadata": {
        "id": "F0oTjoLtZkqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrpI-9WY25M0"
      },
      "source": [
        "### TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJqstz-u25M0"
      },
      "source": [
        "$TF{\\text -}IDF$ ($TF$ — term frequency, $IDF$ — inverse document frequency)  — это способ векторизации текста, отражающий важность слова в документе, а не только частоту его появления.\n",
        "\n",
        "Частота слов ($TF$) — это мера частоты употребления слова $w$ в документе $d$. $TF$ определяется как отношение появления слова в документе к общему количеству слов в документе.\n",
        "\n",
        "$$TF(w,d) = \\frac{количество\\:вхождений\\:слова\\:w\\:в\\:документе\\:d}{общее\\:количество\\:слов\\:n\\:в\\:документе\\:d}$$\n",
        "\n",
        "Обратная частота документов ($IDF$) —  это мера важности слова. Некоторые слова могут присутствовать наиболее часто, но не имеют большого значения. $IDF$ присваивает вес каждому слову в зависимости от его частоты в корпусе $D$.\n",
        "\n",
        "$$IDF(w,D) = ln(\\frac{общее\\:количество\\:документов\\:N\\:в\\:корпусе\\:D}{количество\\:документов,\\:содержащих\\:слово\\:w})$$\n",
        "\n",
        "$TF{\\text -}IDF$ является произведением $TF$ и $IDF$.\n",
        "$$TF{\\text -}IDF(w,d,D)=TF(w,d)*IDF(w,D)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBTSkt4G25M0"
      },
      "source": [
        "<center><img src =\"https://i.postimg.cc/pd8Fcr3J/tf-idf.png\" width=\"500\" ></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1FLdNwd25M0"
      },
      "source": [
        "Воспользуемся классом [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) для векторизации. Применим метод `fit_transform` и посмотрим на результат."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LcquJPb25M0"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "corpus_tfidf = tfidf.fit_transform(corpus)\n",
        "corpus_tfidf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nI63Ig7S25M0"
      },
      "outputs": [],
      "source": [
        "tfidf_df = pd.DataFrame(corpus_tfidf.toarray(),\n",
        "                      columns = tfidf.get_feature_names_out(),\n",
        "                      index=corpus)\n",
        "tfidf_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCd_S9hB25M0"
      },
      "source": [
        "Можно ограничить размер словаря и включать только слова, которые встречаются не реже N раз(`min_df`). Также можно убрать слова, которые встречаются слишком часто и являются стоп-словами в пределах данного корпуса (`max_df`). Оба параметра могут быть выражены целым числом либо числом с плавающей точкой в диапазоне [0.0, 1.0]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IY4UEUXs25M0"
      },
      "outputs": [],
      "source": [
        "tfidf1 = TfidfVectorizer(min_df=2)\n",
        "corpus_tfidf1 = tfidf1.fit_transform(corpus)\n",
        "corpus_tfidf1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfwBUhJT25M0"
      },
      "outputs": [],
      "source": [
        "tfidf1_df = pd.DataFrame(corpus_tfidf1.toarray(),\n",
        "                      columns = tfidf1.get_feature_names_out(),\n",
        "                      index=corpus)\n",
        "tfidf1_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Обучение модели"
      ],
      "metadata": {
        "id": "ViAuY8uIBPSe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Рассмотрим алгоритмы, которые из-за простоты использования удобно применять в качестве базовой (baseline) модели. Более сложные алгоритмы практически всегда, будут работать лучше, НО помимо качества в практических задачах часто есть много других требований и ограничений (скорость, память, интерпретируемость), которые могут перевесить разницу в точности."
      ],
      "metadata": {
        "id": "dqV76ts_BRBG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Наивный байесовский классификатор"
      ],
      "metadata": {
        "id": "uVxb7GCKG6Er"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Наивный байесовский классификатор принимает решение по вероятностям, рассчитаным на обучающем корпусе. При предсказании выбирается класс, набравший наибольшую оценку. Наивным такой классификатор называется, потому что делается предположение о независимости слов в документе друг от друга.\n",
        "\n",
        "Для каждого класса $c$ требуется найти $P(c|d)$ — вероятность класса $c$ для документа $d$. Она рассчитывается по формуле Байеса:\n",
        "\n",
        "$$P(c|d) = \\frac{P(d|c) P(c)} {P(d)}$$\n",
        "\n",
        "Среди значений $P(c|d)$ для всех классов выбирается $\\hat c$, набравший наибольшую оценку.\n",
        "\n",
        "$$\\hat c = \\underset{c \\in C }{\\operatorname{argmax}} P(c|d) = \\underset{c \\in C }{\\operatorname{argmax}} \\frac{P(d|c)P(c)} {P(d)}$$\n",
        "\n",
        "Значение в знаменателе одинаково для всех классов, поэтому его можно опустить.\n",
        "\n",
        "$$\\hat c = \\underset{c \\in C }{\\operatorname{argmax}} P(d|c)P(c) $$\n",
        "\n",
        "$P(c)$: вероятность класса $c$ — это доля документов класса $c$ среди всех документов.\n",
        "\n",
        "$P(d|c)$: вероятность документа $d$ для класса $c$ зависит от слов $x_1, x_2, ..., x_n$, входящих в документ:\n",
        "\n",
        "$$P(d|c) = P(x_1,x_2,...,x_n|c) = P(x_1|c)P(x_2|c)...P(x_n|c) $$"
      ],
      "metadata": {
        "id": "5lTcA6HCG2_c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Логистическая регрессия"
      ],
      "metadata": {
        "id": "WVfJvpHWC7gA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Общее назначение регрессии состоит в анализе связи между несколькими независимыми переменными и зависимой переменной. Зависимая переменная $z$ является взвешенной суммой независимых переменных $x_1,x_2,...,x_n$ (признаков). Веса $w_1, w_2, ..., w_n$ подбираются на обучающих данных — в этом и состоит обучение модели.\n",
        "\n",
        "Переменная $z$ может принимать значения в любом диапазоне. Чтобы получить вероятность отнесения объекта к классу, нужно привести его в диапазон от 0 до 1 с помощью функции активации. В случае бинарной классификации используется сигмоида."
      ],
      "metadata": {
        "id": "y3z-RI2GC9sj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://i.postimg.cc/zBFR5MzL/logistic-regression.png\" width=\"900\"></center>"
      ],
      "metadata": {
        "id": "FtyEAl8VDavF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для многоклассовой классификации используется функция активации softmax.  Вероятность $i$-го класса при наличии $K$ классов рассчитывается следующим образом:\n",
        "\n",
        "$$\\text{softmax}(z_{i}) = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}$$\n",
        "$$i=1,...,K$$\n",
        "\n",
        "Для каждого объекта выбирается класс с наибольшей вероятностью.\n",
        "\n",
        "Запишем формулу для функции софтмакс.  Используем `np.exp(x)` из библиотеки [numpy](https://numpy.org/doc/stable/index.html#) для подсчета $e^x$ и `np.sum()` для подсчета суммы."
      ],
      "metadata": {
        "id": "OJQGIR7JGV_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(z):\n",
        "    return np.exp(z) / np.sum(np.exp(z))\n",
        "\n",
        "z = np.array([2.6, 3.2, 0.5])\n",
        "\n",
        "print(softmax(z))\n",
        "print(f\"softmax(z_0) = {softmax(z)[0].round(2)}\")\n",
        "print(f\"softmax(z_1) = {softmax(z)[1].round(2)}\")\n",
        "print(f\"softmax(z_2) = {softmax(z)[2].round(2)}\")\n",
        "print(f\"sum = {softmax(z)[0] + softmax(z)[1] + softmax(z)[2]}\")"
      ],
      "metadata": {
        "id": "aqdwz7EPi2Yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Дерево решений"
      ],
      "metadata": {
        "id": "Vi2oBAbXB-75"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Дерево решений представляет собой иерархическую древовидную структуру, состоящую из правила вида «Если ..., то ...». За счет обучающего множества правила генерируются автоматически в процессе обучения.\n",
        "\n",
        "Деревья решений включают в себя элементы двух типов — узлы (node) и листья (leaf). Узлы включают в себя решающие правила и производят проверку примеров на соответствие правилу. Примеры попадают в узел и разбиваются на два подмножества:\n",
        "- удовлетворяют установленному правилу\n",
        "- не удовлетворяют установленному правилу\n",
        "\n",
        "Далее к каждому подмножеству применяется новое правило. Последний узел, когда не осуществляется проверка и разбиение, становится листом. Лист определяет класс для каждого попавшего в него примера. Пример попадает в лист, если соответствует всем правилам на пути к нему."
      ],
      "metadata": {
        "id": "iBwRbLnmB8Pz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://i.postimg.cc/wvg7WVX9/decision-tree.png\" width=\"1000\"></center>"
      ],
      "metadata": {
        "id": "hEE-VaFwCnrP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Метод случайного леса"
      ],
      "metadata": {
        "id": "-ryHvdf0C0Pw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Метод случайного леса — алгоритм машинного обучения, заключающийся в использовании ансамбля решающих деревьев. Каждое дерево строится на случайном подмножестве обучающих данных и случайном подмножестве признаков. В результате каждое дерево в ансамбле получается немного разным, что позволяет уменьшить эффект переобучения и повысить качество предсказаний.\n",
        "\n",
        "Для построения каждого дерева случайного леса происходит следующее:\n",
        "1. Случайным образом выбирается подмножество обучающих объектов из всего набора данных.\n",
        "2. Случайным образом выбирается подмножество признаков. Это позволяет уменьшить корреляцию между деревьями в ансамбле и улучшить их разнообразие.\n",
        "3. Строится дерево решений на выбранном подмножестве данных и признаков.\n",
        "\n",
        "После построения всех деревьев в ансамбле для каждого объекта данных происходит голосование по всем деревьям. Наиболее популярный класс становится предсказанным классом."
      ],
      "metadata": {
        "id": "UfsLpJDfCxYI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Оценка качества"
      ],
      "metadata": {
        "id": "Zqs5eJt65nFd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как оценивать качество алгоритма? Допустим, вы хотите внести изменения в алгоритм. Откуда вы знаете что эти изменения сделают алгоритм лучше? Конечно же, надо проверять алгоритм на реальных данных.\n",
        "\n",
        "Основой проверки является тестовая выборка. Для того чтобы принимать решение, хуже или лучше справляется с работой новая версия алгоритма, нам необходима численная метрика его качества."
      ],
      "metadata": {
        "id": "EjHKnZhN6F6V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Метрики для бинарной классификации"
      ],
      "metadata": {
        "id": "qDYVMwd0fod5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для оценки качества классификации используется матрица ошибок.\n",
        "\n",
        "Есть алгоритм, предсказывающий принадлежность каждого объекта одному из классов.\n",
        "- $\\hat y$ — предсказанный алгоритмом класс объекта\n",
        "- $y$ — истинный класс объекта\n",
        "\n",
        "Два класса делятся на положительный (1) и отрицательный (0 или –1).\n",
        "- Объекты, которые алгоритм относит к положительному классу, – положительные (Positive).\n",
        "- Те, которые на самом деле принадлежат к этому классу, – истинно положительные (True Positive).\n",
        "- Остальные – ложно положительные (False Positive).\n",
        "\n",
        "Аналогичная терминология для отрицательного (Negative) класса.\n",
        "\n",
        "Таким образом, ошибки классификации бывают двух видов: False Negative (FN) и False Positive (FP)."
      ],
      "metadata": {
        "id": "4HM8GkceMt0b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\begin{array}{|c|c|} \\hline\n",
        "& y=1 & y=0 \\\\ \\hline\n",
        "\\hat{y}=1 & \\text{True Positive (TP)} & \\text{False Positive (FP)}  \\\\ \\hline\n",
        "\\hat{y}=0 & \\text{False Negative (FN)} & \\text{True Negative (TN)} \\\\ \\hline\n",
        "\\end{array}"
      ],
      "metadata": {
        "id": "qCV3LNp_-mvu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Каждый объект изначально относится к одному из классов:\n",
        "- является фотографией кота → положительный класс (positive, $y=1$).\n",
        "- является фотографией собаки → отрицательный класс (negative, $y=0$).\n",
        "\n",
        "Модель может предсказать, является объект фотографией кота (true, $\\hat y = 1$) или собаки (false, $\\hat y = 0$).\n",
        "\n",
        "Пусть какой-то набор признаков характерен для изображения кота.\n",
        "- Модель верно определила и поставила положительный класс → истинно положительный исход (true positive).\n",
        "- Модель ставит отрицательную метку класса → ложно отрицательный исход (false negative): модель «сказала», что на фотографии изображена собака, но на самом деле изображен кот.\n",
        "\n",
        "В случае, если на фотографии изображена собака, исходы модели остаются аналогичными.\n",
        "- Модель относит объект к положительному классу → ложно положительный исход (false positive): модель «сказала», что на фотографии изображен кот, но на самом деле изображена собака.\n",
        "- Модель определят письмо как отрицательный класс → истинно отрицательный исход (true negative)."
      ],
      "metadata": {
        "id": "_poEbzva_i2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://i.postimg.cc/4NmnFn9H/confusion_matrix.png\" width=\"500\"></center>"
      ],
      "metadata": {
        "id": "t_I73BlXAQNd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Матрица ошибок используется для подсчета метрик качества классификации."
      ],
      "metadata": {
        "id": "iJRHkOuG6708"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy"
      ],
      "metadata": {
        "id": "XnHh_68v64At"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy — доля правильных ответов алгоритма среди всех ответов:\n",
        "\n",
        "$$Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}$$"
      ],
      "metadata": {
        "id": "TRvf1A6Y6-hg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Эта метрика непоказательна в задачах с неравными классами.\n",
        "\n",
        "Допустим, мы хотим оценить работу спам-фильтра почты.\n",
        "\n",
        "- У нас есть 100 не-спам писем (отрицательный класс).\n",
        "  - 90 из них классификатор определил верно,\n",
        "  - остальные 10 – неверно.\n",
        "- Также есть 10 спам-писем (положительный класс).\n",
        "  - 6 из них классификатор определил верно,\n",
        "  - остальные 4 – неверно.\n",
        "\n",
        "Посчитаем accuracy."
      ],
      "metadata": {
        "id": "r_dLxXEWKlc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TP = 6\n",
        "TN = 90\n",
        "FP = 10\n",
        "FN = 4\n",
        "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "accuracy"
      ],
      "metadata": {
        "id": "xDS7HftN7PFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Что будет, если мы просто будем предсказывать все письма как не-спам?"
      ],
      "metadata": {
        "id": "dvpKbSfq7ZoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TP = 0\n",
        "TN = 100\n",
        "FP = 0\n",
        "FN = 10\n",
        "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
        "accuracy"
      ],
      "metadata": {
        "id": "qQ3br26c7bCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "При этом, наша модель совершенно не обладает никакой предсказательной силой, так как изначально мы хотели определять письма со спамом. Преодолеть это нам поможет переход с общей для всех классов метрики к отдельным показателям качества классов."
      ],
      "metadata": {
        "id": "buLNcFhF7dR2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Точность, полнота, F-мера"
      ],
      "metadata": {
        "id": "jQ_gXvZJQvi_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Точность — доля объектов, названных классификатором положительными и при этом действительно являющимися положительными.\n",
        "\n",
        "$$Precision = \\frac{TP}{TP + FP}$$\n",
        "\n",
        "Полнота — доля объектов положительного класса, которые нашел алгоритм, из всех объектов положительного класса.\n",
        "\n",
        "$$Recall = \\frac{TP}{TP + FN}$$\n",
        "\n",
        "Полнота демонстрирует способность алгоритма обнаруживать данный класс вообще, а точность — способность отличать этот класс от других классов. Точность и полнота, в отличие от accuracy, не зависят от соотношения классов и потому применимы в условиях несбалансированных выборок."
      ],
      "metadata": {
        "id": "y6MuqZdzMyGO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://i.postimg.cc/HkjMCMNM/68747470733a2f2f6564756e65742e6b65612e73752f7265706f2f4564754e65742d636f6e74656e742f6465762d322e312f.png\" width=\"700\"></center>"
      ],
      "metadata": {
        "id": "WLfJZfNC8nbn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "F-мера — среднее гармоническое точности и полноты:\n",
        "\n",
        "$$F{\\text -}score=2\\frac{Precision×Recall}{Precision+Recall}$$\n",
        "​\n",
        "Данная формула придает одинаковый вес точности и полноте, поэтому F-мера будет падать одинаково при уменьшении и точности, и полноты. Можно рассчитать F-меру, придав различный вес точности и полноте.\n",
        "\n",
        "$$F{\\text -}score=(β^2+1)\\frac{Precision×Recall}{β^2Precision+Recall}$$\n",
        "​\n",
        "$β$ принимает значения в диапазоне, $0<β<1$ если нужно отдать приоритет точности, а при $β>1$ приоритет отдается полноте. При $β=1$ формула сводится к предыдущей, что дает сбалансированную F-меру (также ее называют F1)."
      ],
      "metadata": {
        "id": "T-SPcC1J8Zp9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Метрики для многоклассовой класификации"
      ],
      "metadata": {
        "id": "ciPyu37Jfsm5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В многоклассовых задачах подсчет качества сводится к вычислению одной из двухклассовых метрик.\n",
        "\n",
        "Пусть выборка состоит из  $K$  классов. Задача классификации ставится как $K$  задач об отделении класса  $i$  от остальных $(i=1,...,K)$. Для каждой из них можно посчитать свою матрицу ошибок.\n",
        "\n",
        "Выделяют три подхода:\n",
        "\n",
        "1. Микроусреднение\n",
        "\n",
        "Сначала элементы матрицы ошибок усредняются по всем классам. Например $TP = \\frac{1}{K}\\sum_{i=1}^KTP_i$. Затем по одной усреднённой матрице ошибок считаем точность, полноту, F-меру.\n",
        "\n",
        " $$Micro{\\text -}precision = \\frac{\\sum_{i=1}^KTP_i}{\\sum_{i=1}^KTP_i+\\sum_{i=1}^KFP_i}$$\n",
        "\n",
        " $$Micro{\\text -}recall = \\frac{\\sum_{i=1}^KTP_i}{\\sum_{i=1}^KTP_i+\\sum_{i=1}^KFN_i}$$\n",
        "\n",
        "2. Макрусреднение\n",
        "\n",
        "Сначала вычисляется итоговая метрика для каждого класса, а затем результаты усредняются по всем классам.\n",
        "\n",
        "$$Macro{\\text -}precision = \\frac{\\sum_{i=1}^KPrecision_i}{K}$$\n",
        "\n",
        "$$Macro{\\text -}recall = \\frac{\\sum_{i=1}^KRecall_i}{K}$$\n",
        "\n",
        "3. Взвешивание\n",
        "\n",
        "Метрики для каждого класса умножаются на его вес, а затем складываются.\n",
        "\n",
        "$$Weighted{\\text -}precision = \\sum_{i=1}^K{w_i*Precision_i}$$\n",
        "\n",
        "$$Weighted{\\text -}recall = \\sum_{i=1}^K{w_i*Recall_i}$$\n",
        "\n",
        "$$w_i = \\frac{количество \\; объектов\\; класса\\; i}{общее \\; количество \\; объектов}$$"
      ],
      "metadata": {
        "id": "GYl7G4RwqFEX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Пример работы с данными и моделью"
      ],
      "metadata": {
        "id": "C20bZxZDHEoT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Скачаем набор данных [News Classification Dataset](https://www.kaggle.com/datasets/amananandrai/ag-news-classification-dataset). Он содержит новости на английском языке, размеченные по четырем тематикам: политика (world), спортивные (sport), экономические (business) и научные (science)."
      ],
      "metadata": {
        "id": "k_V17X6lHWI7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Загрузка и анализ данных"
      ],
      "metadata": {
        "id": "dZ4xl1FunOMu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузим данные с гитхаба."
      ],
      "metadata": {
        "id": "C3C8dOwKRMGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/Xeanst/NLP_course_FBB/main/data/news.csv"
      ],
      "metadata": {
        "id": "VKs1qx8SQVOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "В датасете указана тематика новости, ее заголовок и содержание, номер класа."
      ],
      "metadata": {
        "id": "bAeG-S08RKFU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCzISsMQfw_C"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "news = pd.read_csv('news.csv')\n",
        "news.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Посмотрим, какое количество новостей каждого класса представлено в датасете."
      ],
      "metadata": {
        "id": "8q0Q2lOkT7M_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news['Class Label'].value_counts()"
      ],
      "metadata": {
        "id": "5yD4ZMXHT6ZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.pie(news['Class Label'].value_counts(), labels=news['Class Label'].unique(),\n",
        "        autopct='%.0f%%')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EDZwnuvcWhW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создадим облако слов из новостных статей каждой тематики, чтобы увидеть наиболее часто встречающиеся слова.\n",
        "\n",
        "Облако слов (Word Cloud) — это визуализация, содержащая слова из некоторого набора данных, при этом размер шрифта прямо пропорционален частотности слова в наборе"
      ],
      "metadata": {
        "id": "2qufhahGhkZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "STOPWORDS.update(['said', 'say', 'href', 'quot', 'AP'])\n",
        "\n",
        "def plot_wordcloud(class_label):\n",
        "  text = news.Description[news[news['Class Label']==class_label].index]\n",
        "  plt.figure(figsize = (7,15))\n",
        "  wc = WordCloud(max_words=500,background_color='White',stopwords=STOPWORDS,random_state=42)\n",
        "  wc.generate(\" \".join(text))\n",
        "  plt.imshow(wc)\n",
        "  plt.axis(\"off\")\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "MLvAkx0d7NJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_wordcloud(\"World\")"
      ],
      "metadata": {
        "id": "R-igx6TQLNgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_wordcloud(\"Sport\")"
      ],
      "metadata": {
        "id": "D7NgZAuYLhJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_wordcloud(\"Business\")"
      ],
      "metadata": {
        "id": "Sxh-J1KqLmfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_wordcloud(\"Science\")"
      ],
      "metadata": {
        "id": "ro6mgezWLryt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Предобработка и векторизация текста"
      ],
      "metadata": {
        "id": "YDXZvDiOfLrR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Объединим заголовок и содержание новости в один столбец."
      ],
      "metadata": {
        "id": "ljb0lNxHwkvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news['Summary'] = news['Title'] + ' ' + news['Description']\n",
        "news.head()"
      ],
      "metadata": {
        "id": "VkvcbUcroaub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вначале приведем все слова к нижнему регистру и почистим текст: унифицируем сокращения, удалим URL-адреса и HTML-теги."
      ],
      "metadata": {
        "id": "ffXfqtsJwvlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def text_clean(x):\n",
        "    x = str(x).lower()\n",
        "    x = x.replace('%',' percent').replace('₹',' rupee').replace('$',' dollar').replace('€',' euro')\\\n",
        "                                .replace(',000,000','m').replace('000','k').replace('′',\"'\").replace(\"’\",\"'\")\\\n",
        "                                .replace(\"won't\",\"will not\").replace(\"can't\",'can not').replace(\"shouldn't\",\"should not\")\\\n",
        "                                .replace(\"what's\",'\"what is\"').replace(\"that's\",'that is').replace(\"he's\",\"he is\")\\\n",
        "                                .replace(\"she's\",\"she is\").replace(\"it's\",\"it is\").replace(\"'ve\",\" have\").replace(\"'re\",\" are\")\\\n",
        "                                .replace(\"'ll\",\" will\").replace(\"i'm\",\"i am\").replace(\"n't\", \" not\")\n",
        "    x = re.sub(r'([0-9]+)000000',r'\\1m',x)\n",
        "    x = re.sub(r'([0-9]+)000',r'\\1k',x)\n",
        "    #remove urls\n",
        "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    x = url_pattern.sub(r'', x)\n",
        "    # remove_html\n",
        "    html_pattern = re.compile('<.*?>')\n",
        "    x = html_pattern.sub(r'', x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "Dduh7NUapG5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news.Summary=news.Summary.apply(text_clean)\n",
        "news.head()"
      ],
      "metadata": {
        "id": "nbN1hKCN6AKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Осуществим предобработку текста.\n",
        "Она должна включать:\n",
        "- токенизацию по словам,\n",
        "- удаление знаков препинания, чисел и стоп-слов,\n",
        "- лемматизацию.\n",
        "\n",
        "На выходе мы хотим получить строку из токенов после предобработки, разделенных пробелами.\n",
        "\n",
        "Необходимые инструменты для предобработки текстов есть в билиотеке [NLTK](https://www.nltk.org/)."
      ],
      "metadata": {
        "id": "6EwP9PPF0ZCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import WordNetLemmatizer\n",
        "\n",
        "def pos_mapping(text):\n",
        "    word_tag_array = []\n",
        "    for word_tag in nltk.pos_tag(text):\n",
        "        tag = word_tag[1][0]\n",
        "        tag_dict = {\"J\": \"a\",\n",
        "                    \"N\": \"n\",\n",
        "                    \"V\": \"v\",\n",
        "                    \"R\": \"r\"}\n",
        "        word_tag_array.append((word_tag[0], tag_dict.get(tag, \"n\")))\n",
        "    return word_tag_array\n",
        "\n",
        "def text_preprocess(text):\n",
        "    tokenized_text = word_tokenize(text)\n",
        "    STOPWORDS = set(stopwords.words(\"english\") + ['said', 'href', 'lt','gt',\n",
        "                                                  'ii','iii','ie','quot','com'])\n",
        "    text_without_stopwords = [word for word in tokenized_text\n",
        "                              if word.isalpha() and word not in STOPWORDS]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_text = [lemmatizer.lemmatize(word, tag) for word, tag in pos_mapping(text_without_stopwords)]\n",
        "    return ' '.join(lemmatized_text)"
      ],
      "metadata": {
        "id": "fOpq3BvV5DpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news.Summary=news.Summary.apply(text_preprocess)\n",
        "news.head()"
      ],
      "metadata": {
        "id": "O-8LZp976Crh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сравним тексты до и после предобработки."
      ],
      "metadata": {
        "id": "XaJRjW9tMqvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5,10):\n",
        "  print(f\"Title:\\n{news['Title'][i]}\")\n",
        "  print(f\"Description:\\n{news['Description'][i]}\")\n",
        "  print(f\"Summary after normalization:\\n{news['Summary'][i]}\\n\")"
      ],
      "metadata": {
        "id": "yIdRgl0nMtgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Запишем в отдельные переменные предобработанные тексты `X` и метки классов `y`."
      ],
      "metadata": {
        "id": "EICu5hBj6aAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = news['Summary'], news['Class Index']\n",
        "X[:5], y[:5]"
      ],
      "metadata": {
        "id": "XPiNial46Udh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Разделим данные на обучающую и тестовую выборку с помощью метода [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) (random_state=42)."
      ],
      "metadata": {
        "id": "cMpOSsjK6mbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "X_train.shape, X_test.shape"
      ],
      "metadata": {
        "id": "zdAu0Tap6k5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чтобы подавать тексты в модель машинного обучения, необходимо представить каждое предложение в виде набора признаков — вектора. В качестве признаков будем подавать модели слова, встретившиеся в обучающей выборке. Те слова, которые встретятся в тестовой выборке, но отсутствуют в обучающей, не могут быть проинтерпретироованы моделью.\n",
        "\n",
        "Будем использовать алгоритм векторизации TF-IDF. Словарь необходимо собирать на основе обучающей выборки. При этом преобразование текстов в векторы на основе собранного словаря нужно осуществить для всего датасета.\n",
        "\n",
        "Будем включать в словарь только те слова, которые встретились минимум 5 раз."
      ],
      "metadata": {
        "id": "Yy76buhX6qOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vect = TfidfVectorizer(min_df=5)\n",
        "#vect.fit(X_train)\n",
        "#X_train_vect = vect.transform(X_train)\n",
        "X_train_vect = vect.fit_transform(X_train)\n",
        "X_test_vect = vect.transform(X_test)\n",
        "X_train_vect, X_test_vect"
      ],
      "metadata": {
        "id": "_eFwCN7p6r6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Обучение и тестирование моделей"
      ],
      "metadata": {
        "id": "YcOdbTLep5-b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для классификации будем использовать готовые методы из библиотеки [scikit-learn](https://scikit-learn.org/stable/) для машинного обучения.\n",
        "\n",
        "Обучим модели на основе 4 алгоритмов:\n",
        "- Наивный байесовский классификатор ([MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html))\n",
        "- Логистическая регрессия ([LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html))\n",
        "- Дерево решений ([DecisionTreeClassifier](https://scikit-learn.org/dev/modules/generated/sklearn.tree.DecisionTreeClassifier.html))\n",
        "- Метод случайного леса ([RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html))\n",
        "\n",
        "У них стандартные функции:\n",
        "- `fit` обучает модель на обучающей выборке\n",
        "- `predict` предсказывает классы на тестовой выборке"
      ],
      "metadata": {
        "id": "bJO2RLqbzkBN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Составление матрицы ошибок и подсчет всех метрик также может осуществляться инструментами sklearn.\n",
        "- [матрица ошибок](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)\n",
        "- [точность](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)\n",
        "- [полнота](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)\n",
        "- [F-мера](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)\n",
        "\n",
        "Чтобы не считать все метрики по отдельности, можно сразу получить [отчет о классификации](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html):\n",
        "- метрики для каждого класса\n",
        "  - точность (precision)\n",
        "  - полнота (recall)\n",
        "  - F-мера (f1-score)\n",
        "  - количество объектов каждого класса (support)\n",
        "- усредненные метрики\n",
        "  - микроусредненные (micro avg)\n",
        "  - макроусредненные (macro avg)\n",
        "  - взвешенные (weighted avg)\n",
        "  \n",
        "Если микроусредненные точность, полнота и F-мера равны, выводится одно значение, равное также accuracy."
      ],
      "metadata": {
        "id": "zzkpdyGd5NXm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def show_confusion_matrix(confusion_matrix):\n",
        "  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Greens\")\n",
        "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
        "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
        "  plt.ylabel('True emotion')\n",
        "  plt.xlabel('Predicted emotion')\n",
        "\n",
        "class_names = ['World', 'Sport', 'Business', 'Science']"
      ],
      "metadata": {
        "id": "v33cJDFC82rV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Наивный байесовский классификатор"
      ],
      "metadata": {
        "id": "2eDucxYy-OyT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучим модель — [Наивный байесовский классификатор](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) — и запишем предсказания."
      ],
      "metadata": {
        "id": "gNTCZO-N90hx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(X_train_vect, y_train)\n",
        "y_mnb = mnb.predict(X_test_vect)"
      ],
      "metadata": {
        "id": "a_oZ71zH9Ng-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(20,25):\n",
        "  print(f\"Text:\\n{news['Description'][X_test.index[i]]}\")\n",
        "  print(f\"True label: {class_names[y_test[y_test.index[i]]-1]}\")\n",
        "  print(f\"Predicted label: {class_names[y_mnb[i]-1]}\\n\")"
      ],
      "metadata": {
        "id": "4f13_UrhKwmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выведем отчет о классификации и изобразим матрицу ошибок в виде тепловой карты."
      ],
      "metadata": {
        "id": "csnNyHV69-FO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_mnb, target_names=class_names))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_mnb)\n",
        "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
        "show_confusion_matrix(df_cm)"
      ],
      "metadata": {
        "id": "-J_B7_-M9Zf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Логистическая регрессия"
      ],
      "metadata": {
        "id": "-sx09ctS-S68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучим модель [логистической регрессии](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) и запишем её предсказания.\n",
        "\n",
        "Укажем количество итераций, равное 200, и случайное состояние 42."
      ],
      "metadata": {
        "id": "BftMRP3W9_bo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logreg = LogisticRegression(max_iter=200, random_state=42)\n",
        "logreg.fit(X_train_vect, y_train)\n",
        "y_logreg = logreg.predict(X_test_vect)"
      ],
      "metadata": {
        "id": "hx3EU-bI7E-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(20,25):\n",
        "  print(f\"Text:\\n{news['Description'][X_test.index[i]]}\")\n",
        "  print(f\"True label: {class_names[y_test[y_test.index[i]]-1]}\")\n",
        "  print(f\"Predicted label: {class_names[y_logreg[i]-1]}\\n\")"
      ],
      "metadata": {
        "id": "HVWqKYQiK7s9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_logreg, target_names=class_names))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_logreg)\n",
        "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
        "show_confusion_matrix(df_cm)"
      ],
      "metadata": {
        "id": "NrDGSzDn8iy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для каждого класса были посчитаны свои коэффициенты регрессии. Они представляют матрицу $K \\times n$, где $K$ — количество классов, $n$ — количество признаков (слов)."
      ],
      "metadata": {
        "id": "GeDL-LWNCTHi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "coefficient_matrix = logreg.coef_\n",
        "print(coefficient_matrix)\n",
        "print(coefficient_matrix.shape)"
      ],
      "metadata": {
        "id": "8FoOG-2qEaR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Каждый из коэффициентов регрессии описывает размер вклада соответствующего признака.\n",
        " - Положительный коэффициент — признак повышает вероятность принадлежности к классу, отрицательный коэффициент — признак уменьшает вероятность.\n",
        " - Большой коэффициент — признак существенно влияет на вероятность принадлежности к классу, почти нулевой коэффициент — признак имеет небольшое влияние на вероятность результата.\n",
        "\n",
        "Выведем признаки с самыми большими коэффициентами для каждого класса и получим наиболее характерные слова для каждой эмоции.\n",
        "- Необходимо определить порядок, в котором должны быть записаны коэффициенты и слова по возрастанию. Это можно сделать с помощью метода `.argsort()`."
      ],
      "metadata": {
        "id": "MIFLg8Y_FgYc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "order = coefficient_matrix[0].argsort()\n",
        "order"
      ],
      "metadata": {
        "id": "dYpgzn_EfYic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- В этом порядке нужно сортировать коэффициенты `coefficient_matrix[0]` и слова `tf_idf.get_feature_names_out()`. Чтобы получить порядок по убыванию, нужно развернуть массивы."
      ],
      "metadata": {
        "id": "llY1-7cBgDNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "coefficient_matrix[0][order][::-1]"
      ],
      "metadata": {
        "id": "I4oewyEZgjTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vect.get_feature_names_out()[order][::-1]"
      ],
      "metadata": {
        "id": "IFB8Fhd3gMPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выполним то же самое в цикле, проходя по коэффициентам каждого класса.\n",
        "- Создаем список признаков-слов\n",
        "- Выводим название класса\n",
        "- Записываем индексы в порядке возрастания коэффициентов (`order`)\n",
        "- Сортируем коэффициенты (`class_coefficients`) и слова (`feature_names`), переворачиваем массив и записываем топ-5 значений\n",
        "- Выводим слова и коэффициенты"
      ],
      "metadata": {
        "id": "a77z5JELg34U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(coefficient_matrix.shape[0]):\n",
        "\n",
        "    print(f\"\\n{class_names[i]}:\")\n",
        "\n",
        "    feature_names = vect.get_feature_names_out() # список признаков-слов\n",
        "    order = coefficient_matrix[i].argsort()\n",
        "    class_coefficients = coefficient_matrix[i][order][::-1][:5]\n",
        "    feature_names = feature_names[order][::-1][:5]\n",
        "\n",
        "    for feature, coefficient in zip(feature_names, class_coefficients):\n",
        "      print(feature, coefficient.round(2))"
      ],
      "metadata": {
        "id": "DQ0VgyHWCLVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Дерево решений"
      ],
      "metadata": {
        "id": "xuB_qqZN-WwX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучение модели на основе [дерева решений](https://scikit-learn.org/dev/modules/generated/sklearn.tree.DecisionTreeClassifier.html) займет около 3 минут.\n",
        "\n",
        "Загрузим уже обученную модель с помощью метода `load()` модуля [joblib](https://joblib.readthedocs.io/en/latest/generated/joblib.load.html) и запишем предсказания (см. [пример](https://scikit-learn.org/1.3/model_persistence.html))."
      ],
      "metadata": {
        "id": "G5aEn3my9yKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "dectree = DecisionTreeClassifier(random_state=42)\n",
        "dectree.fit(X_train_vect, y_train)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "LPgEwgR59o0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/Xeanst/NLP_course_FBB/main/1_intro_to_nlp/dectree.joblib"
      ],
      "metadata": {
        "id": "wFcnBuF1MFxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import load\n",
        "\n",
        "dectree = load('dectree.joblib')\n",
        "y_dectree = dectree.predict(X_test_vect)"
      ],
      "metadata": {
        "id": "XpSezYExXEX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(20,25):\n",
        "  print(f\"Text:\\n{news['Description'][X_test.index[i]]}\")\n",
        "  print(f\"True label: {class_names[y_test[y_test.index[i]]-1]}\")\n",
        "  print(f\"Predicted label: {class_names[y_dectree[i]-1]}\\n\")"
      ],
      "metadata": {
        "id": "0R1fUIWEMF2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_dectree, target_names=class_names))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_dectree)\n",
        "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
        "show_confusion_matrix(df_cm)"
      ],
      "metadata": {
        "id": "MKq-1j5Ks-Jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Плюсом деревьев решений является их интерпретируемость — для каждого предсказания можно вывести цепочку условий, которая привела к такому выводу."
      ],
      "metadata": {
        "id": "un4ky7VHtNoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import export_text\n",
        "\n",
        "text_representation = export_text(dectree,\n",
        "                                  class_names=class_names,\n",
        "                                  feature_names=list(vect.get_feature_names_out()))\n",
        "print(text_representation)"
      ],
      "metadata": {
        "id": "UZf60YZfUTC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Метод случайного леса"
      ],
      "metadata": {
        "id": "REgIVWER_C0i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Поскольку метод случайного леса является ансамблем деревьев решений, обучение модели займет больше времени (около 7 минут). Сама модель весит больше 100МБ, поэтому загружена на гитхаб в виде архива.\n",
        "\n",
        "Скачаем модель и разархивируем её."
      ],
      "metadata": {
        "id": "TYn1wT2JKPgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rfc = RandomForestClassifier(random_state=42)\n",
        "rfc.fit(X_train_vect, y_train)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "sbrnU69-tHj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/Xeanst/NLP_course_FBB/main/1_intro_to_nlp/rfc.zip"
      ],
      "metadata": {
        "id": "O5vEi9AOXkhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/rfc.zip"
      ],
      "metadata": {
        "id": "ggrs7hr2XoF1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузим уже обученную модель с помощью метода `load()` модуля joblib и запишем предсказания."
      ],
      "metadata": {
        "id": "dzz_jV2hMXE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import load\n",
        "\n",
        "rfc = load('rfc.joblib')\n",
        "y_rfc = rfc.predict(X_test_vect)"
      ],
      "metadata": {
        "id": "dgjegbLQXxls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(20,25):\n",
        "  print(f\"Text:\\n{news['Description'][X_test.index[i]]}\")\n",
        "  print(f\"True label: {class_names[y_test[y_test.index[i]]-1]}\")\n",
        "  print(f\"Predicted label: {class_names[y_rfc[i]-1]}\\n\")"
      ],
      "metadata": {
        "id": "AzGNcJxTQ47W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_rfc, target_names=class_names))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_rfc)\n",
        "df_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
        "show_confusion_matrix(df_cm)"
      ],
      "metadata": {
        "id": "DPYqWVbX-k1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Сравнение результатов"
      ],
      "metadata": {
        "id": "ZeuHgo-i-uFr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Отобразим результаты для всех 4 моделей в виде столбчатой диаграммы."
      ],
      "metadata": {
        "id": "pmVj1Pbx_QYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnb_f1 = round(f1_score(y_mnb, y_test, average='weighted')*100,4)\n",
        "logreg_f1 = round(f1_score(y_logreg, y_test, average='weighted')*100,4)\n",
        "dectree_f1 = round(f1_score(y_dectree, y_test, average='weighted')*100,4)\n",
        "rfc_f1 = round(f1_score(y_rfc, y_test, average='weighted')*100,4)\n",
        "\n",
        "sns.set()\n",
        "fig = plt.figure()\n",
        "ax = fig.add_axes([0,0,1,1])\n",
        "Models = [\"Naive\\nBayes\\nclassifier\", \"Logistic\\nRegression\",\n",
        "          \"Decison\\nTree\", \"Random\\nForest\\nclassifier\"]\n",
        "Accuracy=[mnb_f1, logreg_f1, dectree_f1, rfc_f1]\n",
        "ax.bar(Models,Accuracy,color=['#702963','#8a2be2', '#9966cc', '#cf2aed']);\n",
        "for i in ax.patches:\n",
        "    ax.text(i.get_x()+.1, i.get_height()-7.8, str(round(i.get_height(),2))+'%', fontsize=20, color='white')\n",
        "plt.title('Comparison of Different Classification Models')\n",
        "plt.ylabel('F1 score')\n",
        "plt.xlabel('Classification Models')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QIIgW2uz-zI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мы получили довольно высокое качество, но можно ли еще улучшить его? Например, за счет гиперпараметров векторизации?"
      ],
      "metadata": {
        "id": "G1mN159D3FZK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Подбор гиперпараметров"
      ],
      "metadata": {
        "id": "mfiFS2vuKphb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Можно произвести **несколько разбиений** датасета на **обучающий и валидационный**, чтобы получить уверенность оценок качества для моделей с разными гиперпараметрами.\n",
        "\n",
        "Такой подход называется [K-Fold кросс-валидация](https://scikit-learn.org/stable/modules/cross_validation.html) — перекрестная проверка.\n",
        "\n",
        "1. Фиксируется целое число $k$, меньшее числа примеров в датасете.\n",
        "2. Датасет разбивается на $k$ одинаковых частей.\n",
        "3. Происходит $k$ итераций, в каждой из которых одна часть выступает в роли валидационного множества, а объединение остальных — в роли тренировочного.\n",
        "4. Финальный результат модели измеряется на отложенном тестовом множестве, не участвовавшем в кросс-валидации."
      ],
      "metadata": {
        "id": "Vo0JPnwY_pNp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src =\"https://i.postimg.cc/Wpfzmq7K/cross_validation.png\" width=\"600\"></center>"
      ],
      "metadata": {
        "id": "qhmQ43Nz4NVB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попробуем определить наиболее оптимальные параметры векторизации с помощью кросс-валидации.\n",
        "\n",
        "Все этапы обработки — векторизацию и классификацию — объединим  в [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html).\n",
        "\n",
        "В качестве алгоритма для векторизации будем использовать [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html), для классификации обучим [MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)."
      ],
      "metadata": {
        "id": "BjI4lm8ADNcW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "pipeline = Pipeline([\n",
        "           ('vect', TfidfVectorizer()),\n",
        "           ('clf', MultinomialNB())\n",
        "])"
      ],
      "metadata": {
        "id": "guQszl5wKs1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создайдим словарь `parameters`, содержащий диапазон значений каждого параметра:\n",
        "- `ngram_range`\n",
        "\n",
        "По умолчанию в качестве признаков используются слова — униграммы (`default=(1, 1)`). С помощью параметра `ngram_range` можно считать частоту встречаемости для *n*-грамм. Необходимо задать значения `min_n` и `max_n`. Например, при `ngram_range=(1, 3)` в словарь войдут униграммы, биграммы и триграммы.\n",
        "- `max_df`\n",
        "\n",
        "По умолчанию включаются слова из всех 100% документов (`default=1.0`). Можно ограничить размер словаря и включать только слова, которые встречаются слишком часто и являются стоп-словами в пределах данного корпуса. Например, при `max_df=0.5` — исключаем токены, которые встретились в больше чем 50% документов."
      ],
      "metadata": {
        "id": "Ej9enaPWDYV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parameters = {\n",
        "    'vect__ngram_range': ((1,1), (1, 3)),\n",
        "    'vect__max_df': (0.5, 1.0)\n",
        "}"
      ],
      "metadata": {
        "id": "RRS6EcIbLd6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для автоматического подбора параметров будем использовать модуль [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). Он создает модель для каждой возможной комбинации параметров.\n",
        "\n",
        "Для этого необходимо инициализировать объект `grid_search`, передавая ему пайплайн (векторизатор и модель). Установим количество кросс-валидаций, равное 5 (`cv=5`)."
      ],
      "metadata": {
        "id": "jfGyAaEn3Pmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "grid_search = GridSearchCV(pipeline,\n",
        "                           parameters,\n",
        "                           n_jobs=-1,\n",
        "                           verbose=1,\n",
        "                           cv=5)\n",
        "grid_search.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "j7rmVdRFLyU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выведем лучшие значения параметров."
      ],
      "metadata": {
        "id": "USpEZTfz3mBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search.best_params_"
      ],
      "metadata": {
        "id": "-EHH6GK23o_r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Запишем результаты кросс-валидации в виде датафрейма и отобразим их на графике."
      ],
      "metadata": {
        "id": "ZU2nXbQG3tBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grid_search_df = pd.DataFrame(grid_search.cv_results_).sort_values('rank_test_score').reset_index(drop=True)\n",
        "grid_search_df = grid_search_df.drop([\n",
        "            'mean_fit_time',\n",
        "            'std_fit_time',\n",
        "            'mean_score_time',\n",
        "            'std_score_time',\n",
        "            'params',\n",
        "            'std_test_score'],\n",
        "            axis=1)\n",
        "grid_search_df['param_vect__ngram_range'] = grid_search_df['param_vect__ngram_range'].astype(str)\n",
        "grid_search_df"
      ],
      "metadata": {
        "id": "EsnSfRb_3zjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "fig, axs = plt.subplots(ncols=2, nrows=1)\n",
        "sns.set(style=\"whitegrid\", color_codes=True)\n",
        "fig.set_size_inches(10,5)\n",
        "\n",
        "sns.barplot(x='param_vect__ngram_range', y='mean_test_score', data=grid_search_df, ax=axs[0], color='lightgrey')\n",
        "axs[0].set_title(label = 'vect__ngram_range', size=10, weight='bold')\n",
        "\n",
        "sns.barplot(x='param_vect__max_df', y='mean_test_score', data=grid_search_df, ax=axs[1], color='coral')\n",
        "axs[1].set_title(label = 'vect__max_df', size=10, weight='bold')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Dw6Q_vEWm_9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Инициализируем пайплайн с параметрами векторизатора, которые оказались наилучшими при кросс-валидации."
      ],
      "metadata": {
        "id": "44oQOEM6IcnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline([\n",
        "           ('vect', TfidfVectorizer(ngram_range=grid_search.best_params_['vect__ngram_range'],\n",
        "                                    max_df=grid_search.best_params_['vect__max_df'])),\n",
        "           ('clf', MultinomialNB())\n",
        "])"
      ],
      "metadata": {
        "id": "g5XnBjmxL8qS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучим модель и запишем её предсказания на тестовых данных. Выведем отчет о классификации."
      ],
      "metadata": {
        "id": "ZUyCBY2sIxIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.fit(X_train, y_train)\n",
        "y_pred = pipeline.predict(X_test)\n",
        "class_names = ['World', 'Sport', 'Business', 'Science']\n",
        "print(classification_report(y_test, y_pred, target_names=class_names))"
      ],
      "metadata": {
        "id": "fihyAsMcSU_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "📌 Изменилось ли качество за счет автоматического подбора гиперпараметров?"
      ],
      "metadata": {
        "id": "tCKyPFOkM6BX"
      }
    }
  ]
}