{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "В этом блокноте мы рассмотрим некоторые практические аспекты использования больших языковых моделей, а именно доступность и подбор промтов."
      ],
      "metadata": {
        "id": "XevGGl0RN77g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Использование LLM в Google Colab"
      ],
      "metadata": {
        "id": "exzWPkJ8a-lG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Существует несколько способов использовать большие языковые модели в среде Google Colab. Для всех них необходимы ключи доступа, которые можно хранить в разделе \"Секреты\"."
      ],
      "metadata": {
        "id": "_077ucSsbBLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "rdxidYbjq0zT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Yandex Cloud"
      ],
      "metadata": {
        "id": "Ceaqk4itGL9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Доступны](https://yandex.cloud/ru/docs/foundation-models/concepts/yandexgpt/models) модели серии YandexGPT и LLama.\n",
        "\n",
        "Плюсы: быстрая работа.\n",
        "\n",
        "Минусы: платное использование по завершении приветственного гранта в 4000 рублей.\n",
        "\n",
        "Цена за 1000 токенов:\n",
        "- YandexGPT Lite (`yandexgpt-lite`) — 0,20 ₽\n",
        "- YandexGPT Pro (`yandexgpt`) — 1,20 ₽\n",
        "- Llama 8b (`llama-lite`) — 0,20 ₽\n",
        "- Llama 70b (`llama`) — 1,20 ₽\n",
        "\n",
        "Для доступа необходимы API-ключ и идентификатор модели."
      ],
      "metadata": {
        "id": "OloRmk66L6zr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQt_Y0A9E6I3"
      },
      "outputs": [],
      "source": [
        "url = \"https://llm.api.cloud.yandex.net/foundationModels/v1/completion\"\n",
        "yandex_gpt = userdata.get(\"yandex_gpt\")\n",
        "headers = {\n",
        "    \"Content-Type\": \"application/json\",\n",
        "    \"Authorization\": f\"Api-Key {yandex_gpt}\" # ваш секретный ключ\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import requests\n",
        "import json\n",
        "\n",
        "prompt = {\n",
        "    \"modelUri\": \"gpt://b1gu40ajd726f9s9h843/yandexgpt\", # идентификатор модели\n",
        "    \"completionOptions\": {\n",
        "        \"stream\": False, # отключение режима диалога\n",
        "        \"temperature\": 1 # степень рандомности генерации\n",
        "        },\n",
        "    \"messages\": [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"text\": \"Что изучает лингвистика?\"\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "\n",
        "response = requests.post(url, headers=headers, json=prompt).text\n",
        "print(response)\n",
        "text = json.loads(response)[\"result\"][\"alternatives\"][0][\"message\"][\"text\"]\n",
        "print(text)"
      ],
      "metadata": {
        "id": "JMf5sOn_GWDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def yandexgpt(content):\n",
        "  prompt = {\n",
        "      \"modelUri\": \"gpt://b1gu40ajd726f9s9h843/yandexgpt\",\n",
        "      \"completionOptions\": {\n",
        "          \"stream\": False,\n",
        "          \"temperature\": 1\n",
        "          },\n",
        "      \"messages\": [\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"text\": content\n",
        "              }\n",
        "          ]\n",
        "      }\n",
        "\n",
        "  response = requests.post(url, headers=headers, json=prompt).text\n",
        "  text = json.loads(response)[\"result\"][\"alternatives\"][0][\"message\"][\"text\"]\n",
        "  return text"
      ],
      "metadata": {
        "id": "18uCK0ZuNOtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "questions = [\"Что изучает лингвистика?\",\n",
        "             \"Чем отличаются фонетика и фонология?\",\n",
        "             \"Почему языки такие разные?\"]\n",
        "\n",
        "for question in questions:\n",
        "  answer = yandexgpt(question)\n",
        "  print(f\"Вопрос:\\n{question}\")\n",
        "  print(f\"Ответ:\\n{answer}\\n\")\n",
        "  print()"
      ],
      "metadata": {
        "id": "pId5gLfvNkdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Сбер"
      ],
      "metadata": {
        "id": "ChQfN9Fba8RP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Доступны](https://developers.sber.ru/docs/ru/gigachat/api/tariffs) модели серии GigaChat: Lite (`GigaChat-Lite`), Pro (`GigaChat-Pro`) и Max (`GigaChat-Max`).\n",
        "\n",
        "Плюсы: быстрая работа.\n",
        "\n",
        "Минусы: платное использование по завершении приветственного гранта на 50 000 токенов для каждой модели.\n",
        "\n",
        "Тарификация:\n",
        "\n",
        "- 5 000 000 токенов GigaChat Lite — 1 000 ₽\n",
        "- 30 000 000 токенов GigaChat Lite — 5 820 ₽\n",
        "- 1 000 000 токенов GigaChat Pro — 1 500 ₽\n",
        "- 5 000 000 токенов GigaChat Pro — 7 275 ₽\n",
        "- 1 000 000 токенов GigaChat Max — 1 950 ₽\n",
        "- 4 000 000 токенов GigaChat Max — 7 566 ₽\n",
        "\n",
        "Для доступа необходимы идентификатор клиента (`RqUID`) и API-ключ (`Authorization`), которые позволяют получить токен доступа (`access_token`). Также нужно указать идентификатор модели."
      ],
      "metadata": {
        "id": "XRQvLirjbS6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "url = \"https://ngw.devices.sberbank.ru:9443/api/v2/oauth\"\n",
        "\n",
        "payload={\n",
        "  'scope': 'GIGACHAT_API_PERS'\n",
        "}\n",
        "\n",
        "gc_RqUID = userdata.get(\"gc_RqUID\")\n",
        "gc_key = userdata.get(\"gc_key\")\n",
        "headers = {\n",
        "  'Content-Type': 'application/x-www-form-urlencoded',\n",
        "  'Accept': 'application/json',\n",
        "  'RqUID': gc_RqUID,\n",
        "  'Authorization': f'Basic {gc_key}'\n",
        "}\n",
        "\n",
        "response = requests.post(url, headers=headers, data=payload, verify=False)\n",
        "token = response.json().get('access_token')\n",
        "print(token)"
      ],
      "metadata": {
        "id": "sCRh8WOgcnbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "url = \"https://gigachat.devices.sberbank.ru/api/v1/chat/completions\"\n",
        "headers = {\n",
        "    'Authorization': 'Bearer ' + token,\n",
        "    'Content-Type': 'application/json',\n",
        "    }\n",
        "payload=json.dumps({\n",
        "    \"model\": \"GigaChat-Pro\",\n",
        "    \"messages\": [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Что изучает лингвистика?\"\n",
        "            }\n",
        "        ],\n",
        "    \"temperature\": 1,\n",
        "    \"stream\": False,\n",
        "    })\n",
        "response = requests.post(url, headers=headers, data=payload, verify=False)\n",
        "print(response.text)\n",
        "text = response.json()['choices'][0]['message']['content']\n",
        "print(text)"
      ],
      "metadata": {
        "id": "tdtZ-3CldO9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gigachat(content):\n",
        "  url = \"https://gigachat.devices.sberbank.ru/api/v1/chat/completions\"\n",
        "  headers = {\n",
        "      'Authorization': f'Bearer {token}',\n",
        "      'Content-Type': 'application/json',\n",
        "      }\n",
        "  payload=json.dumps({\n",
        "      \"model\": \"GigaChat-Pro\",\n",
        "      \"messages\": [\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": content\n",
        "              }\n",
        "          ],\n",
        "      \"temperature\": 1,\n",
        "      \"stream\": False,\n",
        "      })\n",
        "  response = requests.post(url, headers=headers, data=payload, verify=False)\n",
        "  text = response.json()['choices'][0]['message']['content']\n",
        "  return text"
      ],
      "metadata": {
        "id": "4koJOd2wduFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "questions = [\"Что изучает лингвистика?\",\n",
        "             \"Чем отличаются фонетика и фонология?\",\n",
        "             \"Почему языки такие разные?\"]\n",
        "\n",
        "for question in questions:\n",
        "  answer = gigachat(question)\n",
        "  print(f\"Вопрос:\\n{question}\")\n",
        "  print(f\"Ответ:\\n{answer}\\n\")\n",
        "  print()"
      ],
      "metadata": {
        "id": "3ye062dfeKyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mistral AI"
      ],
      "metadata": {
        "id": "gisryB6FeeSo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Доступны](https://docs.mistral.ai/getting-started/models/models_overview/) модели серии Mistral:\n",
        "- мультиязычные LLM\n",
        "  - Mistral Large (`mistral-large-latest`)\n",
        "  - Ministral 8B (`ministral-8b-latest`)\n",
        "  - Ministral 3B\t(`ministral-3b-latest`)\n",
        "- LLM для языков Ближнего Востока и Южной Азии\n",
        "  - Mistral Saba (`mistral-saba-latest`)\n",
        "- LLM для кода\n",
        "  - Codestral (`codestral-latest`)\n",
        "- мультимодальная LLM\n",
        "  - Pixtral Large (`pixtral-large-latest`)\n",
        "\n",
        "Плюсы: бесплатное использование.\n",
        "\n",
        "Минусы: долгая работа.\n",
        "\n",
        "Для доступа необходимы API-ключ и идентификатор модели."
      ],
      "metadata": {
        "id": "7ud0o4OAekUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import requests\n",
        "\n",
        "mistral = userdata.get(\"mistral\")\n",
        "headers = {\n",
        "      'Authorization': f'Bearer {mistral}',\n",
        "      'Content-Type': 'application/json',\n",
        "      }\n",
        "payload = {\n",
        "    \"model\": \"mistral-large-latest\",\n",
        "    \"temperature\": 1,\n",
        "    \"stream\": False,\n",
        "    \"messages\": [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Что изучает лингвистика?\"\n",
        "            }\n",
        "        ]\n",
        "    }\n",
        "response = requests.post(\"https://api.mistral.ai/v1/chat/completions\", headers=headers, json=payload)\n",
        "print(response.json())\n",
        "text = response.json()['choices'][0]['message']['content']\n",
        "print(text)"
      ],
      "metadata": {
        "id": "bfx7ms5TgGG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def mistral(content):\n",
        "  mistral = userdata.get(\"mistral\")\n",
        "  headers = {\n",
        "        'Authorization': f'Bearer {mistral}',\n",
        "        'Content-Type': 'application/json',\n",
        "        }\n",
        "  payload = {\n",
        "      \"model\": \"mistral-large-latest\",\n",
        "      \"temperature\": 1,\n",
        "      \"stream\": False,\n",
        "      \"messages\": [\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": content\n",
        "              }\n",
        "          ]\n",
        "      }\n",
        "  response = requests.post(\"https://api.mistral.ai/v1/chat/completions\", headers=headers, json=payload)\n",
        "  text = response.json()['choices'][0]['message']['content']\n",
        "  return text"
      ],
      "metadata": {
        "id": "hQVIp3X7hO-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "questions = [\"Что изучает лингвистика?\",\n",
        "             \"Чем отличаются фонетика и фонология?\",\n",
        "             \"Почему языки такие разные?\"]\n",
        "\n",
        "for question in questions:\n",
        "  answer = mistral(question)\n",
        "  print(f\"Вопрос:\\n{question}\")\n",
        "  print(f\"Ответ:\\n{answer}\\n\")\n",
        "  print()"
      ],
      "metadata": {
        "id": "ONZFaviQiIPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Together AI"
      ],
      "metadata": {
        "id": "ivYQ_7rFkHs6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Доступно](https://api.together.ai/models) достаточно много разных моделей по разным тарифам:\n",
        "- DeepSeek R1\n",
        "- Qwen 2.5 Instruct\n",
        "- Gemma Instruct\n",
        "\n",
        "Однако есть две бесплатные модели:\n",
        "- Meta Llama 3.3 70B Instruct Turbo Free (`meta-llama/Llama-3.3-70B-Instruct-Turbo-Free`)\n",
        "- DeepSeek R1 Distill Llama 70B Free (`deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free`)\n",
        "\n",
        "Плюсы: возможно бесплатное использование.\n",
        "\n",
        "Минусы: долгая работа.\n",
        "\n",
        "Для доступа необходимы API-ключ и идентификатор модели."
      ],
      "metadata": {
        "id": "cO6HPXuokN_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import os\n",
        "import openai\n",
        "\n",
        "together_ai = userdata.get(\"together_ai\")\n",
        "client = openai.OpenAI(\n",
        "    api_key=together_ai,\n",
        "    base_url=\"https://api.together.xyz/v1\",\n",
        "    )\n",
        "response = client.chat.completions.create(\n",
        "    model= \"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\",\n",
        "    temperature=1,\n",
        "    stream=False,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Что изучает лингвистика?\"\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "print(response)\n",
        "text = response.choices[0].message.content\n",
        "print(text)"
      ],
      "metadata": {
        "id": "0I4-In6Ald4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def llama(content):\n",
        "  together_ai = userdata.get(\"together_ai\")\n",
        "  client = openai.OpenAI(\n",
        "      api_key=together_ai,\n",
        "      base_url=\"https://api.together.xyz/v1\",\n",
        "      )\n",
        "  response = client.chat.completions.create(\n",
        "      model= \"meta-llama/Llama-3.3-70B-Instruct-Turbo-Free\",\n",
        "      temperature=1,\n",
        "      stream=False,\n",
        "      messages=[\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": content\n",
        "              }\n",
        "          ]\n",
        "      )\n",
        "  text = response.choices[0].message.content\n",
        "  return text"
      ],
      "metadata": {
        "id": "OjvKd_HZmR36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "questions = [\"Что изучает лингвистика?\",\n",
        "             \"Чем отличаются фонетика и фонология?\",\n",
        "             \"Почему языки такие разные?\"]\n",
        "\n",
        "for question in questions:\n",
        "  answer = llama(question)\n",
        "  print(f\"Вопрос:\\n{question}\")\n",
        "  print(f\"Ответ:\\n{answer}\\n\")\n",
        "  print()"
      ],
      "metadata": {
        "id": "tdkdgk2umjGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hugging Face"
      ],
      "metadata": {
        "id": "fgxllJRsmzDn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "На Hugging Face [доступно](https://huggingface.co/models?pipeline_tag=text-generation&sort=trending) огромное количество моделей. Модели, дообученные с помощью SFT, обозначены как Instruct.\n",
        "\n",
        "Возможность использования графического процессора в Google Colab позволяет напрямую импортировать модели до 2 миллиардов параметров.\n",
        "\n",
        "Плюсы: бесплатное использование.\n",
        "\n",
        "Минусы: долгая работа, ограничения на размер модели.\n",
        "\n",
        "Для доступа необходимы GPU и идентификатор модели."
      ],
      "metadata": {
        "id": "eRstOD6jm2Gn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -q"
      ],
      "metadata": {
        "id": "pgW67lamnYDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "\n",
        "device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "qwen_1_5_b = pipeline(\"text-generation\",\n",
        "                      model=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
        "                      temperature=1,\n",
        "                      max_new_tokens=1000,\n",
        "                      device=device)"
      ],
      "metadata": {
        "id": "npFKLJW3ocux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Что изучает лингвистика?\"\n",
        "        },\n",
        "]\n",
        "response = qwen_1_5_b(messages)\n",
        "print(response)\n",
        "text = response[0][\"generated_text\"][1][\"content\"]\n",
        "print(text)"
      ],
      "metadata": {
        "id": "9o_ZjeN6oC3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def qwen(content):\n",
        "  messages = [\n",
        "      {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": content\n",
        "          },\n",
        "      ]\n",
        "  response = qwen_1_5_b(messages)\n",
        "  text = response[0][\"generated_text\"][1][\"content\"]\n",
        "  return text"
      ],
      "metadata": {
        "id": "1K1fNVLNuVWr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "questions = [\"Что изучает лингвистика?\",\n",
        "             \"Чем отличаются фонетика и фонология?\",\n",
        "             \"Почему языки такие разные?\"]\n",
        "\n",
        "for question in questions:\n",
        "  answer = qwen(question)\n",
        "  print(f\"Вопрос:\\n{question}\")\n",
        "  print(f\"Ответ:\\n{answer}\\n\")\n",
        "  print()"
      ],
      "metadata": {
        "id": "g_sOjG5TthI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Модели большего размера доступны через [InferenceClient](https://huggingface.co/docs/huggingface_hub/v0.16.2/en/package_reference/inference_client#huggingface_hub.InferenceClient).\n",
        "\n",
        "Плюсы: бесплатное использование.\n",
        "\n",
        "Минусы: ограничение на 1000 запросов в сутки, долгая работа.\n",
        "\n",
        "Для доступа необходимы токен и идентификатор модели."
      ],
      "metadata": {
        "id": "qcE6jPVuwmtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from huggingface_hub import InferenceClient\n",
        "\n",
        "hf_token = userdata.get(\"hf_token\")\n",
        "model_name = \"Qwen/Qwen2.5-72B-Instruct\"\n",
        "client = InferenceClient(model_name, token=hf_token)\n",
        "\n",
        "output = client.chat.completions.create(\n",
        "          messages=[\n",
        "              {\n",
        "                  \"role\": \"user\",\n",
        "                  \"content\": \"Что изучает лингвистика?\"\n",
        "                  },\n",
        "              ],\n",
        "          max_tokens=1000,\n",
        "          temperature=1)\n",
        "print(output)\n",
        "text = output.choices[0].get('message')['content']\n",
        "print(text)"
      ],
      "metadata": {
        "id": "qA3rjU0nxNyE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def qwen_72(content):\n",
        "  hf_token = userdata.get(\"hf_token\")\n",
        "  model_name = \"Qwen/Qwen2.5-72B-Instruct\"\n",
        "  client = InferenceClient(model_name, token=hf_token)\n",
        "\n",
        "  output = client.chat.completions.create(\n",
        "      messages=[\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": content\n",
        "              },\n",
        "          ],\n",
        "      max_tokens=1000,\n",
        "      temperature=1)\n",
        "  text = output.choices[0].get('message')['content']\n",
        "  return text"
      ],
      "metadata": {
        "id": "bokwaTBxyCLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "for question in questions:\n",
        "  answer = qwen_72(question)\n",
        "  print(f\"Вопрос:\\n{question}\")\n",
        "  print(f\"Ответ:\\n{answer}\\n\")\n",
        "  print()"
      ],
      "metadata": {
        "id": "OTzXxA1Gyg-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GroqCloud"
      ],
      "metadata": {
        "id": "hJ24J-Z4yukb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Доступны](https://console.groq.com/docs/rate-limits) различные модели (сайт открывается через VPN):\n",
        "- `llama-3.3-70b-versatile`\n",
        "- `qwen-2.5-32b`\n",
        "- `deepseek-r1-distill-qwen-32b`\n",
        "- `deepseek-r1-distill-llama-70b`\n",
        "\n",
        "Плюсы: бесплатное использование, быстрая работа.\n",
        "\n",
        "Минусы: ограничение на количество запросов (для каждой модели отличается).\n",
        "\n",
        "Для доступа необходимы API-ключ и идентификатор модели."
      ],
      "metadata": {
        "id": "XpzsITFpzE68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq -q"
      ],
      "metadata": {
        "id": "FzLYgwYt7nxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "from groq import Groq\n",
        "\n",
        "groq = userdata.get(\"groq\")\n",
        "client = Groq(\n",
        "    api_key=groq,\n",
        ")\n",
        "\n",
        "chat_completion = client.chat.completions.create(\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Что изучает лингвистика?\",\n",
        "        }\n",
        "    ],\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    temperature=1\n",
        ")\n",
        "\n",
        "print(chat_completion)\n",
        "text = chat_completion.choices[0].message.content\n",
        "print(text)"
      ],
      "metadata": {
        "id": "rTwwhx7u7M1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def deepseek(content):\n",
        "\n",
        "  groq = userdata.get(\"groq\")\n",
        "  client = Groq(\n",
        "      api_key=groq,\n",
        "  )\n",
        "\n",
        "  chat_completion = client.chat.completions.create(\n",
        "      messages=[\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": content\n",
        "              }\n",
        "          ],\n",
        "      model=\"llama-3.3-70b-versatile\",\n",
        "      temperature=1\n",
        "      )\n",
        "\n",
        "  text = chat_completion.choices[0].message.content\n",
        "  return text"
      ],
      "metadata": {
        "id": "7_RmIOQw8uIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "questions = [\"Что изучает лингвистика?\",\n",
        "             \"Чем отличаются фонетика и фонология?\",\n",
        "             \"Почему языки такие разные?\"]\n",
        "\n",
        "for question in questions:\n",
        "  answer = deepseek(question)\n",
        "  print(f\"Вопрос:\\n{question}\")\n",
        "  print(f\"Ответ:\\n{answer}\\n\")\n",
        "  print()"
      ],
      "metadata": {
        "id": "O17YjH3C9DQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt engineering"
      ],
      "metadata": {
        "id": "loovPi5D9XIQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Затравочное программирование (prompt engineering) — разработка и оптимизация затравок (промптов) для эффективного использования больших языковых моделей.\n",
        "\n",
        "Рассмотрим различные способы формулировки промптов на примере модуля ChatGroq библиотеки LangChain. Он позволяет использовать модели из GroqCloud."
      ],
      "metadata": {
        "id": "v3pRsGq79dD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain_groq -q"
      ],
      "metadata": {
        "id": "yCbKu5-K-lAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Базовое использование"
      ],
      "metadata": {
        "id": "dhKI3iLA_oGq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для инициализации модели необходим API-ключ и ее идентификатор."
      ],
      "metadata": {
        "id": "J3XXIB-f_rba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "groq = userdata.get(\"groq\")\n",
        "llm = ChatGroq(\n",
        "    temperature=1,\n",
        "    groq_api_key = groq,\n",
        "    model_name = \"llama-3.3-70b-versatile\"\n",
        ")"
      ],
      "metadata": {
        "id": "icxp385G-mmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "basic_prompt = \"Что изучает лингвистика?\"\n",
        "print(llm.invoke(basic_prompt).content)"
      ],
      "metadata": {
        "id": "T7DKkymv-pat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "В зависимости от конкретной формулировки промпта ответ может различаться."
      ],
      "metadata": {
        "id": "gkttXOMXB4je"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\n",
        "    \"Перечисли 4 примера применения искусственного интеллекта в здравоохранении\",\n",
        "    \"Объясни, как искусственный интеллект меняет область здравоохранения на 4 конкретных примерах\",\n",
        "    \"Ты врач. Опиши 4 способа, с помощью которых искусственный интеллект улучшил твою повседневную работу в больнице\"\n",
        "]\n",
        "\n",
        "for i, prompt in enumerate(prompts, 1):\n",
        "  print(f\"\\nПромпт {i}: \")\n",
        "  print(prompt)\n",
        "  print(\"\\nОтвет: \")\n",
        "  print(llm.invoke(prompt).content)\n",
        "  print(\"-\"*100)"
      ],
      "metadata": {
        "id": "R5mejUBMBNKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Структурированный промпт"
      ],
      "metadata": {
        "id": "X5yWAzd8Dk1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Модуль `PromptTemplate` позволяет использовать переменные внутри промпта."
      ],
      "metadata": {
        "id": "_RsGnxqL-vsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "structured_prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template = \"Что изучает {topic}?\"\n",
        ")\n",
        "chain = structured_prompt | llm\n",
        "input_variables = {\"topic\": \"лингвистика\"}\n",
        "output = chain.invoke(input_variables).content\n",
        "print(output)"
      ],
      "metadata": {
        "id": "5aPEEVcG-vTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fact_check_prompt = PromptTemplate(\n",
        "    input_variables=[\"statement\"],\n",
        "    template=\"\"\"Оцени приведенное ниже утверждение на предмет достоверности. Если оно неверно, укажи правильную информацию:\n",
        "    Утверждение: {statement}\n",
        "    Оценка:\"\"\"\n",
        ")\n",
        "\n",
        "input_variables = {\"statement\": \"Столицей Индии является Лондон.\"}\n",
        "chain = fact_check_prompt | llm\n",
        "print(chain.invoke(input_variables).content)"
      ],
      "metadata": {
        "id": "PSRWoLA8B-fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "problem_solving_prompt = PromptTemplate(\n",
        "    input_variables=[\"problem\"],\n",
        "    template = \"\"\"Реши следующую задачу шаг за шагом:\n",
        "    Задача: {problem}\n",
        "    Решение:\n",
        "    1)\"\"\"\n",
        ")\n",
        "\n",
        "input_variables = {\"problem\": \"Банк предлагает годовую процентную ставку в размере 6%, которая ежегодно увеличивается. \\\n",
        "За последние 5 лет суммы годовых депозитов были следующими: $1000, $1500, $2000, $2500, и $3000 долларов США. \\\n",
        "Рассчитайте общую сумму на счете по истечении 5 лет с учетом ежегодного начисления процентов.\"}\n",
        "chain = problem_solving_prompt | llm\n",
        "print(chain.invoke(input_variables).content)"
      ],
      "metadata": {
        "id": "CJKRdxNUC7IW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Последовательность реплик"
      ],
      "metadata": {
        "id": "_9uLhwdMDnSz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для ведения диалога более чем из 1 реплики используется модуль ConversationChain."
      ],
      "metadata": {
        "id": "GEAfKpUpFPQI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm = llm,\n",
        "    verbose = True,\n",
        "    memory = ConversationBufferMemory()\n",
        ")\n",
        "\n",
        "print(conversation.predict(input=\"Что такое галактика?\"))\n",
        "print(conversation.predict(input=\"Сколько галактик во Вселенной?\"))\n",
        "print(conversation.predict(input=\"Как называется галактика, в которой мы живем?\"))"
      ],
      "metadata": {
        "id": "3nRHWz2IDtKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Это помогает запоминать предыдущий контекст диалога."
      ],
      "metadata": {
        "id": "YeoMPGy6FloI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\n",
        "    \"Какой город является столицей Франции?\",\n",
        "    \"Каково его население?\",\n",
        "    \"Какая самая известная достопримечательность этого города?\"\n",
        "]\n",
        "\n",
        "for prompt in prompts:\n",
        "    print(f\"Q: {prompt}\")\n",
        "    print(f\"A: {llm.invoke(prompt).content}\\n\")"
      ],
      "metadata": {
        "id": "NnlxQ0goFZwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = ConversationChain(llm=llm, memory=ConversationBufferMemory())\n",
        "for prompt in prompts:\n",
        "    print(f\"Q: {prompt}\")\n",
        "    print(f\"A: {conversation.predict(input=prompt)}\\n\")"
      ],
      "metadata": {
        "id": "NO9wFQkOFwHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Режим zero-shot"
      ],
      "metadata": {
        "id": "rwOP09mgF4al"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "При достаточно подробной формулировке инструкции задача может быть решена без каких-либо демонстрационных примеров."
      ],
      "metadata": {
        "id": "Ds-8bPL8GBdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_chain(prompt_template):\n",
        "  prompt = PromptTemplate.from_template(prompt_template)\n",
        "  return prompt | llm"
      ],
      "metadata": {
        "id": "kC3Ecv2cF_dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "direct_task_prompt = \"\"\"Классифицируй тональность следующего текста как положительную, отрицательную или нейтральную. \\\n",
        "Не объясняй свои доводы, просто приведи классификацию.\n",
        "\n",
        "Текст: {text}\n",
        "Тональность: \"\"\"\n",
        "\n",
        "direct_task_chain = create_chain(direct_task_prompt)\n",
        "\n",
        "texts = [\n",
        "    \"В новом кафе в городе такая уютная атмосфера, и кофе превосходный!\",\n",
        "    \"Книга была неплохой, но я бы не сказал, что она выделялась на фоне других, которые я читал.\",\n",
        "    \"Опыт онлайн-покупок был разочаровывающим; веб-сайт постоянно зависал.\"\n",
        "]\n",
        "\n",
        "for text in texts:\n",
        "  result = direct_task_chain.invoke({\"text\": text}).content\n",
        "  print(f\"Текст: {text}\")\n",
        "  print(f\"Тональность: {result}\")"
      ],
      "metadata": {
        "id": "atpQmeviGAoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для определенных задач важно детально задать формат ответа."
      ],
      "metadata": {
        "id": "MAd9XE4qGnve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "format_spec_prompt = \"\"\"Создай короткую новостную статью по теме \"{topic}\".\n",
        "Структурируй свой ответ в следующем формате:\n",
        "\n",
        "Заголовок: [Броский заголовок для статьи]\n",
        "\n",
        "Введение: [Краткий вводный абзац, в котором излагаются ключевые моменты]\n",
        "\n",
        "Основное содержание: [2-3 коротких абзаца с более подробной информацией]\n",
        "\n",
        "Вывод: [Заключительное предложение или призыв к действию]\"\"\"\n",
        "\n",
        "format_spec_chain = create_chain(format_spec_prompt)\n",
        "\n",
        "topic = \"Прорыв в технологии хранения возобновляемой энергии\"\n",
        "result = format_spec_chain.invoke({\"topic\": topic}).content\n",
        "print(result)"
      ],
      "metadata": {
        "id": "w2mIqUnwGkxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Кроме того, можно задать конкретные этапы выполнения задачи."
      ],
      "metadata": {
        "id": "MQxxLHEQHh3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "multi_step_prompt = \"\"\"Проанализируй следующий текст на предмет его основного аргумента, подтверждающих доказательств и потенциальных контраргументов.\n",
        "Проведи свой анализ по следующим этапам:\n",
        "\n",
        "1. Главный аргумент: Определи и сформулируй основное утверждение или тезис.\n",
        "2. Подтверждающие доказательства: Перечисли ключевые моменты или доказательства, используемые в поддержку основного аргумента.\n",
        "3. Возможные контраргументы: Предложите возможные возражения или альтернативные точки зрения на основной аргумент.\n",
        "\n",
        "Текст: {text}\n",
        "\n",
        "Анализ:\"\"\"\n",
        "\n",
        "multi_step_chain = create_chain(multi_step_prompt)\n",
        "\n",
        "text = \"\"\"В последние годы удаленная работа становится все более популярной, предлагая многочисленные преимущества как сотрудникам, так и работодателям.\n",
        "Работники пользуются большей гибкостью, сокращают время на дорогу и могут создавать более персонализированную рабочую среду.\n",
        "Работодатели выигрывают от снижения расходов на офис и доступа к более широкому кадровому резерву.\n",
        "Однако такие проблемы, как поддержание совместной работы в команде, управление производительностью и обеспечение безопасности данных по-прежнему сохраняются, что делает переход на удаленную работу не лишенным недостатков.\"\"\"\n",
        "\n",
        "result = multi_step_chain.invoke({\"text\": text}).content\n",
        "print(result)"
      ],
      "metadata": {
        "id": "EpT3DGZGHLIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Режим few-shot"
      ],
      "metadata": {
        "id": "pKIRR2DCH-m2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Добавление примеров помогает добиться лучшего решения задачи."
      ],
      "metadata": {
        "id": "R6tVfpDGIZa6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def few_shot_sentiment_classification(input_text):\n",
        "  few_shot_prompt = PromptTemplate(\n",
        "      input_variables=[\"input_text\"],\n",
        "      template=\"\"\"\n",
        "      Классифицируй тональность как положительную, отрицательную или нейтральную.\n",
        "\n",
        "      Пример:\n",
        "      Текст: Мне нравится этот продукт! Он потрясающий.\n",
        "      Тональность: Положительная\n",
        "      Текст: Этот фильм был ужасен. Я его возненавидел.\n",
        "      Тональность: Отрицательная\n",
        "      Текст: Погода сегодня неплохая.\n",
        "      Тональность: Нейтральная\n",
        "\n",
        "      Теперь классифицируй следующее предложение\n",
        "      Текст: {input_text}\n",
        "      Тональность:\n",
        "      \"\"\"\n",
        "  )\n",
        "  chain = few_shot_prompt | llm\n",
        "  result = chain.invoke(input_text).content\n",
        "  result = result.strip()\n",
        "\n",
        "  if ':' in result:\n",
        "    result = result.split(':')[1].strip()\n",
        "\n",
        "  return result\n",
        "\n",
        "test_text = \"Я не могу поверить, насколько велик и духовен кедарнатх!\"\n",
        "\n",
        "result = few_shot_sentiment_classification(test_text)\n",
        "print(f\"Текст : {test_text}\")\n",
        "print(f\"Тональность: {result}\")"
      ],
      "metadata": {
        "id": "VgB9Eak4ICrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multi_task_few_shot(input_text, task):\n",
        "    few_shot_prompt = PromptTemplate(\n",
        "        input_variables=[\"input_text\", \"task\"],\n",
        "        template=\"\"\"\n",
        "        Выполни указанное задание по данному тексту.\n",
        "\n",
        "        Примеры:\n",
        "        Текст: Мне нравится этот продукт! Он потрясающий.\n",
        "        Задание: тональность\n",
        "        Результат: положительная\n",
        "\n",
        "        Текст: Это самый худший опыт, который у меня когда-либо был.\n",
        "        Задание: тональность\n",
        "        Результат: отрицательная\n",
        "\n",
        "        Текст: Bonjour, comment allez-vous?\n",
        "        Задание: язык\n",
        "        Результат: французский\n",
        "\n",
        "        Текст: Guten Tag, wie geht es Ihnen?\n",
        "        Задание: язык\n",
        "        Результат: немецкий\n",
        "\n",
        "        Текст: কেমন আছেন? (Kemon achhen?)\n",
        "        Задание: язык\n",
        "        Результат: бенгальский\n",
        "\n",
        "        Текст: От топота копыт пыль по полю летит.\n",
        "        Задание: подсчет слов\n",
        "        Результат: 7\n",
        "\n",
        "        Теперь выполни следующее задание:\n",
        "        Текст: {input_text}\n",
        "        Задание: {task}\n",
        "        Результат:\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    chain = few_shot_prompt | llm\n",
        "    return chain.invoke({\"input_text\": input_text, \"task\": task}).content\n",
        "\n",
        "print(multi_task_few_shot(\"Я не могу поверить, насколько это здорово!\", \"тональность\"))\n",
        "print(multi_task_few_shot(\"Guten Tag, wie geht es Ihnen?\", \"язык\"))\n",
        "print(multi_task_few_shot(\"কেমন আছেন?\", \"язык\"))\n",
        "print(multi_task_few_shot(\"Бык тупогуб, тупогубенький бычок, у быка бела губа была тупа.\", \"подсчет слов\"))"
      ],
      "metadata": {
        "id": "KiVEJ0BsIhtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def in_context_learning(task_description, examples, input_text):\n",
        "    example_text = \"\".join([f\"Ввод: {e['input']}\\nВывод: {e['output']}\\n\\n\" for e in examples])\n",
        "\n",
        "    in_context_prompt = PromptTemplate(\n",
        "        input_variables=[\"task_description\", \"examples\", \"input_text\"],\n",
        "        template=\"\"\"\n",
        "        Задание: {task_description}\n",
        "\n",
        "        Примеры:\n",
        "        {examples}\n",
        "\n",
        "        Теперь выполни задание со следующими входными данными:\n",
        "        Ввод: {input_text}\n",
        "        Вывод:\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    chain = in_context_prompt | llm\n",
        "    return chain.invoke({\"task_description\": task_description, \"examples\": example_text, \"input_text\": input_text}).content\n",
        "\n",
        "task_desc = \"Преобразуй данный текст.\"\n",
        "examples = [\n",
        "    {\"input\": \"hello\", \"output\": \"ellohay\"},\n",
        "    {\"input\": \"apple\", \"output\": \"appleay\"}\n",
        "]\n",
        "test_input = \"python\"\n",
        "\n",
        "result = in_context_learning(task_desc, examples, test_input)\n",
        "print(f\"Ввод: {test_input}\")\n",
        "print(f\"Вывод: {result}\")"
      ],
      "metadata": {
        "id": "5fiYcro1Jw_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Цепочка размышлений"
      ],
      "metadata": {
        "id": "rde5Ayry9rDf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Промтинг с помощью цепочки размышлений (Chain-of-Thoughts, CoT), представленный в работе [Wei et al. (2022)](https://arxiv.org/abs/2201.11903), позволяет LLM выполнять сложные задачи, требующие промежуточных шагов рассуждения. На популярном [бенчмарке](https://habr.com/ru/articles/840530/) по школьной арифметике GSM8K данный метод улучшает результат вдвое."
      ],
      "metadata": {
        "id": "f_qhqfwl9tAQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://i.postimg.cc/J0q1n7LZ/CoT.png\" width=\"800\"></center>"
      ],
      "metadata": {
        "id": "Hzk1O6IeEHG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Стандартный промпт\n",
        "standard_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"Кратко ответь на следующий вопрос: {question}.\"\n",
        ")\n",
        "\n",
        "# Промпт с цепочкой рассуждений\n",
        "cot_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"Кратко ответь на следующий вопрос шаг за шагом: {question}\"\n",
        ")\n",
        "\n",
        "standard_chain = standard_prompt | llm\n",
        "cot_chain = cot_prompt | llm\n",
        "\n",
        "# Пример вопроса\n",
        "question = \"Прямоугольник имеет длину 10 см и ширину 5 см. Какова его площадь в квадратных сантиметрах?\"\n",
        "\n",
        "standard_response = standard_chain.invoke(question).content\n",
        "cot_response = cot_chain.invoke(question).content\n",
        "\n",
        "print(\"Стандартный ответ:\")\n",
        "print(standard_response)\n",
        "print(\"\\nОтвет CoT: \")\n",
        "print(cot_response)\n"
      ],
      "metadata": {
        "id": "xTOoEZU09tP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "advanced_cot_prompt = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"\"\"Реши следующую задачу шаг за шагом. Для каждого шага:\n",
        "1. Укажи, что ты собираешься рассчитать\n",
        "2. Напиши формулу, которую будешь использовать (если применимо).\n",
        "3. Выполни расчет\n",
        "4. Объясни результат\n",
        "\n",
        "Вопрос: {question}\n",
        "\n",
        "Решение:\"\"\"\n",
        ")\n",
        "\n",
        "advanced_cot_chain = advanced_cot_prompt | llm\n",
        "\n",
        "complex_question = \"Цилиндрический резервуар имеет радиус 5 метров и высоту 10 метров. Сколько воды в кубических метрах он может вместить, если наполнить его на 80%?\"\n",
        "\n",
        "advanced_cot_response = advanced_cot_chain.invoke(complex_question).content\n",
        "print(advanced_cot_response)"
      ],
      "metadata": {
        "id": "OlQPRGr4BxOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Самосогласованность"
      ],
      "metadata": {
        "id": "xplZs_eqDKIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Одной из продвинутых техник для создания промтов является самосогласованность (self-consistency). Эта техника была предложена в работе [Wang et al. (2022)](https://arxiv.org/abs/2203.11171) в качестве замены «жадного» декодирования, используемого в цепочках рассуждений (CoT). Идея заключается в том, чтобы сэмплировать несколько разнообразных путей рассуждений через Few-shot CoT и использовать эти генерации для выбора наиболее согласованного ответа. Это помогает улучшить производительность CoT-промтов в задачах, связанных с арифметическими и логическими рассуждениями.\n",
        "\n",
        "Вначале сгенерируем несколько разных цепочек размышлений."
      ],
      "metadata": {
        "id": "qfbclrXwDW8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_multiple_paths(problem, num_paths=3):\n",
        "  prompt_template = PromptTemplate(\n",
        "      input_variables=[\"problem\" , \"path_number\"],\n",
        "      template=\"\"\"Решите следующую задачу, каждый раз используя уникальный подход. Это способ рассуждения номер {path_number}.\n",
        "      Задача: {problem}\n",
        "      Способ рассуждения {path_number}:\"\"\"\n",
        "  )\n",
        "  paths = []\n",
        "  for i in range(num_paths):\n",
        "    chain = prompt_template | llm\n",
        "    response = chain.invoke({\"problem\": problem, \"path_number\": i+1}).content\n",
        "    paths.append(response)\n",
        "  return paths"
      ],
      "metadata": {
        "id": "D-WMuUnEEcuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "problem = \"Если поезд движется со скоростью 60 км/ч, сколько времени потребуется, чтобы преодолеть 180 км?\"\n",
        "paths = generate_multiple_paths(problem)\n",
        "\n",
        "for i, path in enumerate(paths, 1):\n",
        "  print(f\"Способ {i}: \\n{path}\\n\")"
      ],
      "metadata": {
        "id": "uzdbsOLwErfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Следующим шагом самосогласованности будет агрегация и анализ ответов, направленный на выявление самого оптимального, который и станет окончательным."
      ],
      "metadata": {
        "id": "mlzWRVJtGrSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def aggregate_results(paths):\n",
        "    prompt_template = PromptTemplate(\n",
        "        input_variables=[\"paths\"],\n",
        "        template=\"\"\"Проанализируйте приведенные ниже рассуждения и найдите наиболее логичный ответ. Если есть расхождения, объясните, почему, и укажите наиболее вероятный правильный ответ.\n",
        "        Способы рассуждения:\n",
        "        {paths}\n",
        "\n",
        "        Наиболее последовательный ответ:\"\"\"\n",
        "    )\n",
        "\n",
        "    chain = prompt_template | llm\n",
        "    response = chain.invoke({\"paths\": \"\\n\".join(paths)}).content\n",
        "    return response"
      ],
      "metadata": {
        "id": "Eae651tbGgLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aggregated_results = aggregate_results(paths)\n",
        "print(\"Итоговый результат: \\n\", aggregated_results)"
      ],
      "metadata": {
        "id": "TX0uJCG8Gzvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Можно проводить сравнение ответов по конкретным заданным критериям."
      ],
      "metadata": {
        "id": "DBBWeAz1Je2z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def self_consistency_check(problem, aggregated_result):\n",
        "    prompt_template = PromptTemplate(\n",
        "        input_variables=[\"problem\", \"result\"],\n",
        "        template=\"\"\"Оцените согласованность и надежность следующего результата для данной задачи.\n",
        "        Задача: {problem}\n",
        "        Результат: {result}\n",
        "\n",
        "        Оценка (учитывай такие факторы, как логическая последовательность, соответствие известным фактам и потенциальные предубеждения):\"\"\"\n",
        "    )\n",
        "\n",
        "    chain = prompt_template | llm\n",
        "    response = chain.invoke({\"problem\": problem, \"result\": aggregated_result}).content\n",
        "    return response"
      ],
      "metadata": {
        "id": "J5z2mG-SJXCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "consistency_evaluation = self_consistency_check(problem, aggregate_results)\n",
        "print(\"Оценка с помощью самосогласованности: \\n\", consistency_evaluation)"
      ],
      "metadata": {
        "id": "_93sPlshJj6a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Аналогично можно решать различные задачи."
      ],
      "metadata": {
        "id": "qtb5w-OKJ0Gw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def solve_problem(problem):\n",
        "    paths = generate_multiple_paths(problem)\n",
        "    aggregated_result = aggregate_results(paths)\n",
        "    consistency_evaluation = self_consistency_check(problem, aggregated_result)\n",
        "    return aggregated_result, consistency_evaluation\n",
        "\n",
        "# Примеры задач\n",
        "problems = [\n",
        "    \"Какой город является столицей Франции?\",\n",
        "    \"Объясни концепцию спроса и предложения в экономике.\",\n",
        "    \"Если поезд движется со скоростью 70 км/ч, сколько времени потребуется, чтобы преодолеть 180 км?\"\n",
        "]\n",
        "\n",
        "for problem in problems:\n",
        "    print(f\"Задача: {problem}\")\n",
        "    result, evaluation = solve_problem(problem)\n",
        "    print(\"Итоговый результат:\\n\", result)\n",
        "    print(\"\\nОценка согласованности:\\n\", evaluation)\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
      ],
      "metadata": {
        "id": "KBRSo3mHIKbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Установка роли"
      ],
      "metadata": {
        "id": "noOnqskIKT7Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Промпт может содержать указание, поведение какого специалиста должна имитировать LLM."
      ],
      "metadata": {
        "id": "a5ti0iHNKpEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tech_writer_prompt = PromptTemplate(\n",
        "    input_variables=[\"topic\"],\n",
        "    template = \"\"\"Ты технический писатель, специализирующийся на создании понятной и сжатой документации к программному продукту.\n",
        "    Твоя задача — написать краткое объяснение темы {topic} для руководства пользователя.\n",
        "    Пожалуйста, предоставь объяснение в 2-3 предложениях, которое будет легко понятно пользователям, не имеющим технических знаний\"\"\"\n",
        ")\n",
        "chain = tech_writer_prompt | llm\n",
        "response = chain.invoke({\"topic\": \"Машинное обучение\"})\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "5HUF1zz3KP0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "financial_advisor_prompt = PromptTemplate(\n",
        "    input_variables=[\"client_situation\"],\n",
        "    template=\"\"\"Вы опытный финансовый консультант с более чем 20-летним опытом работы в области личных финансов, инвестиционных стратегий и пенсионного планирования.\n",
        "    У вас есть опыт оказания помощи клиентам из разных слоев общества в достижении их финансовых целей.\n",
        "    Ваш подход характеризуется следующими принципами:\n",
        "    1. Тщательный анализ уникальной финансовой ситуации каждого клиента\n",
        "    2. Четкое изложение сложных финансовых концепций без использования жаргона\n",
        "    3. Соблюдение этических норм во всех рекомендациях\n",
        "    4. Акцент на долгосрочном финансовом благополучии и стабильности\n",
        "\n",
        "    Учитывая следующую ситуацию с клиентом, предоставьте краткую (3-4 предложения) финансовую консультацию:\n",
        "    {client_situation}\n",
        "\n",
        "    Ваш ответ должен отражать ваш опыт и соответствовать вашему характерному подходу.\"\"\"\n",
        ")\n",
        "\n",
        "chain = financial_advisor_prompt | llm\n",
        "response = chain.invoke({\"client_situation\": \"35-летний специалист, зарабатывающий 800 000 рублей в год, имеющий сбережения в размере 300 000 рублей, без долгов и пенсионного плана.\"})\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "xmSfkubBKvGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Объяснение одних и тех же понятий может отличаться для разных ролей."
      ],
      "metadata": {
        "id": "uNMC2rGhLOvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "roles = [\n",
        "    (\"Ученый\", \"Вы ученый-исследователь, специализирующийся на изменении климата. Объясните следующую концепцию в научных терминах:\"),\n",
        "    (\"Учитель\", \"Вы учитель естествознания в средней школе. Объясните следующую концепцию простыми словами, подходящими для 12-летних учеников:\"),\n",
        "    (\"Журналист\", \"Вы журналист, пишущий для научно-популярного журнала. Объясните следующую концепцию в увлекательной и информативной форме для широкой взрослой аудитории:\")\n",
        "]\n",
        "\n",
        "topic = \"парниковый эффект\"\n",
        "\n",
        "for role, description in roles:\n",
        "    role_prompt = PromptTemplate(\n",
        "        input_variables=[\"topic\"],\n",
        "        template=f\"{description} {{topic}}\"\n",
        "    )\n",
        "    chain = role_prompt | llm\n",
        "    response = chain.invoke({\"topic\": topic})\n",
        "    print(f\"\\nОбъясняет {role}:\\n\")\n",
        "    print(response.content)\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "WU4o_1mlLOLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "storyteller_prompt = PromptTemplate(\n",
        "    input_variables=[\"style\", \"scenario\"],\n",
        "    template=\"\"\"Вы прекрасный рассказчик, известный своей способностью адаптироваться к различным стилям повествования.\n",
        "    Ваша текущая задача - писать в стиле {style}.\n",
        "    Ключевые характеристики этого стиля включают:\n",
        "    1. {style_char1}\n",
        "    2. {style_char2}\n",
        "    3. {style_char3}\n",
        "\n",
        "    Напишите короткий абзац (3-4 предложения) в таком стиле о следующем сценарии:\n",
        "    {scenario}\n",
        "\n",
        "    Убедитесь, что ваш текст четко соответствует указанному стилю.\"\"\"\n",
        ")\n",
        "\n",
        "styles = [\n",
        "    {\n",
        "        \"name\": \"Готический хоррор\",\n",
        "        \"char1\": \"Атмосферные и зловещие описания\",\n",
        "        \"char2\": \"Темы разложения, смерти и сверхъестественного\",\n",
        "        \"char3\": \"Обостренные эмоции и чувство страха\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Минималистский реализм\",\n",
        "        \"char1\": \"Скупой, лаконичный язык\",\n",
        "        \"char2\": \"Фокус на повседневных, заурядных событиях\",\n",
        "        \"char3\": \"Тонкие намеки, а не явные заявления\"\n",
        "    }\n",
        "]\n",
        "\n",
        "scenario = \"Человек входит в пустой дом в сумерках\"\n",
        "\n",
        "for style in styles:\n",
        "    chain = storyteller_prompt | llm\n",
        "    response = chain.invoke({\n",
        "        \"style\": style[\"name\"],\n",
        "        \"style_char1\": style[\"char1\"],\n",
        "        \"style_char2\": style[\"char2\"],\n",
        "        \"style_char3\": style[\"char3\"],\n",
        "        \"scenario\": scenario\n",
        "    })\n",
        "    print(f\"\\n{style['name']}:\\n\")\n",
        "    print(response.content)\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "YzeAfAJwLuyf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}