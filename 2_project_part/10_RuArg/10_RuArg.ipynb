{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axjKNdheefPW"
      },
      "source": [
        "Анализ аргументации\n",
        "\n",
        "[Репозиторий на GitHub](https://github.com/dialogue-evaluation/RuArg)\n",
        "\n",
        "[Страница на CodaLab](https://codalab.lisn.upsaclay.fr/competitions/786)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPd2NwUOefPY"
      },
      "source": [
        "## Данные"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9Cf5m69efPZ"
      },
      "source": [
        "### Загрузка"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSrO2LkaefPZ"
      },
      "outputs": [],
      "source": [
        "!wget -q https://raw.githubusercontent.com/dialogue-evaluation/RuArg/main/data/train.tsv\n",
        "!wget -q https://raw.githubusercontent.com/dialogue-evaluation/RuArg/main/data/val_empty.tsv\n",
        "!wget -q https://raw.githubusercontent.com/dialogue-evaluation/RuArg/main/data/test-no_labels.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fy9zPiOgefPa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "train = pd.read_csv('train.tsv', sep='\\t')\n",
        "print(train.shape)\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wB7ggCMvefPb"
      },
      "outputs": [],
      "source": [
        "validation = pd.read_csv('val_empty.tsv', sep='\\t')\n",
        "print(validation.shape)\n",
        "validation.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbTTiZPXefPb"
      },
      "outputs": [],
      "source": [
        "test = pd.read_csv('test-no_labels.tsv', sep='\\t')\n",
        "print(test.shape)\n",
        "test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWszmchPefPb"
      },
      "source": [
        "Разметка содержится только в обучающей выборке."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYi63xCaefPc"
      },
      "source": [
        "### Анализ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RghIemw7efPc"
      },
      "source": [
        "Проанализируем данные обучающей выборки.\n",
        "\n",
        "Определим минимальную, максимальную и среднюю длину текста. Отобразим распределение на графике."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIZH3WfJefPc"
      },
      "outputs": [],
      "source": [
        "lens = [len(x.split()) for x in train['text']]\n",
        "\n",
        "max_l, min_l, mean_l = max(lens), min(lens), sum(lens)/len(lens)\n",
        "\n",
        "print(f'Минимальная длина текста: {min_l}')\n",
        "print(f'Максимальная длина текста: {max_l}')\n",
        "print(f'Средняя длина текста: {mean_l:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VmeJ08kzefPc"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "len_counts = Counter(lens)\n",
        "plt.figure(figsize = (6,3))\n",
        "plt.bar(len_counts.keys(), len_counts.values())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tu1a4JBKefPc"
      },
      "source": [
        "Выведем самый длинный текст."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92jc5Y8befPc"
      },
      "outputs": [],
      "source": [
        "for i in range(len(train)):\n",
        "    if len(train['text'][i].split()) == max_l:\n",
        "        print(train['text'][i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiTgWOlMefPd"
      },
      "source": [
        "Проанализируем распределение по темам:\n",
        "- маски\n",
        "- карантин\n",
        "- вакцины"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a06yDLK2efPd"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.xlabel('Тема')\n",
        "plt.ylabel('Количество примеров')\n",
        "plt.title('Распределение типов сущностей')\n",
        "plt.bar('Маски', train[train['masks_stance'] != -1].shape)\n",
        "plt.bar('Карантин', train[train['quarantine_stance'] != -1].shape)\n",
        "plt.bar('Вакцины', train[train['vaccines_stance'] != -1].shape)\n",
        "plt.xticks(ticks=['Маски', 'Карантин', 'Вакцины'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRXRqmq5efPd"
      },
      "source": [
        "Посмотрим на распределение текстов по классам:\n",
        "- «за» (2),\n",
        "- «против» (0),\n",
        "- другое/ нет аргумента (1),\n",
        "- неактуально (-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rVLyOz8efPd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "label2 = [train[train['masks_stance'] == 2].shape[0], train[train['masks_argument'] == 2].shape[0]]\n",
        "label0 = [train[train['masks_stance'] == 0].shape[0], train[train['masks_argument'] == 0].shape[0]]\n",
        "label1 = [train[train['masks_stance'] == 1].shape[0], train[train['masks_argument'] == 1].shape[0]]\n",
        "\n",
        "r = np.arange(2)\n",
        "width = 0.25\n",
        "plt.bar(r, label2, color = 'lightgreen',\n",
        "        width = width, label='\"за\"')\n",
        "plt.bar(r + width, label0, color = 'lightpink',\n",
        "        width = width, label='\"против\"')\n",
        "plt.bar(r + width*2, label1, color = 'lightblue',\n",
        "        width = width, label='другое/\\nнет аргумента')\n",
        "\n",
        "plt.ylabel(\"Количество примеров\")\n",
        "plt.title('Распределение классов по теме \"маски\"')\n",
        "plt.xticks(r + width,['для позиций','для доводов'])\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YTGlagbefPd"
      },
      "outputs": [],
      "source": [
        "label2 = [train[train['quarantine_stance'] == 2].shape[0], train[train['quarantine_argument'] == 2].shape[0]]\n",
        "label0 = [train[train['quarantine_stance'] == 0].shape[0], train[train['quarantine_argument'] == 0].shape[0]]\n",
        "label1 = [train[train['quarantine_stance'] == 1].shape[0], train[train['quarantine_argument'] == 1].shape[0]]\n",
        "\n",
        "r = np.arange(2)\n",
        "width = 0.25\n",
        "plt.bar(r, label2, color = 'lightgreen',\n",
        "        width = width, label='\"за\"')\n",
        "plt.bar(r + width, label0, color = 'lightpink',\n",
        "        width = width, label='\"против\"')\n",
        "plt.bar(r + width*2, label1, color = 'lightblue',\n",
        "        width = width, label='другое/\\nнет аргумента')\n",
        "\n",
        "plt.ylabel(\"Количество примеров\")\n",
        "plt.title('Распределение классов по теме \"карантин\"')\n",
        "plt.xticks(r + width,['для позиций','для доводов'])\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_01ScDHuefPd"
      },
      "outputs": [],
      "source": [
        "label2 = [train[train['vaccines_stance'] == 2].shape[0], train[train['vaccines_argument'] == 2].shape[0]]\n",
        "label0 = [train[train['vaccines_stance'] == 0].shape[0], train[train['vaccines_argument'] == 0].shape[0]]\n",
        "label1 = [train[train['vaccines_stance'] == 1].shape[0], train[train['vaccines_argument'] == 1].shape[0]]\n",
        "\n",
        "r = np.arange(2)\n",
        "width = 0.25\n",
        "plt.bar(r, label2, color = 'lightgreen',\n",
        "        width = width, label='\"за\"')\n",
        "plt.bar(r + width, label0, color = 'lightpink',\n",
        "        width = width, label='\"против\"')\n",
        "plt.bar(r + width*2, label1, color = 'lightblue',\n",
        "        width = width, label='другое/\\nнет аргумента')\n",
        "\n",
        "plt.ylabel(\"Количество примеров\")\n",
        "plt.title('Распределение классов по теме \"вакцины\"')\n",
        "plt.xticks(r + width,['для позиций','для доводов'])\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3CSq2jsefPd"
      },
      "source": [
        "## Модель"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vh58MyYoefPe"
      },
      "source": [
        "В этом разделе осуществим тонкую настройку модели [Sentence RuBERT](https://huggingface.co/DeepPavlov/rubert-base-cased-sentence)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPan-G0mefPe"
      },
      "source": [
        "### Предобработка"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhMZ-X2iefPe"
      },
      "source": [
        "Для обучения нейронной сети необходимо, чтобы метки классов были положительными числами. Создадим столбец с преобразованными метками."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PM8kkjbrefPe"
      },
      "outputs": [],
      "source": [
        "classes = [\"quarantine\", \"vaccines\", \"masks\"]\n",
        "label_dict = {-1: 0, 0: 1, 1: 2, 2: 3}\n",
        "for c in classes:\n",
        "  train[f'raw_{c}_stance'] = train[f'{c}_stance']\n",
        "  train[f'{c}_stance'] = train[f'raw_{c}_stance'].map(label_dict)\n",
        "  train[f'raw_{c}_argument'] = train[f'{c}_argument']\n",
        "  train[f'{c}_argument'] = train[f'raw_{c}_argument'].map(label_dict)\n",
        "train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ousZs4wDefPe"
      },
      "source": [
        "Преобразуем данные в формат датасетов Hugging Face.\n",
        "\n",
        "Поскольку разметка присутствует только в обучающей выборке, из нее необходимо выделить часть для валидации."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jr9kcb9efPe"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_Cgje5QefPe"
      },
      "outputs": [],
      "source": [
        "!pip install -q datasets transformers evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9mMX4ULefPe"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "dataset = Dataset.from_pandas(train, preserve_index=False).train_test_split(test_size=0.2)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMaUqrEvefPe"
      },
      "source": [
        "Объединим обучающую и валидационную выборку с тестовой, в которой отсутствует разметка."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmDJWj7TefPe"
      },
      "outputs": [],
      "source": [
        "from datasets import DatasetDict\n",
        "dataset_dict = DatasetDict({\"train\": dataset[\"train\"],\n",
        "                            \"validation\": dataset[\"test\"],\n",
        "                            \"test\": Dataset.from_pandas(test[[\"text_id\", \"text\"]])})\n",
        "dataset_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrIvBht-efPe"
      },
      "source": [
        "Загрузим токенизатор модели."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wk93KP4eefPf"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = 'DeepPavlov/rubert-base-cased-sentence'\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-j4j5hAefPf"
      },
      "source": [
        "Применим токенизацию ко всем подвыборкам датасета. Удалим лишние столбцы."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAG7EpivefPf"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTd5Zwu9efPf"
      },
      "outputs": [],
      "source": [
        "tokenized_dataset = dataset_dict.map(tokenize_function, batched=True)\n",
        "tokenized_dataset = tokenized_dataset.remove_columns(['text_id', 'text'])\n",
        "tokenized_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ucv5XECefPf"
      },
      "source": [
        "Создадим объекты класса DataLoader для деления на батчи и паддинга."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMbDCHG1efPf"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    tokenized_dataset[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    tokenized_dataset[\"validation\"], batch_size=8, collate_fn=data_collator\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    tokenized_dataset[\"test\"], batch_size=8, collate_fn=data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PugvgklLefPf"
      },
      "source": [
        "### Определение функций"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk5ifQzgefPj"
      },
      "source": [
        "Будем использовать графический процессор для вычислений."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOXJUKvEefPj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Prjjw3GqefPk"
      },
      "source": [
        "Будем осуществлять дообучение для классификации отдельной модели по каждой теме: «маски», «карантин», «вакцины». Для этого возьмем предобученный BERT с незамороженными весами и добавим два линейных слоя: для определения позиции и для классификации доводов.\n",
        "\n",
        "Сложность этого решения состоит в том, что класс [BertForSequenceClassification](https://huggingface.co/docs/transformers/v4.48.2/en/model_doc/bert#transformers.BertForSequenceClassification) позволяет добавить только один линейный слой. Поэтому мы создадим свой класс CustomBertForSequenceClassification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCcyRpdSefPk"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "class CustomBertForSequenceClassification(nn.Module):\n",
        "\n",
        "  def __init__(self, n_labels):\n",
        "    super().__init__()\n",
        "    self.bert = AutoModel.from_pretrained(checkpoint)\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    self.stance_out = nn.Linear(self.bert.config.hidden_size, n_labels)\n",
        "    self.argument_out = nn.Linear(self.bert.config.hidden_size, n_labels)\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    _, pooled_output = self.bert(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      return_dict=False)\n",
        "    output = self.drop(pooled_output)\n",
        "    stance_logits = self.stance_out(output)\n",
        "    argument_logits = self.argument_out(output)\n",
        "\n",
        "    return {\"stance\": stance_logits, \"argument\": argument_logits}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wlEMM93efPk"
      },
      "source": [
        "Реализуем функцию для одной эпохи обучения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5X6QElDyefPk"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def train_epoch(current_class, model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
        "  model = model.train() # переводим модель в состояние обучения\n",
        "\n",
        "  losses = [] # значения функции потерь\n",
        "  # значения accuracy\n",
        "  stance_correct_predictions = 0\n",
        "  argument_correct_predictions = 0\n",
        "\n",
        "  for d in data_loader: # итерация по батчам\n",
        "    input_ids = d[\"input_ids\"].to(device) # индексы токенов\n",
        "    attention_mask = d[\"attention_mask\"].to(device) # маски внимания\n",
        "    # метки классов\n",
        "    stance_targets = d[f\"{current_class}_stance\"].to(device)\n",
        "    argument_targets = d[f\"{current_class}_argument\"].to(device)\n",
        "\n",
        "    # применяем модель\n",
        "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    # позиция максимального значения\n",
        "    stance_preds = torch.argmax(outputs[\"stance\"], dim=1)\n",
        "    argument_preds = torch.argmax(outputs[\"argument\"], dim=1)\n",
        "    # подсчет функции потерь\n",
        "    stance_loss = loss_fn(outputs[\"stance\"], stance_targets)\n",
        "    argument_loss = loss_fn(outputs[\"argument\"], argument_targets)\n",
        "    loss = stance_loss + argument_loss\n",
        "\n",
        "    # количество совпадений\n",
        "    stance_correct_predictions += torch.sum(stance_preds == stance_targets)\n",
        "    argument_correct_predictions += torch.sum(argument_preds == argument_targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward() # подсчет градиента\n",
        "    optimizer.step() # обновление весов\n",
        "    scheduler.step() # изменение скорости обучения\n",
        "    optimizer.zero_grad() # обнуление градиентов\n",
        "\n",
        "  return stance_correct_predictions / n_examples, argument_correct_predictions / n_examples, np.mean(losses) # accuracy, среднее значение ошибки"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xjb2QBqRefPk"
      },
      "source": [
        "Также реализуем функцию для валидации."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tSfolU4iefPk"
      },
      "outputs": [],
      "source": [
        "def eval_model(current_class, model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval() # переводим модель в состояние валидации\n",
        "\n",
        "  losses = [] # значения функцим потерь\n",
        "  # значения accuracy\n",
        "  stance_correct_predictions = 0\n",
        "  argument_correct_predictions = 0\n",
        "\n",
        "  with torch.no_grad(): # грандиент не считается\n",
        "    for d in data_loader: # итерация по батчам\n",
        "      input_ids = d[\"input_ids\"].to(device) # индексы токенов\n",
        "      attention_mask = d[\"attention_mask\"].to(device) # маски внимания\n",
        "      # метки классов\n",
        "      stance_targets = d[f\"{current_class}_stance\"].to(device)\n",
        "      argument_targets = d[f\"{current_class}_argument\"].to(device)\n",
        "\n",
        "      # применяем модель\n",
        "      outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "      # позиция максимального значения\n",
        "      stance_preds = torch.argmax(outputs[\"stance\"], dim=1)\n",
        "      argument_preds = torch.argmax(outputs[\"argument\"], dim=1)\n",
        "      # подсчет функции потерь\n",
        "      stance_loss = loss_fn(outputs[\"stance\"], stance_targets)\n",
        "      argument_loss = loss_fn(outputs[\"argument\"], argument_targets)\n",
        "      loss = stance_loss + argument_loss\n",
        "\n",
        "      # количество совпадений\n",
        "      stance_correct_predictions += torch.sum(stance_preds == stance_targets)\n",
        "      argument_correct_predictions += torch.sum(argument_preds == argument_targets)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "  return stance_correct_predictions / n_examples, argument_correct_predictions / n_examples, np.mean(losses) # accuracy, среднее значение ошибки"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJhWoRR8efPk"
      },
      "source": [
        "### Обучение по классам"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1lqxLMpefPk"
      },
      "source": [
        "Напишем функцию для обучения модели, чтобы потом применить ее для каждой тематики.\n",
        "\n",
        "Вначале загрузим предобученную модель и добавим два линейных слоя.\n",
        "\n",
        "Затем установим количество эпох и скорость обучения. Будем использовать планировщик (`scheduler`), он регулирует скорость обучения: первые несколько шагов (`num_warmup_steps`) она может увеличиваться, а потом уменьшается. Также определим функцию потерь.\n",
        "\n",
        "Наконец, реализуем процедуру обучения и валидации.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGwynvnHefPl"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "from transformers import AutoModel\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "def fine_tuning(current_class):\n",
        "\n",
        "  print(f\"Trainig {current_class} model:\\n\")\n",
        "\n",
        "  # Загрузка предобученной модели\n",
        "  bert_model = AutoModel.from_pretrained(checkpoint)\n",
        "  # Добавление линейных слоев\n",
        "  model = CustomBertForSequenceClassification(n_labels = 4).to(device)\n",
        "\n",
        "  EPOCHS = 2\n",
        "  # Обучение всех слоев\n",
        "  optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "  total_steps = len(train_dataloader) * EPOCHS\n",
        "  scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "  loss_fn = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "  for epoch in range(EPOCHS): # итерация по эпохам\n",
        "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "    print('-' * 10)\n",
        "\n",
        "    # обучение\n",
        "    train_stance_acc, train_argument_acc, train_loss = train_epoch(current_class, model, train_dataloader, loss_fn, optimizer, device, scheduler, len(dataset[\"train\"]))\n",
        "\n",
        "    print(f'Train loss {train_loss} stance accuracy {train_stance_acc} argument accuracy {train_argument_acc}')\n",
        "\n",
        "    # валидация\n",
        "    val_stance_acc, val_argument_acc, val_loss = eval_model(current_class, model, val_dataloader, loss_fn, device, len(dataset[\"test\"]))\n",
        "\n",
        "    print(f'Val loss {val_loss} stance accuracy {val_stance_acc} argument accuracy {val_argument_acc}')\n",
        "    print()\n",
        "\n",
        "  return bert_model, model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Eho9LTwefPl"
      },
      "source": [
        "Обучим модель для каждой из тематик: карантин, вакцины и маски."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBxPlDxpefPl"
      },
      "outputs": [],
      "source": [
        "classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvwtj4keefPl"
      },
      "outputs": [],
      "source": [
        "bert_model, quarantine_model = fine_tuning(classes[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYOc8RIgefPl"
      },
      "outputs": [],
      "source": [
        "bert_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTa6TKabefPl"
      },
      "outputs": [],
      "source": [
        "quarantine_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jC9g7rOTefPl"
      },
      "outputs": [],
      "source": [
        "bert_model, vaccines_model = fine_tuning(classes[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpaJZTWxefPl"
      },
      "outputs": [],
      "source": [
        "bert_model, masks_model = fine_tuning(classes[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUiQNpeEefPm"
      },
      "source": [
        "### Получение предсказаний"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvcfdepjefPm"
      },
      "source": [
        "Напишем функции для получения предсказаний обученной модели и подсчета макро F1-меры."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_mx52FdefPm"
      },
      "outputs": [],
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "\n",
        "  # предсказанные метки\n",
        "  stance_predictions = []\n",
        "  argument_predictions = []\n",
        "\n",
        "  with torch.no_grad(): # грандиент не считается\n",
        "    for d in data_loader: # итерация по батчам\n",
        "      input_ids = d[\"input_ids\"].to(device) # индексы токенов\n",
        "      attention_mask = d[\"attention_mask\"].to(device) # маски внимания\n",
        "\n",
        "      # применяем модель\n",
        "      outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "      # позиция максимального значения\n",
        "      stance_preds = torch.argmax(outputs[\"stance\"], dim=1)\n",
        "      argument_preds = torch.argmax(outputs[\"argument\"], dim=1)\n",
        "\n",
        "      stance_predictions.extend(stance_preds)\n",
        "      argument_predictions.extend(argument_preds)\n",
        "\n",
        "  stance_predictions = torch.stack(stance_predictions).cpu()\n",
        "  argument_predictions = torch.stack(argument_predictions).cpu()\n",
        "\n",
        "  # преобразуем обратно к исходной разметке -1, 0, 1 , 2\n",
        "  reverse_label_dict = {v:k for k, v in label_dict.items()}\n",
        "  stance_predictions = [reverse_label_dict[x.item()] for x in stance_predictions]\n",
        "  argument_predictions = [reverse_label_dict[x.item()] for x in argument_predictions]\n",
        "\n",
        "  return stance_predictions, argument_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJvLQb3GefPm"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "def compute_metrics(preds, labels):\n",
        "    metric = evaluate.load(\"f1\")\n",
        "    return metric.compute(predictions=preds, references=labels, average=\"macro\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-m8zJU8efPm"
      },
      "source": [
        "Посчитаем метрику для каждой из моделей. При этом тексты с меткой -1 (неактуально) не будут учитываться при подсчете."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoakAsKQefPm"
      },
      "outputs": [],
      "source": [
        "def validation_score(current_class, model):\n",
        "  val_stance_predictions, val_argument_predictions = get_predictions(model, val_dataloader)\n",
        "  tokenized_dataset[\"validation\"] = tokenized_dataset[\"validation\"].add_column(f\"{current_class}_stance_predictions\", val_stance_predictions)\n",
        "  tokenized_dataset[\"validation\"] = tokenized_dataset[\"validation\"].add_column(f\"{current_class}_argument_predictions\", val_argument_predictions)\n",
        "  filtered_validation = tokenized_dataset[\"validation\"].filter(lambda example: example[f\"raw_{current_class}_stance\"]!=-1)\n",
        "  stance_f1 = compute_metrics(filtered_validation[f\"{current_class}_stance_predictions\"], filtered_validation[f\"raw_{current_class}_stance\"])\n",
        "  argument_f1 = compute_metrics(filtered_validation[f\"{current_class}_argument_predictions\"], filtered_validation[f\"raw_{current_class}_argument\"])\n",
        "  return stance_f1['f1'], argument_f1['f1']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uqt22P3WefPm"
      },
      "outputs": [],
      "source": [
        "classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BuH6X_RefPm"
      },
      "outputs": [],
      "source": [
        "quarantine_stance_f1, quarantine_argument_f1 = validation_score(classes[0], quarantine_model)\n",
        "print(f\"Quarantine Stance F1 = {quarantine_stance_f1}\")\n",
        "print(f\"Quarantine Argument F1 = {quarantine_argument_f1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbiz3UinefPm"
      },
      "outputs": [],
      "source": [
        "vaccines_stance_f1, vaccines_argument_f1 = validation_score(classes[1], vaccines_model)\n",
        "print(f\"Vaccines Stance F1 = {vaccines_stance_f1}\")\n",
        "print(f\"Vaccines Argument F1 = {vaccines_argument_f1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzjvWRiDefPn"
      },
      "outputs": [],
      "source": [
        "masks_stance_f1, masks_argument_f1 = validation_score(classes[2], masks_model)\n",
        "print(f\"Masks Stance F1 = {masks_stance_f1}\")\n",
        "print(f\"Masks Argument F1 = {masks_argument_f1}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtDrXIbNefPn"
      },
      "source": [
        "Чтобы посчитать итоговую метрику, усредним по трем тематикам."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTB9kqusefPn"
      },
      "outputs": [],
      "source": [
        "final_stance_metrics = (quarantine_stance_f1 + vaccines_stance_f1 + masks_stance_f1) / 3\n",
        "final_argument_metrics = (quarantine_argument_f1 + vaccines_argument_f1 + masks_argument_f1) / 3\n",
        "print(f\"Final Stance F1 = {final_stance_metrics}\")\n",
        "print(f\"Final Argument F1 = {final_argument_metrics}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HPecYxuefPn"
      },
      "source": [
        "Запишем предсказания тестовой выборки в виде датафрейма. Преобразуем их в формат .tsv, а затем заархивируем."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SsfKRudiefPn"
      },
      "outputs": [],
      "source": [
        "quarantine_test_stance_predictions, quarantine_test_argument_predictions = get_predictions(quarantine_model, test_dataloader)\n",
        "vaccines_test_stance_predictions, vaccines_test_argument_predictions = get_predictions(vaccines_model, test_dataloader)\n",
        "masks_test_stance_predictions, masks_test_argument_predictions = get_predictions(masks_model, test_dataloader)\n",
        "test_predictions = pd.DataFrame.from_dict({\"masks_stance\": masks_test_stance_predictions, \"masks_argument\": masks_test_argument_predictions,\n",
        "                                           \"quarantine_stance\": quarantine_test_stance_predictions, \"quarantine_argument\": quarantine_test_argument_predictions,\n",
        "                                           \"vaccines_stance\": vaccines_test_stance_predictions, \"vaccines_argument\": vaccines_test_argument_predictions})\n",
        "test_predictions.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbStrR7MefPn"
      },
      "outputs": [],
      "source": [
        "test_predictions.to_csv(\"test_predictions.tsv\", sep='\\t', index=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvspYpJ_efPn"
      },
      "outputs": [],
      "source": [
        "!zip test_predictions.zip test_predictions.tsv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2irczpFmefPn"
      },
      "source": [
        "Файл test_predictions.zip может быть загружен на платформу CodaLab для подсчета метрики на тестовой подвыборке."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}