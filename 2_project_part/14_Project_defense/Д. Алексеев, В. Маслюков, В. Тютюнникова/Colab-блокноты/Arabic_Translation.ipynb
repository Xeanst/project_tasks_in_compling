{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnOp24_TxI9P"
      },
      "outputs": [],
      "source": [
        "%pip install sacremoses\n",
        "%pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ud3XjmvjzpUU"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_id = \"ModelSpace/GemmaX2-28-2B-v0.1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side='left')\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cuda\", load_in_8bit=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pN9r7Iy_eKk8"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "with open('arabic_stance_master.json', 'r') as fin:\n",
        "  data = json.load(fin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7KDJGIZfbqS"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "prog = re.compile(r'MENTION\\s*')\n",
        "for k, v in data.items():\n",
        "  ar = v[0]\n",
        "  ar_clean = prog.sub('', ar)\n",
        "  data[k] = [v[0], ar_clean, v[-1]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SfE3bYcfcgc"
      },
      "outputs": [],
      "source": [
        "data['0'][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrjwS9N5ifxY"
      },
      "outputs": [],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBk7vNPj3BLj"
      },
      "outputs": [],
      "source": [
        "outdata = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvzmCy_u0lIZ"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "# for_translation = ['بس التطعيمات الي اولادنا اخذوها دي اخذت وقتها في تصنيعها وتجارب الي ان طلعت لناس لكن تطعيم كورونا ماخلص من التجارب لسه وبيطبق علينا بس حرام الاطفال كمان تدخل في نظام التجارب دي واحنا لسه مو عارفين مضاعفاته وابعاده ايش هي بعدين',\n",
        "# \"#خذ_الخطوه_بلغ_عن_اضرار_اللقاح #لا_للتطعيم_الاجباري186 #لا_للتطعيم_الاجباري #متضرري_اللقاح #ولي_العهد كورونا #الجرعة_التنشيطية #الجرعة_الثالثة تكفى ياملكنا وقف اللقاحات الإجبارية منشنو ان شاء الله يشوف تغريداتنا\"\n",
        "# ]\n",
        "\n",
        "template = \"Translate this from Arabic to English:\\nArabic: {}\\nEnglish:\"\n",
        "\n",
        "for i in range(0, 1200, 100):\n",
        "  input_sents = [template.format(x[1]) for x in list(data.values())[i:i+100]]\n",
        "  inputs = tokenizer(input_sents, return_tensors=\"pt\", padding=True)\n",
        "  inputs.to('cuda')\n",
        "  outputs = model.generate(**inputs, max_new_tokens=150)\n",
        "  out = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "  outdata.extend(out)\n",
        "  del inputs\n",
        "  gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCXmVrrt5pym"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "with open('ar_to_en.csv', 'w') as fout:\n",
        "  csv.writer(fout).writerows([[x.split('\\n')[-1].split(':', maxsplit=1)[1]] for x in outdata])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntWDFWpS6hU5"
      },
      "outputs": [],
      "source": [
        "new_json = {k: v[:-1] + [outdata[i].split('\\n')[-1].split(':', maxsplit=1)[1]] + [v[-1]] for i, (k, v) in enumerate(data.items())}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzW__a9D6zVN"
      },
      "outputs": [],
      "source": [
        "with open('arabic_stance_trans_master.json', 'w') as fout:\n",
        "  json.dump(new_json, fout, ensure_ascii=False, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Перевод на русский"
      ],
      "metadata": {
        "id": "kC2W5WZHUkkA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6V3Vn1plB7J9"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "with open('arabic_stance_trans_master.json', 'r') as fin:\n",
        "  data = json.load(fin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1y253xtCeLQ"
      },
      "outputs": [],
      "source": [
        "outdata = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5mzDrYVCgpC"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "template = \"Translate this from English to Russian:\\nEnglish: {}\\nRussian:\"\n",
        "\n",
        "for i in tqdm(range(0, 1200, 100)):\n",
        "  input_sents = [template.format(x[-2]) for x in list(data.values())[i:i+100]]\n",
        "  inputs = tokenizer(input_sents, return_tensors=\"pt\", padding=True)\n",
        "  inputs.to('cuda')\n",
        "  outputs = model.generate(**inputs, max_new_tokens=150)\n",
        "  out = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "  outdata.extend(out)\n",
        "  del outputs, out\n",
        "  del inputs\n",
        "  gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outdata[1]"
      ],
      "metadata": {
        "id": "48BZ8zLfTF1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_translation(s):\n",
        "  return s.split('\\n')[-1].split(':', maxsplit=1)[1]"
      ],
      "metadata": {
        "id": "q3jsJdRETNc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_json = {k: v[:-1] + [get_translation(outdata[i])] + [v[-1]] for i, (k, v) in enumerate(data.items())}"
      ],
      "metadata": {
        "id": "rgvlCh_NTWDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('arabic_stance_trans_grandmaster.json', 'w') as fout:\n",
        "  json.dump(new_json, fout, ensure_ascii=False, indent=4)"
      ],
      "metadata": {
        "id": "vkKGr0CSTf_U"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}