{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Загружаем данные"
      ],
      "metadata": {
        "id": "iGwNwb2qxIuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "collapsed": true,
        "id": "kRZarvhmyAk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from collections import defaultdict\n",
        "\n",
        "with open('arabic_stance_trans_grandmaster.json') as f:\n",
        "    d = json.load(f)\n",
        "\n",
        "arabic_stance_reformatted = []\n",
        "for key in d.keys():\n",
        "  x = {\"arb\":d[key][0], \"arb_clean\":d[key][1], \"eng_tr\":d[key][2], \"rus\":d[key][3], \"label\":d[key][4]+1}\n",
        "  arabic_stance_reformatted.append(x)"
      ],
      "metadata": {
        "id": "Id4HOGNP0Eoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "def dict_of_lists(data, corpus=None):\n",
        "\n",
        "  answer = []\n",
        "  if corpus == 'ruarg':\n",
        "    for row in data:\n",
        "      x = {\"eng_tr\": row[2], \"rus\": row[0], \"label\": int(row[1])+1}\n",
        "      answer.append(x)\n",
        "  else:\n",
        "    for row in data:\n",
        "      x = {\"eng_tr\": row[0], \"rus\": row[2], \"label\": int(row[1])+1}\n",
        "      answer.append(x)\n",
        "  return(answer)\n",
        "\n",
        "\n",
        "with open('RuArg Translation.csv', 'r') as csvfile:\n",
        "    ra = list(csv.reader(csvfile))\n",
        "with open('TweetStance Translation.csv', 'r') as csvfile:\n",
        "    ts = list(csv.reader(csvfile))\n",
        "\n",
        "ra_formatted = dict_of_lists(ra, corpus='ruarg')\n",
        "ts_formatted = dict_of_lists(ts, corpus='tweetstance')"
      ],
      "metadata": {
        "id": "SjdXRpAYSgOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "arabic_train, arabic_val, arabic_test = torch.utils.data.random_split(arabic_stance_reformatted, [0.8, 0.1, 0.1], generator=torch.Generator().manual_seed(121))\n",
        "ruarg_train, ruarg_val, ruarg_test = torch.utils.data.random_split(ra_formatted, [0.8, 0.1, 0.1], generator=torch.Generator().manual_seed(121))\n",
        "tweetstance_train, tweetstance_val, tweetstance_test = torch.utils.data.random_split(ts_formatted, [0.8, 0.1, 0.1], generator=torch.Generator().manual_seed(121))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "kkFvUXeW4fYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('arabic_stance_train.json', 'w') as f:\n",
        "    json.dump(list(arabic_train), f, indent=4, ensure_ascii=False)\n",
        "with open('arabic_stance_val.json', 'w') as f:\n",
        "    json.dump(list(arabic_val), f, indent=4, ensure_ascii=False)\n",
        "with open('arabic_stance_test.json', 'w') as f:\n",
        "    json.dump(list(arabic_test), f, indent=4, ensure_ascii=False)\n",
        "\n",
        "with open('ruarg_train.json', 'w') as f:\n",
        "    json.dump(list(ruarg_train), f, indent=4, ensure_ascii=False)\n",
        "with open('ruarg_val.json', 'w') as f:\n",
        "    json.dump(list(ruarg_val), f, indent=4, ensure_ascii=False)\n",
        "with open('ruarg_test.json', 'w') as f:\n",
        "    json.dump(list(ruarg_test), f, indent=4, ensure_ascii=False)\n",
        "\n",
        "with open('tweetstance_train.json', 'w') as f:\n",
        "    json.dump(list(tweetstance_train), f, indent=4, ensure_ascii=False)\n",
        "with open('tweetstance_val.json', 'w') as f:\n",
        "    json.dump(list(tweetstance_val), f, indent=4, ensure_ascii=False)\n",
        "with open('tweetstance_test.json', 'w') as f:\n",
        "    json.dump(list(tweetstance_test), f, indent=4, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "txlj72DE_Nnf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "\n",
        "with open('ruarg_train.json') as f:\n",
        "    ra_t = json.load(f)\n",
        "with open('ruarg_val.json') as f:\n",
        "    ra_v = json.load(f)\n",
        "with open('tweetstance_train.json') as f:\n",
        "    ts_t = json.load(f)\n",
        "with open('tweetstance_val.json') as f:\n",
        "    ts_v = json.load(f)\n",
        "with open('arabic_stance_train.json') as f:\n",
        "    arb_t = json.load(f)\n",
        "with open('arabic_stance_val.json') as f:\n",
        "    arb_v = json.load(f)\n",
        "\n",
        "combined_train = ra_t + ts_t + arb_t\n",
        "combined_val = ra_v + ts_v + arb_v\n",
        "random.shuffle(combined_train)\n",
        "random.shuffle(combined_val)\n",
        "\n",
        "with open('combined_train.json', 'w') as f:\n",
        "    json.dump(list(combined_train), f, indent=4, ensure_ascii=False)\n",
        "with open('combined_val.json', 'w') as f:\n",
        "    json.dump(list(combined_val), f, indent=4, ensure_ascii=False)"
      ],
      "metadata": {
        "id": "R-dnaJQXbQ00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files={\"train\": \"combined_train.json\", \"validation\": \"combined_val.json\"})"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fi_JEa8mAhTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Дообучение"
      ],
      "metadata": {
        "id": "S3KErIGa7wHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "def compute_metrics_fixed(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    pred_labels = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(labels, pred_labels)\n",
        "\n",
        "    # Calculate precision, recall, and F1-score for each class\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, pred_labels, average='weighted')\n",
        "\n",
        "    # Convert to percentage\n",
        "    accuracy *= 100\n",
        "    precision *= 100\n",
        "    recall *= 100\n",
        "    f1 *= 100\n",
        "\n",
        "    return {\"Accuracy\": accuracy, \"Precision\": precision, \"Recall\": recall, \"F1\": f1}"
      ],
      "metadata": {
        "id": "pL_lSpOHqFAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "def make_dataset(data, tokenizer, label_field='label', language=None):\n",
        "    answer = []\n",
        "    for i, (s, label) in enumerate(zip(data[\"rus\"] if language == \"Russian\" else data[\"eng_tr\"], data[\"label\"])):\n",
        "        result = tokenizer(s, max_length=511, truncation=True)\n",
        "        result[\"labels\"] = label\n",
        "        answer.append(result)\n",
        "    return answer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
        "train_dataset = make_dataset(dataset[\"train\"], tokenizer, language=\"Russian\")\n",
        "dev_dataset = make_dataset(dataset[\"validation\"], tokenizer, language=\"Russian\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "AA9FSjeo6aLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import DataCollatorWithPadding, TrainingArguments, Trainer\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# def compute_metrics(eval_pred):\n",
        "#     logits, labels = eval_pred\n",
        "#     pred_labels = np.argmax(logits, axis=-1)\n",
        "#     are_equal = (pred_labels == labels)\n",
        "#     TP = np.sum(are_equal * labels)\n",
        "#     FP = np.sum((1.0 - are_equal) * (1-labels))\n",
        "#     FN = np.sum((1.0 - are_equal) * (1-pred_labels))\n",
        "#     correct, total = np.sum(are_equal), len(labels)\n",
        "#     return {\"Accuracy\": 100 * correct / total, \"P\": 100 * TP / max(TP+FP, 1.0), \"R\": 100 * TP / max(TP+FN, 1.0), \"F1\": 100 * TP / max(TP+0.5*FN+0.5*FP, 1.0)}"
      ],
      "metadata": {
        "id": "wbGeu7jx7Wnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=3)\n",
        "optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n",
        "training_args = TrainingArguments(output_dir=\"trainer_logs\",\n",
        "                                  evaluation_strategy=\"epoch\", save_strategy='epoch', num_train_epochs=5,\n",
        "                                  load_best_model_at_end=True, disable_tqdm=False,\n",
        "                                  per_device_train_batch_size=4, warmup_ratio=0.1,\n",
        "                                  gradient_accumulation_steps=4,\n",
        "                                  metric_for_best_model=\"Accuracy\", report_to=\"none\")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    optimizers=(optimizer, None),\n",
        "    args=training_args,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=dev_dataset,\n",
        "    compute_metrics=compute_metrics_fixed)\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "LalxSmzk7cHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset_arb = load_dataset(\"json\", data_files=\"arabic_stance_test.json\")\n",
        "test_dataset_ra = load_dataset(\"json\", data_files=\"ruarg_test.json\")\n",
        "test_dataset_ts = load_dataset(\"json\", data_files=\"tweetstance_test.json\")\n",
        "\n",
        "test_arb = make_dataset(test_dataset_arb['train'], tokenizer)\n",
        "test_ra = make_dataset(test_dataset_ra['train'], tokenizer)\n",
        "test_ts = make_dataset(test_dataset_ts['train'], tokenizer)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "yBSCLui_jQ79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "predictions_arb = trainer.predict(test_arb)\n",
        "print('Arabic stance:\\n',  predictions_arb.metrics)\n",
        "predictions_ra = trainer.predict(test_ra)\n",
        "print('RuArg:\\n', predictions_ra.metrics)\n",
        "predictions_ts = trainer.predict(test_ts)\n",
        "print('English Tweet Stance:\\n', predictions_ts.metrics)"
      ],
      "metadata": {
        "id": "F6L1oGdhixCD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy\n",
        "\n",
        "def predict_with_trainer(trainer, dataset, classes):\n",
        "    predictions = trainer.predict(dataset)\n",
        "    probs = scipy.special.softmax(predictions.predictions, axis=-1)\n",
        "    answer = [{\"label\": classes[np.argmax(elem)], \"probs\": elem} for elem in probs]\n",
        "    return answer"
      ],
      "metadata": {
        "id": "GCzulGBmn3rS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}