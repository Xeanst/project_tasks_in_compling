{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#RAG-система: 2 варианта"
      ],
      "metadata": {
        "id": "QK1PzBSslC1A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Установка зависимостей"
      ],
      "metadata": {
        "id": "MBPN4AKOsnGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers \"transformers>=4.37.0\" qdrant-client datasets pymorphy3 pymorphy3-dicts-ru"
      ],
      "metadata": {
        "id": "unfSGIIm28PR",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y numpy scipy gensim"
      ],
      "metadata": {
        "id": "K4dV6uZlyN6k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.23.5 scipy==1.10.1 gensim==4.3.2 transformers==4.37.0"
      ],
      "metadata": {
        "id": "6Qbo6PES1UvH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Необходимые импорты"
      ],
      "metadata": {
        "id": "C6zxEZs9VELD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list | grep numpy"
      ],
      "metadata": {
        "id": "qDjrN0lc4V-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --force-reinstall scipy gensim transformers"
      ],
      "metadata": {
        "id": "jzVPgzPD4ppe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import transformers\n",
        "print(gensim.__version__)\n"
      ],
      "metadata": {
        "id": "LbPpXlLV4xMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from qdrant_client import QdrantClient, models\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import transformers.data.metrics.squad_metrics as squad_metrics\n",
        "import pymorphy3\n",
        "import gensim.downloader\n",
        "import gensim\n",
        "import math\n",
        "import string\n",
        "import re\n",
        "random_state = 42\n",
        "import shap"
      ],
      "metadata": {
        "id": "Nn9kqRWsVEn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Загрузка датасета, токенизатора и модели для генерации ответов"
      ],
      "metadata": {
        "id": "K4IB1QpqAEgN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"bearberry/sberquadqa\")[\"train\"]"
      ],
      "metadata": {
        "id": "wWlXy082eBVA",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Берем первые 50 примеров из датасета:"
      ],
      "metadata": {
        "id": "j_vgnOOzJfSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions = dataset['question'][:50]\n",
        "correct_answers = dataset['normalized_answers'][:50]\n",
        "documents = dataset['context'][:50]"
      ],
      "metadata": {
        "id": "R15A9EVanCCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\"\n",
        "model_name = \"RefalMachine/RuadaptQwen2.5-1.5B-instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model_to_generate = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=\"auto\",\n",
        "    device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "id": "fvuWAAlPsQUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Предположительно, для более точных ответов, нам нужно будет явно прописать в промпте, сколько слов должно быть сгенерировано. Поэтому сразу посмотрим, какое максимальное количество слов в эталонном ответе."
      ],
      "metadata": {
        "id": "dXQ4_-1TC3-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 0\n",
        "for answer in correct_answers:\n",
        "  for example in answer:\n",
        "    if len(example.split()) > max_len:\n",
        "      max_len = len(example.split())\n",
        "print(max_len) #такое у нас будет ограничение на количество генерируемых слов"
      ],
      "metadata": {
        "id": "UD4v5psps69Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ответная система без retrieval"
      ],
      "metadata": {
        "id": "MThCsRVStIA5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**КОНТЕКСТ И ЧАНКИ:**\n",
        "\n",
        "Первое, над чем нужно задуматься, - сколько подавать чанков в контекст для генерации ответа и как именно это делать. Если просто взять в качестве контекста весь документ (т.е. весь список в поле \"context\"), то генерация будет очень затратной и по времени, и по вычислительным ресурсам. Но в каждом списке для каждого чанка есть информация, релевантен он для ответа или нет, так что мы можем воспользоваться этой информацией и брать, например, только релевантные чанки для формирования контекста. Но, возможно, только релевантных чанков будет слишком мало для точного ответа на вопрос и понадобится больший контекст. В таком случае, можно взять, например, окно чанков размером 3/5/10 чанков (в середине окна всегда релевантный для ответа чанк) и посмотреть, как это повлияет на качество ответов. Формировать контекст так, чтобы релевантный чанк оказывался в середине окна, кажется логичным и оптимальным, потому что полезная для ответа информация может быть и до релевантного чанка и после. Напишем функции, которые будут формировать \"окно контекстов\" из чанков.\n",
        "\n",
        "Еще одна эверистика, которая может помочь в улучшении качества, - просто повторить в промпте один и тот же релевантный чанк несколько раз. Для того, чтобы учесть это в функции, введем параметр rep: если rep == True, то повторяем один релевантный чанк window_size раз, если rep == False, то будет window_size разных чанков."
      ],
      "metadata": {
        "id": "rQXZm1kWDrxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def context_one_chunk(document):\n",
        "    final_chunks = []\n",
        "    for doc in document:\n",
        "      if doc[\"is_relevant\"]:\n",
        "        final_chunks.append(doc[\"chunk\"]) #релевантных чанков может быть несколько, объединяем их всех в один список\n",
        "    return final_chunks\n",
        "\n",
        "def context_chunks_window(document, rep, window_size):\n",
        "    chunks, final_chunks = [], []\n",
        "    if rep:\n",
        "      chunks = context_one_chunk(document)\n",
        "      final_chunks = [\" \".join(chunks) for _ in range(window_size)]\n",
        "    else:\n",
        "      i_rev = None #позиция релевантного чанка\n",
        "      for i, doc in enumerate(document):\n",
        "          chunks.append(doc[\"chunk\"])\n",
        "          if doc[\"is_relevant\"]:\n",
        "              i_rev = i\n",
        "      i_start = window_size//2\n",
        "      i_final = math.ceil(window_size/2) #округляем вверх\n",
        "      start = max(0, i_rev - i_start)\n",
        "      end = min(len(chunks), i_rev + i_final)\n",
        "      final_chunks = chunks[start:end]\n",
        "    return final_chunks"
      ],
      "metadata": {
        "id": "GjaU-iiHuqT_",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ПРОМПТ:**\n",
        "\n",
        "Для лучших ответов промпт нужно сформулировать точно и лаконично, сделав акцент на характере ответа, который мы хотим получить. Проанализирова эталонные ответы, можно сказать, что для нас самое главное, чтобы ответ модели был как можно более точным и кратким и полностью следовал из контекста. Также, чтобы ответ не был слишком большим, можно эксплицитно указать на ограничение по словам. Еще одна важная деталь для формирования подходящего промпта: сначала задать контекст, а только потом вопрос, чтобы модель как бы \"сосредоточилась\" именно на контексте и брала информацию из него, а потом только из вопроса. Чтобы усилить эффект промпта, мы также пропишем в роли для системы, что модель - эксперт по кратким и точным ответам. Это должно помочь. Таким образом, финальный промпт должен выглядеть примерно так:\n",
        "\n"
      ],
      "metadata": {
        "id": "01si89crO295"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = (\n",
        "    \"Ответь максимально точно и кратко на вопрос, используя только данный контекст. \"\n",
        "    \"Ответ не должен превышать 10 слов. Вопрос и контекст приведены ниже.\\n\\n\"\n",
        "    \"КОНТЕКСТ: \\n{}\\n\"\n",
        "    \"ВОПРОС: {}\"\n",
        ")"
      ],
      "metadata": {
        "id": "ZZ1J4sPyOZMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ФУНКЦИЯ ДЛЯ ФОРМИРОВАНИЯ ЗАПРОСОВ ЯЗЫКОВОЙ МОДЕЛИ И ГЕНЕРАЦИИ ОТВЕТОВ:**\n",
        "\n",
        "Для генерации новых токенов существует множество разных параметров, которые можно контролировать, это влияет на качество ответов модели и скорость генерации. Мы будем контролировать 2 параметра: **max_new_token** (максимальное количество новых токенов, которые модель может сгенерировать) и **num_beams** (количество лучей (beam search) при поиске наилучшего ответа; **Beam Search** исследует несколько возможных вариантов ответа и выбирает наиболее вероятный).\n",
        "\n",
        "Если значение **max_new_token** маленькое, то ответ будет коротким, если большое, то может быть слишком длинным. Нам нужны короткие ответы, будем брать **маленькие значения** для этого параметра (10-20 токенов).\n",
        "\n",
        "Чем больше **num_beams**, тем тщательнее модель подбирает ответ, но это замедляет генерацию. Оптимальное значение **3–5** (будет баланс между качеством и скоростью)."
      ],
      "metadata": {
        "id": "OUs-P3I7TPY5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response(questions, documents, rep, window_size, prompt, max_new_tokens, num_beams):\n",
        "  responses = []\n",
        "  for i, document in enumerate(documents):\n",
        "    question = questions[i]\n",
        "    final_chunks = context_chunks_window(document, rep, window_size)\n",
        "    new_prompt = prompt.format('\\n'.join(final_chunks), question)\n",
        "    message  = [{\"role\": \"system\", \"content\": \"Ты эксперт по кратким и точным ответам.\"}, #уточняем \"роль\" модели\n",
        "                      {\"role\": \"user\", \"content\": new_prompt}]\n",
        "    text = tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True) #форматируем входные сообщения в нужный формат для LLM\n",
        "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
        "    if num_beams is None: #базовый случай, num_beams по умолчанию = 1\n",
        "      generated_ids = model_to_generate.generate(model_inputs.input_ids, max_new_tokens=max_new_tokens, do_sample=True)\n",
        "    else: #с подбором параметра num_beams\n",
        "      generated_ids = model_to_generate.generate(\n",
        "            model_inputs.input_ids, max_new_tokens=max_new_tokens, num_beams=num_beams, early_stopping=True\n",
        "        )\n",
        "    response = tokenizer.batch_decode(generated_ids[:, model_inputs.input_ids.shape[-1]:], skip_special_tokens=True)[0]\n",
        "    responses.append(response)\n",
        "  return responses"
      ],
      "metadata": {
        "id": "tLMVsmwJgskt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Возьмем window_size = 10 (в части с ретривером это аналогично top_k, его мы тоже возьмем, равным 10), чтобы ответы были точнее (окно/top_k большего размера рассматривать не будем, т.к. генерация заметно замедлится)."
      ],
      "metadata": {
        "id": "2DglXrCyS5IH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "params = [\n",
        "    {\"rep\": False, \"max_new_tokens\": 15, \"num_beams\": None, \"window_size\": 10},\n",
        "    {\"rep\": True, \"max_new_tokens\": 15, \"num_beams\": 5, \"window_size\": 10},\n",
        "    {\"rep\": False, \"max_new_tokens\": 15, \"num_beams\": 5, \"window_size\": 10}\n",
        "]\n",
        "\n",
        "#тестируем базовый случай (num_beams по умолчанию равно 1)\n",
        "model_0 = generate_response(questions, documents, params[0][\"rep\"], params[0][\"window_size\"], prompt, params[0][\"max_new_tokens\"], params[0][\"num_beams\"])\n",
        "model_0  #смотрим, что сгенерировалось"
      ],
      "metadata": {
        "id": "Hp44xF6kYZSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "z5pnex3O8P7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder = LabelEncoder()\n",
        "def model(x):\n",
        "    responses = generate_response(x, [documents[0]], params[0][\"rep\"], params[0][\"window_size\"], prompt, params[0][\"max_new_tokens\"], params[0][\"num_beams\"])\n",
        "    #if len(responses) != len(x):  # Если количество ответов не совпадает с количеством вопросов\n",
        "    #    responses = [responses[0]] * len(x)  # Клонируем первый ответ для всех вопросов\n",
        "    encoded_responses = label_encoder.fit_transform(responses)\n",
        "    return np.array(responses).reshape(-1, 1)\n",
        "\n",
        "explainer = shap.Explainer(model, tokenizer, max_new_tokens=512)\n",
        "data = [questions[0]]\n",
        "#data = questions\n",
        "print(data)"
      ],
      "metadata": {
        "id": "rmkHaEFD1HxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "explanation = explainer(data)"
      ],
      "metadata": {
        "id": "Yi8LgX3F28QG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_1 = generate_response(questions, documents, params[1][\"rep\"], params[1][\"window_size\"], prompt, params[1][\"max_new_tokens\"], params[1][\"num_beams\"])\n",
        "model_1"
      ],
      "metadata": {
        "id": "_bs2UXxHTj_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2 = generate_response(questions, documents, params[2][\"rep\"], params[1][\"window_size\"], prompt, params[2][\"max_new_tokens\"], params[2][\"num_beams\"])\n",
        "model_2"
      ],
      "metadata": {
        "id": "JeKQAzXLCwY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Нормализация ответов"
      ],
      "metadata": {
        "id": "pelhZat0Uby0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для дальнейшего сравнения реализуем три функции для нормализации: с морфологическим анализом (**лемматизация**), со **стеммингом** и с учетом **синонимизации**. С помощью синонимизации мы смягчим метрику и будем считать положительными те случаи, когда модель не сгенерировала слово, как в эталоне, но выдавала синоним этого слова."
      ],
      "metadata": {
        "id": "MGQVKlaDX0Bg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "morph = pymorphy3.MorphAnalyzer()\n",
        "stemmer = SnowballStemmer(\"russian\")\n",
        "w2v_model = gensim.downloader.load(\"word2vec-ruscorpora-300\")\n",
        "\n",
        "#Функция для получения синонимов слова с помощью модели word2vec\n",
        "def get_synonym(word):\n",
        "    if word in w2v_model:\n",
        "        similar_words = w2v_model.most_similar(word, topn=5)\n",
        "        for similar_word, score in similar_words:\n",
        "            if score > 0.6:\n",
        "                return similar_word\n",
        "    return word\n",
        "\n",
        "#Предобработка ответа\n",
        "def process_response(response):\n",
        "  response = response.lower()\n",
        "  split_response = re.split(r'[.,!?;:()\\[\\]{}/-]', response)  #разделяем по знакам препинания\n",
        "  response = [s.strip() for s in split_response if s.strip()] #убираем пустые строки, если они есть\n",
        "  return response\n",
        "\n",
        "#Нормализация с морфологическим анализом\n",
        "def normalize_responses_morph(responses):\n",
        "  norm_responses = []\n",
        "  for response in responses:\n",
        "    response = process_response(response)\n",
        "    new_words = []\n",
        "    for x in response:\n",
        "        new_words += [morph.parse(word)[0].normal_form for word in x.split()]\n",
        "    norm_responses.append(\" \".join(new_words))\n",
        "  return norm_responses\n",
        "\n",
        "#Нормализация со стеммингом\n",
        "def normalize_responses_stem(responses):\n",
        "  norm_responses = []\n",
        "  for response in responses:\n",
        "    response = process_response(response)\n",
        "    new_words = []\n",
        "    for x in response:\n",
        "        new_words += [stemmer.stem(word) for word in x.split()]\n",
        "    norm_responses.append(\" \".join(new_words))\n",
        "  return norm_responses\n",
        "\n",
        "#Нормализация с учетом ближайших векторов и синонимов\n",
        "def normalize_with_vectors(responses):\n",
        "    norm_responses = []\n",
        "    new_words = normalize_responses_morph(responses)\n",
        "    new_words = [get_synonym(word) for word in new_words]\n",
        "    norm_responses.append(\" \".join(new_words))\n",
        "    return norm_responses"
      ],
      "metadata": {
        "id": "aqNNXMoxUYxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Функция подсчета метрики, оценка результатов"
      ],
      "metadata": {
        "id": "Pj1AMO19aE5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics_norm(norm_responses, norm_correct_answers):\n",
        "    #считаем f-меру для каждой пары ответов и затем берем среднее значение\n",
        "    return 100.0 * sum(squad_metrics.compute_f1(a, r) for a, r in zip(norm_correct_answers, norm_responses)) / len(norm_responses)"
      ],
      "metadata": {
        "id": "cpYXkt1e_icj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для корректного сравнения и ответы модели, и эталонные ответы из датасета должны быть нормализованы одинаково, так что применим нормализацию к обоим спискам ответов:"
      ],
      "metadata": {
        "id": "nKYJlhamYavr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cor_answers = []\n",
        "for answer in correct_answers:\n",
        "  cor_answers.append(\" \".join(x for x in answer)) #делаем из списка списков список строк\n",
        "\n",
        "#нормализуем эталонные ответы\n",
        "norm_cor_ans = [\n",
        "    normalize_responses_morph(cor_answers),\n",
        "    normalize_responses_stem(cor_answers),\n",
        "    normalize_with_vectors(cor_answers)\n",
        "    ]"
      ],
      "metadata": {
        "id": "XYtHzL1TndGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#нормализуем ответы моделей, выводим f-меру\n",
        "normalizations = [\"morph\", \"stem\", \"vectors\"]\n",
        "models_with_params = [model_0, model_1, model_2]\n",
        "for model, param in zip(models_with_params, params):\n",
        "  norm_pred_ans = [\n",
        "      normalize_responses_morph(model),\n",
        "      normalize_responses_stem(model),\n",
        "      normalize_with_vectors(model)\n",
        "  ]\n",
        "  print(param)\n",
        "  for normalization, norm_pred_answer, norm_cor_answer in zip(normalizations, norm_pred_ans, norm_cor_ans):\n",
        "    print(f\"F1 score with {normalization}: {compute_metrics_norm(norm_pred_answer, norm_cor_answer)}\")\n",
        "  print(\"\")"
      ],
      "metadata": {
        "id": "Q2nOhWXvU7nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мы видим, что при увеличении количества лучей поиска качество ответов заметно улучшается, а также, что, если сравнивать не только нормализованные слова, но и прикрутить синонимизацию, то качество будет выше. Если не пользоваться синонимизацией, то можно опираться на морфологический анализ, с помощью него получается более высокая метрика. Также видно, что если просто повторять релевантные чанки в промпте, а не брать разные, то качество ответов будет лучше."
      ],
      "metadata": {
        "id": "3qtUxuk0oMDi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ответная система с retrieval"
      ],
      "metadata": {
        "id": "rPT8RhE0bdDN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Будем использовать Qdrant в качестве базы данных для эмбеддингов (хорошо подходит, т.к. быстро ищет похожие документы, в ней можно хранить текст и метаданные, у нее простая интеграция с Python):"
      ],
      "metadata": {
        "id": "-LfyhHBAM699"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Подключение к локальной векторной БД\n",
        "client = QdrantClient(\":memory:\")\n",
        "\n",
        "#Генерируем эмбеддинги для документов\n",
        "def generate_embeddings(docs, model):\n",
        "    embeddings = []\n",
        "    for chunk in docs:\n",
        "        embedding = model.encode(chunk, convert_to_tensor=True)\n",
        "        embeddings.append(embedding.cpu().numpy())\n",
        "    return np.array(embeddings)\n",
        "\n",
        "#Создаем векторную БД\n",
        "def create_qdrant_collection(collection_name, vector_dim):\n",
        "  client.create_collection(\n",
        "        collection_name=collection_name,\n",
        "        vectors_config=models.VectorParams(\n",
        "        size=vector_dim,\n",
        "        distance=models.Distance.COSINE\n",
        "            )\n",
        "      )\n",
        "\n",
        "#Загрузка данных в Qdrant\n",
        "def store_embeddings_in_qdrant(embeddings, metadata, collection_name):\n",
        "    points = []\n",
        "    for i, embedding in enumerate(embeddings):\n",
        "        point = models.PointStruct(\n",
        "            id=i,\n",
        "            vector=embedding.tolist(),\n",
        "            payload=metadata[i]\n",
        "        )\n",
        "        points.append(point)\n",
        "    batch_size = 100\n",
        "    for start in range(0, len(points), batch_size):\n",
        "        batch_points = points[start:start + batch_size]\n",
        "        client.upsert(collection_name=collection_name, points=batch_points)\n",
        "\n",
        "#Поиск наиболее близких к запросу документов, повторения возможны\n",
        "def query_qdrant_rep(query_embedding, collection_name, top_k):\n",
        "    search_result = client.search(\n",
        "        collection_name=collection_name,\n",
        "        query_vector=query_embedding,\n",
        "        limit=top_k,\n",
        "        with_payload=True\n",
        "    )\n",
        "    return [hit.payload for hit in search_result]\n",
        "\n",
        "#Поиск наиболее близких к запросу документов, документы не повторяются\n",
        "def query_qdrant_no_rep(query_embedding, collection_name, top_k, exclude_ids=None):\n",
        "    if exclude_ids is None:\n",
        "        exclude_ids = set()\n",
        "\n",
        "    query_filter = models.Filter(\n",
        "        must_not=[models.HasIdCondition(has_id=list(exclude_ids))]\n",
        "    ) if exclude_ids else None\n",
        "\n",
        "    search_result = client.search(\n",
        "        collection_name=collection_name,\n",
        "        query_vector=query_embedding,\n",
        "        limit=top_k,\n",
        "        query_filter=query_filter,\n",
        "    )\n",
        "\n",
        "    retrieved_docs = [(hit.id, hit.payload[\"chunk\"]) for hit in search_result]\n",
        "    return retrieved_docs\n",
        "\n",
        "#Функция для формировния контекстов из разных документов\n",
        "def retrieve_dif_docs(query_embedding, collection_name, top_k):\n",
        "    found_docs = set() #исключаем ID уже найденных докуметнов\n",
        "    all_retrieved_docs = []\n",
        "\n",
        "    for i in range(top_k):\n",
        "        new_docs = query_qdrant_no_rep(query_embedding, collection_name, top_k, exclude_ids=found_docs)\n",
        "\n",
        "        if not new_docs:\n",
        "            break\n",
        "\n",
        "        found_docs.update(doc[0] for doc in new_docs)  #запоминаем ID\n",
        "        for doc in new_docs:\n",
        "          if doc[1] not in all_retrieved_docs:\n",
        "            all_retrieved_docs.append(doc[1])\n",
        "\n",
        "    return all_retrieved_docs"
      ],
      "metadata": {
        "id": "dxbrMq9EM9sW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Используем библиотеку SentenceTransformer, т.к. она разработана специально для векторизации предложений и поиска семантического сходства. В качестве модели для создания эмбеддингов возьмем **\"intfloat/multilingual-e5-large\"** (хорошо подходит, т.к. создает вектора, совместимые с Qdrant, поддерживает русский язык и подходит для RAG, т.к. обучена на задачах поиска)."
      ],
      "metadata": {
        "id": "hr01JNIy33X3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rmodel = SentenceTransformer(\"intfloat/multilingual-e5-large\")\n",
        "\n",
        "#Подготавливаем документы для векторизации\n",
        "docs = []\n",
        "for elem in documents:\n",
        "  for chunk in elem:\n",
        "    docs.append(chunk[\"chunk\"])\n",
        "\n",
        "#Подготавливаем метаданные, в этом словаре будут храниться сами тексты\n",
        "metadata = [{'chunk': chunk} for chunk in docs]\n",
        "\n",
        "collection_name = \"RAG_vectors\"\n",
        "\n",
        "#Создадим БД\n",
        "context_vectors = generate_embeddings(docs, model)\n",
        "create_qdrant_collection(collection_name, context_vectors.shape[1])\n",
        "store_embeddings_in_qdrant(context_vectors, metadata, collection_name)"
      ],
      "metadata": {
        "id": "akJ9HdWBQZm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Посмотрим на пример\n",
        "top_k = 5\n",
        "query_text = \"чем представлены органические остатки?\"\n",
        "query_embedding = model.encode(query_text, convert_to_tensor=True).cpu().numpy()\n",
        "result = query_qdrant_rep(query_embedding, collection_name, top_k) #документы могут повторяться\n",
        "print(\"query result rep:\", result)\n",
        "\n",
        "result = retrieve_dif_docs(query_embedding, collection_name, top_k) #документы не могут повторяться\n",
        "print(\"query result no rep:\", result)"
      ],
      "metadata": {
        "id": "GyYmCIJpiGdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ФУНКЦИЯ ДЛЯ ФОРМИРОВАНИЯ ЗАПРОСОВ ЯЗЫКОВОЙ МОДЕЛИ И ГЕНЕРАЦИИ ОТВЕТОВ:**\n",
        "\n",
        "Принцип подбора значений параметров **max_new_tokens** и **num_beams** такой же, как и в части без retrieval."
      ],
      "metadata": {
        "id": "MQcaOJgUhTm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response_qdrant(questions, rep, collection_name, top_k, max_new_tokens, num_beams, prompt):\n",
        "  responses = []\n",
        "  for question in questions:\n",
        "    query_embedding = model.encode(question, convert_to_tensor=True).cpu().numpy()\n",
        "    if rep:\n",
        "      relevant_metadata = query_qdrant_rep(query_embedding, collection_name, top_k)\n",
        "      context = \"\\n\".join([meta['chunk'] for meta in relevant_metadata])\n",
        "    else:\n",
        "      relevant_metadata = retrieve_dif_docs(query_embedding, collection_name, top_k)\n",
        "      context = \"\\n\".join(meta for meta in relevant_metadata)\n",
        "    new_prompt = prompt.format(context, question)\n",
        "    message  = [{\"role\": \"system\", \"content\": \"Ты эксперт по кратким и точным ответам.\"},\n",
        "                      {\"role\": \"user\", \"content\": new_prompt}]\n",
        "    text = tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True)\n",
        "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
        "    generated_ids = model_to_generate.generate(\n",
        "          model_inputs.input_ids, max_new_tokens=max_new_tokens, num_beams=num_beams, early_stopping=True\n",
        "      )\n",
        "    response = tokenizer.batch_decode(generated_ids[:, model_inputs.input_ids.shape[-1]:], skip_special_tokens=True)[0]\n",
        "    responses.append(response)\n",
        "  return responses"
      ],
      "metadata": {
        "id": "BBhQ_tLr0Jqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#базовую модель (num_beams = 1) тестировать не будем, будем сразу смотреть с num_beams = 5\n",
        "params = [\n",
        "    {\"rep\": True, \"max_new_tokens\": 15, \"num_beams\": 5, \"top_k\": 10},\n",
        "    {\"rep\": True, \"max_new_tokens\": 20, \"num_beams\": 5, \"top_k\": 10},\n",
        "    {\"rep\": False, \"max_new_tokens\": 20, \"num_beams\": 5, \"top_k\": 10}\n",
        "]\n",
        "\n",
        "prompt = (\n",
        "    \"Ответь максимально точно и кратко на вопрос, используя только данный контекст. \"\n",
        "    \"Ответ не должен превышать 10 слов. Вопрос и контекст приведены ниже.\\n\\n\"\n",
        "    \"КОНТЕКСТ: \\n{}\\n\"\n",
        "    \"ВОПРОС: {}\"\n",
        ")\n",
        "largemodel_0 = generate_response_qdrant(questions, params[0][\"rep\"], collection_name, params[0][\"top_k\"], params[0][\"max_new_tokens\"], params[0][\"num_beams\"], prompt)\n",
        "largemodel_0"
      ],
      "metadata": {
        "id": "IkYHl94DiUmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "largemodel_1 = generate_response_qdrant(questions, params[1][\"rep\"], collection_name, params[1][\"top_k\"], params[1][\"max_new_tokens\"], params[1][\"num_beams\"], prompt)\n",
        "largemodel_1"
      ],
      "metadata": {
        "id": "VLtWi68a8uAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "largemodel_2 = generate_response_qdrant(questions, params[2][\"rep\"], collection_name, params[2][\"top_k\"], params[2][\"max_new_tokens\"], params[2][\"num_beams\"], prompt)\n",
        "largemodel_2"
      ],
      "metadata": {
        "id": "X2Whky_pR-KI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#нормализуем ответы моделей, выводим f-меру\n",
        "normalizations = [\"morph\", \"stem\", \"vectors\"]\n",
        "models_with_params = [largemodel_0, largemodel_1, largemodel_2]\n",
        "for model, param in zip(models_with_params, params):\n",
        "  norm_pred_ans_q = [\n",
        "      normalize_responses_morph(model),\n",
        "      normalize_responses_stem(model),\n",
        "      normalize_with_vectors(model)\n",
        "  ]\n",
        "  print(param)\n",
        "  for normalization, norm_pred_answer, norm_cor_answer in zip(normalizations, norm_pred_ans_q, norm_cor_ans):\n",
        "    print(f\"F1 score with {normalization}: {compute_metrics_norm(norm_pred_answer, norm_cor_answer)}\")\n",
        "  print(\"\")"
      ],
      "metadata": {
        "id": "S7T0H60_kDdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Результаты примерно повторяют тенденции того, что мы видели в части без retrieval. Если брать одни и те же чанки для контекста, то качество увеличивается. Если взять макимальную длину генерируемых токенов немного побольше, то качество тоже может немного прирасти, это, видимо, связано с тем, что какие-то нужные слова не обрезаются, если генерируется больше токенов."
      ],
      "metadata": {
        "id": "IAGpe8AYna_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Финальные заметки и выводы"
      ],
      "metadata": {
        "id": "pz1Jm7hTi_fT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В обеих частях удалось достичь неплохого качества (60-77%). В части без retrieval, когда были выбраны те же параметры, что и в части с retrieval, качество оказалось выше (77% vs 72%). Видимо, когда мы подаем модели чанки, которые релевантны согласно разметке в датасете, а не даем ретриверу “самому” найти релевантные чанки,  то ответы модели ближе к эталонным из датасета. Чтобы ответы модели были ближе к эталонным можно сделать следующие вещи:\n",
        "\n",
        "1.   брать достаточно большое количество  чанков для формирования контекста (5-10 или, может быть, больше, если не так важна скорость генерации),\n",
        "2.   контролировать максимальную длину генерируемых токенов (она должна быть не очень большой, т.к. ответы краткие),\n",
        "3.   брать достаточное количество лучей для BeamSearch (5 оказалось оптимальным, можно было попробовать взять больше, но это бы замедлило генерацию),\n",
        "4.   хорошо сработало повторение релевантных чанков в контексте, который\n",
        "прописывается в промпте,\n",
        "5.   если нам не так важно, чтобы модель генерировала именно те же корни и леммы, что и в эталонных ответах, мы можем смягчить подсчет метрик за счет нормализации с использованием синонимизации."
      ],
      "metadata": {
        "id": "ubVc4fkmjDm7"
      }
    }
  ]
}