{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install librosa pandas\n"
      ],
      "metadata": {
        "id": "4Bz3hN-ZGKg7",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "kJpiKDsKxN2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import librosa\n",
        "\n",
        "def create_dataset(audio_dir, transcript_dir, output_csv):\n",
        "    dataset = []\n",
        "    transcript_path = r\"/content/drive/MyDrive/os-transcriptions/AgA_adӕmyxatt_cont.txt\"\n",
        "    import chardet\n",
        "\n",
        "    with open(transcript_path, 'rb') as f:\n",
        "     raw_data = f.read()\n",
        "     result = chardet.detect(raw_data)\n",
        "     encoding = result['encoding']\n",
        "\n",
        "    # Iterate over all the files in the audio directory\n",
        "    for audio_file in os.listdir(audio_dir):\n",
        "        if audio_file.endswith('.WAV'):\n",
        "            # Get corresponding transcript file\n",
        "            transcript_file = audio_file.replace('.WAV', '.txt')\n",
        "            transcript_path = os.path.join(transcript_dir, transcript_file)\n",
        "\n",
        "            if os.path.exists(transcript_path):\n",
        "                # Load audio file\n",
        "                audio_path = os.path.join(audio_dir, audio_file)\n",
        "                audio, sample_rate = librosa.load(audio_path, sr=None)\n",
        "\n",
        "                # Optional: Extract features or keep as is\n",
        "                mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=13)\n",
        "\n",
        "                # Read the transcription file with the correct encoding\n",
        "                with open(transcript_path, 'r', encoding=encoding) as f:\n",
        "                    transcription = f.read().strip()\n",
        "\n",
        "                # Append audio file and transcription to the dataset\n",
        "                dataset.append({\n",
        "                    'audio_file': audio_file,\n",
        "                    'transcript_file': transcript_file,\n",
        "                    'transcription': transcription,\n",
        "                    'mfccs': mfccs.tolist()\n",
        "                })\n",
        "\n",
        "    # Create a DataFrame\n",
        "    df = pd.DataFrame(dataset)\n",
        "\n",
        "    # Save to CSV with UTF-8 encoding\n",
        "    df.to_csv(output_csv, index=False, encoding=encoding)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    audio_directory = r'/content/drive/MyDrive/os-audios'               # Change to your audio directory\n",
        "    transcript_directory = r'/content/drive/MyDrive/os-transcriptions'     # Change to your transcriptions directory\n",
        "    dataset = '/content/sample_data/dataset.csv'               # Output CSV file\n",
        "\n",
        "    create_dataset(audio_directory, transcript_directory, dataset)"
      ],
      "metadata": {
        "id": "8yUiu7q-sCRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_csv(dataset)"
      ],
      "metadata": {
        "id": "xDrJSYn5208-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "kxjBXFAUQakc",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login(token='')"
      ],
      "metadata": {
        "id": "5cThE1mmSk9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds_train = load_dataset(\"mozilla-foundation/common_voice_17_0\", \"os\", split=\"train\")\n",
        "ds_val = load_dataset(\"mozilla-foundation/common_voice_17_0\", \"os\", split=\"validation\")\n",
        "ds_test = load_dataset(\"mozilla-foundation/common_voice_17_0\", \"os\", split=\"test\")"
      ],
      "metadata": {
        "id": "KKQVhF-rPEfh",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ds_train)\n",
        "print(ds_val)\n",
        "print(ds_test)"
      ],
      "metadata": {
        "id": "gLkXUkAQVWxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_test[0][\"audio\"][\"array\"].shape"
      ],
      "metadata": {
        "id": "GSOJD4nTVisQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor, AutoModelForCTC\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"facebook/mms-1b-all\")\n",
        "model = AutoModelForCTC.from_pretrained(\"facebook/mms-1b-all\")"
      ],
      "metadata": {
        "id": "soL6PS4RVmzw",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch accelerate torchaudio datasets\n",
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "id": "rlI8rVotbcCQ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Audio\n",
        "\n",
        "stream_data = load_dataset(\"mozilla-foundation/common_voice_17_0\", \"os\", split=\"test\", streaming=True)\n",
        "stream_data = stream_data.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "\n",
        "sample_iterator = iter(stream_data)\n",
        "first_sample = next(sample_iterator)[\"audio\"][\"array\"]\n",
        "second_sample = next(sample_iterator)[\"audio\"][\"array\"]\n",
        "third_sample = next(sample_iterator)[\"audio\"][\"array\"]\n",
        "fourth_sample = next(sample_iterator)[\"audio\"][\"array\"]\n",
        "fifth_sample = next(sample_iterator)[\"audio\"][\"array\"]\n",
        "sixth_sample = next(sample_iterator)[\"audio\"][\"array\"]\n",
        "seventh_sample = next(sample_iterator)[\"audio\"][\"array\"]\n",
        "eighth_sample = next(sample_iterator)[\"audio\"][\"array\"]\n",
        "ninth_sample = next(sample_iterator)[\"audio\"][\"array\"]\n",
        "tenth_sample = next(sample_iterator)[\"audio\"][\"array\"]\n",
        "eleventh_sample = next(sample_iterator)[\"audio\"][\"array\"]\n",
        "\n",
        "i12_sample = next(sample_iterator)[\"audio\"][\"array\"]\n",
        "i13_sample = next(sample_iterator)[\"audio\"][\"array\"]\n",
        "i14_sample = next(sample_iterator)[\"audio\"][\"array\"]\n",
        "i15_sample = next(sample_iterator)[\"audio\"][\"array\"]\n",
        "i16_sample = next(sample_iterator)[\"audio\"][\"array\"]\n",
        "i17_sample = next(sample_iterator)[\"audio\"][\"array\"]\n",
        "i18_sample = next(sample_iterator)[\"audio\"][\"array\"]\n",
        "i19_sample = next(sample_iterator)[\"audio\"][\"array\"]\n",
        "i20_sample = next(sample_iterator)[\"audio\"][\"array\"]\n",
        "i21_sample = next(sample_iterator)[\"audio\"][\"array\"]\n",
        "i22_sample = next(sample_iterator)[\"audio\"][\"array\"]\n",
        "\n",
        "i23_sample = next(sample_iterator)[\"audio\"][\"array\"]\n",
        "i24_sample = next(sample_iterator)[\"audio\"][\"array\"]\n",
        "i25_sample = next(sample_iterator)[\"audio\"][\"array\"]\n",
        "i26_sample = next(sample_iterator)[\"audio\"][\"array\"]\n",
        "i27_sample = next(sample_iterator)[\"audio\"][\"array\"]\n",
        "i28_sample = next(sample_iterator)[\"audio\"][\"array\"]\n",
        "i29_sample = next(sample_iterator)[\"audio\"][\"array\"]\n",
        "i30_sample = next(sample_iterator)[\"audio\"][\"array\"]\n",
        "i31_sample = next(sample_iterator)[\"audio\"][\"array\"]\n",
        "i32_sample = next(sample_iterator)[\"audio\"][\"array\"]\n",
        "i33_sample = next(sample_iterator)[\"audio\"][\"array\"]\n"
      ],
      "metadata": {
        "id": "fWSs6QvLb_Sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stream_data"
      ],
      "metadata": {
        "id": "AVHZz7xfVPzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(first_sample)\n",
        "print(second_sample)"
      ],
      "metadata": {
        "id": "uRdI-OgIcUvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2ForCTC, AutoProcessor\n",
        "import torch\n",
        "\n",
        "model_id = \"facebook/mms-1b-all\"\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "model = Wav2Vec2ForCTC.from_pretrained(model_id)\n"
      ],
      "metadata": {
        "id": "WZc38EMvchOb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = processor(i31_sample, sampling_rate=16_000, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs).logits\n",
        "\n",
        "ids = torch.argmax(outputs, dim=-1)[0]\n",
        "transcription = processor.decode(ids)\n",
        "print(transcription)\n",
        "\n",
        "inputs = processor(i32_sample, sampling_rate=16_000, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs).logits\n",
        "\n",
        "ids = torch.argmax(outputs, dim=-1)[0]\n",
        "transcription = processor.decode(ids)\n",
        "print(transcription)\n",
        "\n",
        "inputs = processor(i33_sample, sampling_rate=16_000, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs).logits\n",
        "\n",
        "ids = torch.argmax(outputs, dim=-1)[0]\n",
        "transcription = processor.decode(ids)\n",
        "print(transcription)"
      ],
      "metadata": {
        "id": "XGJIP3Qzc1_F",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "vylhEK38nSKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install soundfile"
      ],
      "metadata": {
        "collapsed": true,
        "id": "hACySn9TkRRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "from scipy.signal import resample\n",
        "\n",
        "def resample_audio(input_file, target_sample_rate):\n",
        "    data, original_sample_rate = sf.read(input_file)\n",
        "\n",
        "    number_of_samples = round(len(data) * float(target_sample_rate) / original_sample_rate)\n",
        "    resampled_data = resample(data, number_of_samples)\n",
        "\n",
        "    return resampled_data, target_sample_rate\n",
        "\n",
        "def process_folder(input_folder, output_folder, target_sample_rate=16000):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    for filename in os.listdir(input_folder):\n",
        "        if filename.endswith('.WAV'):\n",
        "            input_file = os.path.join(input_folder, filename)\n",
        "            output_file = os.path.join(output_folder, filename)\n",
        "\n",
        "            resampled_data, sample_rate = resample_audio(input_file, target_sample_rate)\n",
        "\n",
        "            sf.write(output_file, resampled_data, sample_rate)\n",
        "            print(f'Resampled: {filename}')\n",
        "\n",
        "input_folder = '/content/drive/MyDrive/new_os'\n",
        "output_folder = '/content/drive/MyDrive/new_os_resamp'\n",
        "\n",
        "process_folder(input_folder, output_folder)"
      ],
      "metadata": {
        "id": "cagFbR1c3xTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "bdGdPdXB-Cgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Wav2Vec2ForCTC"
      ],
      "metadata": {
        "id": "l1DNNA8P-VI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor"
      ],
      "metadata": {
        "id": "pFiIGllR_VTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_files = [\n",
        "\"/content/drive/MyDrive/new_os_resamp/os-audios-resampled/AgA_adӕmyxatt_cont.WAV\",\n",
        "\"/content/drive/MyDrive/new_os_resamp/os-audios-resampled/AgA_kalak_cont.WAV\",\n",
        "\n",
        "\"/content/drive/MyDrive/new_os_resamp/os-audios-resampled/KG_abӕstag.WAV\",\n",
        "\"/content/drive/MyDrive/new_os_resamp/os-audios-resampled/KG_bajsyn.WAV\",\n",
        "\"/content/drive/MyDrive/new_os_resamp/os-audios-resampled/KG_balc.WAV\",\n",
        "\"/content/drive/MyDrive/new_os_resamp/os-audios-resampled/KG_bik'.WAV\",\n",
        "\"/content/drive/MyDrive/new_os_resamp/os-audios-resampled/KG_c'ar.WAV\",\n",
        "\"/content/drive/MyDrive/new_os_resamp/os-audios-resampled/KG_c'ar.WAV\",\n",
        "\"/content/drive/MyDrive/new_os_resamp/os-audios-resampled/KG_c'ata.WAV\",\n",
        "\"/content/drive/MyDrive/new_os_resamp/os-audios-resampled/KG_c'iw.WAV\",\n",
        "\"/content/drive/MyDrive/new_os_resamp/os-audios-resampled/KG_dzag.WAV\",\n",
        "\"/content/drive/MyDrive/new_os_resamp/os-audios-resampled/KG_gal.WAV\",\n",
        "\"/content/drive/MyDrive/new_os_resamp/os-audios-resampled/KG_k'am.WAV\",\n",
        "\"/content/drive/MyDrive/new_os_resamp/os-audios-resampled/KG_kalak.WAV\",\n",
        "\"/content/drive/MyDrive/new_os_resamp/os-audios-resampled/KG_rak'axyn.WAV\",\n",
        "\"/content/drive/MyDrive/new_os_resamp/os-audios-resampled/KG_rak'axyn.WAV\",\n",
        "\"/content/drive/MyDrive/new_os_resamp/os-audios-resampled/KG_sax'.WAV\",\n",
        "\"/content/drive/MyDrive/new_os_resamp/os-audios-resampled/KG_sax'at.WAV\",\n",
        "\"/content/drive/MyDrive/new_os_resamp/os-audios-resampled/KG_tas.WAV\",\n",
        "\"/content/drive/MyDrive/new_os_resamp/os-audios-resampled/KG_wagaxast.WAV\",\n",
        "\n",
        "\"/content/drive/MyDrive/new_os_resamp/os-audios-resampled/IG_adӕmyxatt_cont.WAV\",\n",
        "\"/content/drive/MyDrive/new_os_resamp/os-audios-resampled/IG_adӕmyxatt_is.WAV\",\n",
        "\"/content/drive/MyDrive/new_os_resamp/os-audios-resampled/IG_becykk_cont.WAV\",\n",
        "\"/content/drive/MyDrive/new_os_resamp/os-audios-resampled/IG_becykk_is.WAV\",\n",
        "\"/content/drive/MyDrive/new_os_resamp/os-audios-resampled/IG_ch'epp_all.WAV\",\n",
        "\"/content/drive/MyDrive/new_os_resamp/os-audios-resampled/IG_chitt_all.WAV\",\n",
        "\"/content/drive/MyDrive/new_os_resamp/os-audios-resampled/IG_dzӕcc_all.WAV\",\n",
        "\"/content/drive/MyDrive/new_os_resamp/os-audios-resampled/IG_fӕnyk_all.WAV\",\n",
        "\"/content/drive/MyDrive/new_os_resamp/os-audios-resampled/IG_gakk_all.WAV\",\n",
        "\"/content/drive/MyDrive/new_os_resamp/os-audios-resampled/IG_gomgӕrcc_all.WAV\",\n",
        "\"/content/drive/MyDrive/new_os_resamp/os-audios-resampled/IG_k'opp_all.WAV\",\n",
        "\"/content/drive/MyDrive/new_os_resamp/os-audios-resampled/IG_k'ӕrcc_all.WAV\",\n",
        "\"/content/drive/MyDrive/new_os_resamp/os-audios-resampled/IG_kalak_all.WAV\",\n",
        "\"/content/drive/MyDrive/new_os_resamp/os-audios-resampled/IG_kurdiat_all.WAV\",\n",
        "\"/content/drive/MyDrive/new_os_resamp/os-audios-resampled/IG_kӕrc_all.WAV\",\n",
        "\"/content/drive/MyDrive/new_os_resamp/os-audios-resampled/IG_mit_all.WAV\",\n",
        "\"/content/drive/MyDrive/new_os_resamp/os-audios-resampled/IG_nymӕc_all.WAV\",\n",
        "\"/content/drive/MyDrive/new_os_resamp/os-audios-resampled/IG_sybyrtt_all.WAV\",\n",
        "\"/content/drive/MyDrive/new_os_resamp/os-audios-resampled/IG_sӕbӕkk_all.WAV\",\n",
        "\"/content/drive/MyDrive/new_os_resamp/os-audios-resampled/IG_tutt_all.WAV\"\n",
        "\n",
        "]\n",
        "import torch\n",
        "import soundfile as sf\n",
        "\n",
        "model_id = \"facebook/mms-1b-all\"\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "model = Wav2Vec2ForCTC.from_pretrained(model_id)\n",
        "\n",
        "def transcribe_audio(file_path):\n",
        "    audio_input, sampling_rate = sf.read(file_path)\n",
        "\n",
        "    inputs = processor(audio_input, sampling_rate=16000, return_tensors=\"pt\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs).logits\n",
        "\n",
        "    ids = torch.argmax(outputs, dim=-1)[0]\n",
        "    transcription = processor.decode(ids)\n",
        "\n",
        "    return transcription\n",
        "\n",
        "for audio_file in audio_files:\n",
        "    transcription = transcribe_audio(audio_file)\n",
        "    print(f\"Transcription for {audio_file}:\\n{transcription}\\n\")"
      ],
      "metadata": {
        "id": "5Kivvkk-kWvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transcriptions_file = r'/content/drive/MyDrive/os-dataset/transcriptions.txt'\n",
        "transcriptions_df = pd.read_csv(transcriptions_file, delimiter='\\t', header=None, names=[\"full\"])\n",
        "transcriptions_df[\"filename\"] = transcriptions_df['full'].apply(lambda x: re.split(\" \", x)[0])\n",
        "transcriptions_df[\"transcriptions\"] = transcriptions_df['full'].apply(lambda x:' '.join(re.split(\" \", x)[1:]))\n",
        "transcriptions_df = transcriptions_df.drop(columns=['full'])\n",
        "print(transcriptions_df)\n",
        "audio_dir = r'/content/drive/MyDrive/os-dataset/os-audios'\n",
        "transcriptions_df['filepath'] = transcriptions_df['filename'].apply(lambda x: os.path.join(audio_dir, x))"
      ],
      "metadata": {
        "id": "ht4jcjLohlMf",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_8IweNs7pgr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import Wav2Vec2ForCTC, AutoProcessor, TrainingArguments, Trainer"
      ],
      "metadata": {
        "id": "Ax9dxAM7poQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset from DataFrame\n",
        "dataset = Dataset.from_pandas(transcriptions_df)\n",
        "print(dataset)\n",
        "# Define a function to load and process audio files\n",
        "def load_audio(example):\n",
        "    example[\"filepath\"] = re.split(\" \", example[\"filepath\"])[0]\n",
        "    audio_input, sampling_rate = sf.read(example[\"filepath\"])\n",
        "    example[\"audio\"] = audio_input\n",
        "    example[\"sampling_rate\"] = sampling_rate\n",
        "    return example\n",
        "\n",
        "dataset = dataset.map(load_audio)\n",
        "print(dataset[0])\n"
      ],
      "metadata": {
        "id": "edu5DhG_pwYU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[1])"
      ],
      "metadata": {
        "id": "yRt7ef0JyAC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"facebook/mms-1b-all\"\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)\n",
        "model = Wav2Vec2ForCTC.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "kn8fXUa92zLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import Wav2Vec2ForCTC, AutoProcessor, TrainingArguments, Trainer\n",
        "\n",
        "# Load transcriptions\n",
        "transcriptions_file = r'/content/drive/MyDrive/os-dataset/transcriptions.txt'\n",
        "transcriptions_df = pd.read_csv(transcriptions_file, delimiter='\\t', header=None, names=[\"full\"])\n",
        "transcriptions_df[\"filename\"] = transcriptions_df['full'].apply(lambda x: re.split(\" \", x)[0])\n",
        "transcriptions_df[\"transcriptions\"] = transcriptions_df['full'].apply(lambda x:' '.join(re.split(\" \", x)[1:]))\n",
        "transcriptions_df = transcriptions_df.drop(columns=['full'])\n",
        "\n",
        "# Get the audio file paths\n",
        "audio_dir = r'/content/drive/MyDrive/os-dataset/os-audios'\n",
        "transcriptions_df['filepath'] = transcriptions_df['filename'].apply(lambda x: os.path.join(audio_dir, x))\n",
        "\n",
        "# Create dataset from DataFrame\n",
        "dataset = Dataset.from_pandas(transcriptions_df)\n",
        "\n",
        "# Define a function to load and process audio files\n",
        "def load_audio(example):\n",
        "    audio_input, sampling_rate = sf.read(example[\"filepath\"])\n",
        "    example[\"audio\"] = audio_input\n",
        "    example[\"sampling_rate\"] = sampling_rate\n",
        "\n",
        "    # Convert audio_input to a numpy float array\n",
        "    example[\"audios\"] = np.array(audio_input, dtype=np.float32)\n",
        "\n",
        "    return example\n",
        "\n",
        "# Map the load_audio function\n",
        "dataset = dataset.map(load_audio)\n",
        "\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "2LZtDgmbAcbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[3])"
      ],
      "metadata": {
        "id": "16aqUqCpbKJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Tensorflow"
      ],
      "metadata": {
        "collapsed": true,
        "id": "pM1eBE45c-jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "yKcetObQdgk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio\n",
        "from transformers import Wav2Vec2Processor\n",
        "\n",
        "# Assuming you have initialized your processor, e.g.,\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    audio_paths = examples[\"audios\"]  # Assuming these are paths to the audio files.\n",
        "\n",
        "    # Load audio files using torchaudio and convert to numpy arrays\n",
        "    audio = [torchaudio.load(audio_path)[0].squeeze().numpy() for audio_path in audio_paths]\n",
        "\n",
        "    # Process audio input\n",
        "    inputs = processor(audio, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    # Process transcriptions, ensuring they are valid strings\n",
        "    transcriptions = [t for t in examples[\"transcriptions\"] if isinstance(t, str) and t]\n",
        "    labels = processor(transcriptions, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).input_ids\n",
        "\n",
        "    # Ensure padding length is identical if necessary\n",
        "    if inputs[\"input_ids\"].size(1) != labels.size(1):\n",
        "        # Truncate labels to match input length\n",
        "        labels = labels[:, :inputs[\"input_ids\"].size(1)]\n",
        "\n",
        "    inputs[\"labels\"] = labels\n",
        "    return inputs\n",
        "\n",
        "# Map the function to your dataset\n",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "jBlTW-y_t6Uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "import torch\n",
        "from datasets import Dataset, load_dataset\n",
        "from transformers import Wav2Vec2ForCTC, AutoProcessor, TrainingArguments, Trainer\n",
        "\n",
        "# Load transcriptions\n",
        "transcriptions_file = r'/content/drive/MyDrive/os-dataset/transcriptions.txt'\n",
        "transcriptions_df = pd.read_csv(transcriptions_file, delimiter='\\t', header=None, names=[\"full\"])\n",
        "transcriptions_df[\"filename\"] = transcriptions_df['full'].apply(lambda x: re.split(\" \", x)[0])\n",
        "transcriptions_df[\"transcriptions\"] = transcriptions_df['full'].apply(lambda x: ' '.join(re.split(\" \", x)[1:]))\n",
        "transcriptions_df = transcriptions_df.drop(columns=['full'])\n",
        "\n",
        "# Get the audio file paths\n",
        "audio_dir = r'/content/drive/MyDrive/os-dataset/os-audios'\n",
        "transcriptions_df['filepath'] = transcriptions_df['filename'].apply(lambda x: os.path.join(audio_dir, x))\n",
        "\n",
        "# Create dataset from DataFrame\n",
        "dataset = Dataset.from_pandas(transcriptions_df)\n",
        "\n",
        "# Define a function to load and process audio files\n",
        "def load_audio(example):\n",
        "    audio_input, sampling_rate = sf.read(example[\"filepath\"])\n",
        "    example[\"audio\"] = audio_input\n",
        "    example[\"sampling_rate\"] = sampling_rate\n",
        "    return example\n",
        "\n",
        "# Map the load_audio function\n",
        "dataset = dataset.map(load_audio)\n",
        "\n",
        "# Load processor and model\n",
        "model_name = \"facebook/mms-1b-all\"\n",
        "processor = AutoProcessor.from_pretrained(model_name)\n",
        "model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
        "\n",
        "# Preprocess dataset\n",
        "def preprocess_function(examples):\n",
        "    # Use the processor to apply necessary transformations\n",
        "    audio = examples['audio']\n",
        "    input_values = processor(audio, sampling_rate=16000).input_values\n",
        "    examples['input_values'] = input_values\n",
        "    examples['labels'] = processor.tokenizer(examples[\"transcriptions\"]).input_ids\n",
        "    return examples\n",
        "\n",
        "# Apply preprocessing\n",
        "processed_dataset = dataset.map(preprocess_function, remove_columns=[\"audio\", \"filepath\", \"transcriptions\"])\n",
        "\n",
        "# Split dataset into training and evaluation sets\n",
        "train_test_split = processed_dataset.train_test_split(test_size=0.2)  # 20% for evaluation\n",
        "\n",
        "# Prepare the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",          # output directory\n",
        "    per_device_train_batch_size=16,  # batch size\n",
        "    per_device_eval_batch_size=16,   # evaluation batch size\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    logging_dir=\"./logs\",            # directory for storing logs\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"epoch\",\n",
        ")\n",
        "\n",
        "# Initialize Trainer with train and eval datasets\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_test_split['train'],\n",
        "    eval_dataset=train_test_split['test'],\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "trainer.save_model(\"./fine-tuned-mms-1b-all\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f2DSTS3YvQpI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}