{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQhgJaP_taZp"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download ru_core_news_md"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4l6d9SdY_gZD"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "# import spacy\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHeBPJzQcVxM"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"ru_core_news_md\", disable=[\"ner\", \"attribute_ruler\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBmnwxW5-nWN"
      },
      "source": [
        "# Предобработка"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EBJvnJwUj8Em"
      },
      "source": [
        "## Отделение морфология нивхского"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Данный раздел посвящен обработки словарных данных – приведению слов к нивсхким основ. Результатом всех функций стал словарь основ, к которому можно обращаться в задаче классификации глосс."
      ],
      "metadata": {
        "id": "Z7ubrSuTQxKL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "CvOToAXuQ-2M"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqCNViwIfrYR"
      },
      "source": [
        "`{'nv_word': str, 'ru_word': list(str), 'nv_stem': str,\n",
        "'affixes': list(str), 'metadata': str}`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9F0jgvHVjxe"
      },
      "outputs": [],
      "source": [
        "!curl -L -o final_dictionary.csv 'https://docs.google.com/spreadsheets/d/1z2wStzMEO41N5qKeNlbkfexZyimQiu3aD7P6SYitVvU/export?exportFormat=csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UwMdOeO9DxT"
      },
      "outputs": [],
      "source": [
        "!curl -L -o final_glosses.csv 'https://docs.google.com/spreadsheets/d/19045IoPzWSiTvmZC3zQIS1vqRYpwazQPCKTEov2dCcU/export?exportFormat=csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UqIz3xM0VfmQ"
      },
      "outputs": [],
      "source": [
        "final_df = pd.read_csv('final_dictionary.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ag6BeSzK9Fqq"
      },
      "outputs": [],
      "source": [
        "final_glosses = pd.read_csv('final_glosses.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "180yVUlA8v80"
      },
      "source": [
        "### Таблички"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-T7ABGZvVK-"
      },
      "source": [
        "Пос-теги получены с помощью Spacy для русского"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6wIS79xsQWG"
      },
      "outputs": [],
      "source": [
        "final_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhyjsTns6wvv"
      },
      "outputs": [],
      "source": [
        "final_df.groupby(['pos_tag']).count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7nH-BZt81ri"
      },
      "outputs": [],
      "source": [
        "final_glosses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vav9p4H843I"
      },
      "source": [
        "## Функции\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbIv1uXjyqBO"
      },
      "outputs": [],
      "source": [
        "pattern = ''.join(list(final_glosses[final_glosses['Gloss'] == 'CONV.3.SG']['Morph'])[0].split(', '))\n",
        "# суффикс адвербиала: CAUS + CONV.3.SG\n",
        "adv_aff = re.compile(f'[гӷ]у[{pattern}]$')\n",
        "# суффикс плуралиса\n",
        "pl_aff = re.compile('([ӻгғкх][оу](ну?)?)$')\n",
        "# суффикс индикатива\n",
        "ind_aff = re.compile('(н?д|т|ӈ)ь?$')\n",
        "# суффикс каузатива\n",
        "caus_aff = re.compile('(ңг?|(к|ӄ|г|ӷ)у?)$')\n",
        "# суффикс атрибутив\n",
        "atr_aff = re.compile('л?а$')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y77z_LCixu_e"
      },
      "outputs": [],
      "source": [
        "def sub_aff(word, aff):\n",
        "    if re.search(aff, word):\n",
        "       return re.search(aff, word), re.sub(aff, '', word)\n",
        "    return False, word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egR3MOVwUadM"
      },
      "outputs": [],
      "source": [
        "def cosine(line, stem, stem_dictionary, a=0.238):\n",
        "    url_api = 'https://rusvectores.org/{MODEL}/{word_1}__{word_2}/api/similarity/'\n",
        "    model = 'ruwikiruscorpora_upos_cbow_300_10_2021'\n",
        "    expl = nlp(line.ru)\n",
        "    for w in expl:\n",
        "        word1 = w.lemma_\n",
        "        for x in stem_dictionary[stem]['ru']:\n",
        "            if len(x.split()) == 1:\n",
        "                word2 = x\n",
        "                # print(word1, word2)\n",
        "                url = url_api.format(MODEL=model, word_1=word1, word_2=word2, FORMAT='csv')\n",
        "                x = requests.get(url)\n",
        "                if x.status_code != 200:\n",
        "                    continue\n",
        "                if x.content.decode('utf8') == 'Unknown':\n",
        "                    continue\n",
        "                # print(x.content.decode('utf8').split('\\t'))\n",
        "                if float(x.content.decode('utf8').split('\\t')[0]) > a:\n",
        "                    return True\n",
        "    return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oc57FVjYdAIA"
      },
      "source": [
        "### Обработка глаголов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4GTdaSHvvSU"
      },
      "outputs": [],
      "source": [
        "def process_verb(line, stem_dictionary, pos):\n",
        "    dictionary = {'nv_word': '', 'ru_word': set(), 'nv_stem': '',\n",
        "                  'affixes': dict(), 'metadata': '', 'pos': None}\n",
        "\n",
        "    nv = line.nv.lower().replace('\\xad', '')\n",
        "    dictionary['nv_word'] = nv\n",
        "    dictionary['pos'] = line.pos_tag\n",
        "    traduction = line.ru\n",
        "\n",
        "    # отделение PL\n",
        "    pl, stem = sub_aff(nv, pl_aff)\n",
        "    # отделение IND\n",
        "    ind, stem = sub_aff(stem, ind_aff)\n",
        "    # отделение CAUS\n",
        "    caus = None\n",
        "    if 'заставить' in line.ru:\n",
        "        caus, stem = sub_aff(stem, caus_aff)\n",
        "        traduction = traduction.replace('заставить ', '')\n",
        "\n",
        "    dictionary['nv_stem'] = stem\n",
        "    if stem not in stem_dictionary:\n",
        "        stem_dictionary[stem] = {'ru': set(), 'idx': set(), 'pos': set()}\n",
        "    stem_dictionary[stem]['ru'].update(set([x for x in traduction.split(';')]))\n",
        "    stem_dictionary[stem]['idx'].add(pos)\n",
        "    stem_dictionary[stem]['pos'].add(line.pos_tag)\n",
        "    dictionary['ru_word'].update(set([x for x in traduction.split(';')]))\n",
        "\n",
        "    if pl:\n",
        "        dictionary['affixes']['PL'] = pl.group()\n",
        "    if ind:\n",
        "        dictionary['affixes']['IND'] = ind.group()\n",
        "    if caus:\n",
        "        dictionary['affixes']['CAUS'] = caus.group()\n",
        "\n",
        "    dictionary['metadata'] = None if isinstance(line.metadata, float) else line.metadata\n",
        "    return stem_dictionary, dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RvXP72PbWjFP"
      },
      "outputs": [],
      "source": [
        "final = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4IxZKAhQU1X"
      },
      "outputs": [],
      "source": [
        "stem_dictionary = dict()\n",
        "verb_final = []\n",
        "for pos, line in enumerate(final_df[final_df['pos_tag'] == 'VERB'].itertuples()):\n",
        "      stem_dictionary, dictionary = process_verb(line, stem_dictionary, pos)\n",
        "      verb_final.append(dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocrFtWXcRLu5"
      },
      "outputs": [],
      "source": [
        "len(verb_final)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUPD6dA1dEWo"
      },
      "source": [
        "### Обработка прилагательных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xk7hsz_4Tnwx"
      },
      "outputs": [],
      "source": [
        "def check_definition(line, stem_dictionary, stem):\n",
        "    for expression in stem_dictionary[stem]['ru']:\n",
        "        if expression.startswith('быть'):\n",
        "            doc = nlp(expression.split()[1])\n",
        "            if doc[0].lemma_ == line.ru:\n",
        "                return True\n",
        "    else:\n",
        "        doc = nlp(expression)\n",
        "        for word in doc:\n",
        "            check = cosine(line, stem, stem_dictionary, a=0.3)\n",
        "    return check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTlOhobeR-v6"
      },
      "outputs": [],
      "source": [
        "def process_adj(line, stem_dictionary, pos):\n",
        "    dictionary = {'nv_word': '', 'ru_word': set(), 'nv_stem': '',\n",
        "                  'affixes': dict(), 'metadata': '', 'pos': None}\n",
        "\n",
        "    nv = line.nv.lower().replace('\\xad', '')\n",
        "    dictionary['nv_word'] = nv\n",
        "    dictionary['pos'] = line.pos_tag\n",
        "    traduction = line.ru\n",
        "    dictionary['ru_word'].update(set([x for x in traduction.split(';')]))\n",
        "\n",
        "    # отделение PL\n",
        "    pl, stem = sub_aff(nv, pl_aff)\n",
        "\n",
        "    atr_affix = None\n",
        "    # поиск ATR и отделение\n",
        "    if re.search('ла$', stem):\n",
        "        if stem[:-2] in stem_dictionary and len(stem[:-2]) > 1:\n",
        "            stem = re.sub('ла$', '', stem)\n",
        "            atr_affix = 'ла'\n",
        "\n",
        "        elif stem[:-1] in stem_dictionary:\n",
        "            stem = re.sub('а$', '', stem)\n",
        "            atr_affix = 'а'\n",
        "\n",
        "    elif re.search('а$', stem):\n",
        "        if stem[:-1] in stem_dictionary:\n",
        "            stem = re.sub('а$', '', stem)\n",
        "            atr_affix = 'а'\n",
        "\n",
        "    dictionary['nv_stem'] = stem\n",
        "\n",
        "    verb_in_dict = False\n",
        "    if stem in stem_dictionary:\n",
        "        verb_in_dict = check_definition(line, stem_dictionary, stem)\n",
        "    else:\n",
        "        stem_dictionary[stem] = {'ru': set(), 'idx': set(), 'pos': set()}\n",
        "    stem_dictionary[stem]['idx'].add(pos)\n",
        "    stem_dictionary[stem]['pos'].add(line.pos_tag)\n",
        "    if not verb_in_dict:\n",
        "        stem_dictionary[stem]['ru'].update(set([x for x in traduction.split(';')]))\n",
        "\n",
        "    if pl:\n",
        "        dictionary['affixes']['PL'] = pl.group()\n",
        "    if atr_affix:\n",
        "        dictionary['affixes']['PL'] = atr_affix\n",
        "\n",
        "    dictionary['metadata'] = None if isinstance(line.metadata, float) else line.metadata\n",
        "\n",
        "    return stem_dictionary, dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TCm5XyoV89m"
      },
      "outputs": [],
      "source": [
        "adj_final = []\n",
        "for pos, line in enumerate(final_df[final_df['pos_tag'] == 'ADJ'].itertuples(), len(verb_final)):\n",
        "      stem_dictionary, dictionary = process_adj(line, stem_dictionary, pos)\n",
        "      adj_final.append(dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAdwRHxucpel"
      },
      "outputs": [],
      "source": [
        "len(adj_final)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPQZFpURdObf"
      },
      "source": [
        "### Обработка существительных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKEDf4W8ZM-t"
      },
      "outputs": [],
      "source": [
        "pers_aff = re.compile('(нивх|нивӈ|ниғвӈ)$')\n",
        "woman_aff = re.compile('(умгу|р̌аӈ[ӄӻӷ])$')\n",
        "cub_aff = re.compile('нонӄ$')\n",
        "kid_aff = re.compile('(о[ӻғ]?ла|эӻлӈ)$')\n",
        "female = re.compile('аньӽ$')\n",
        "month = re.compile('лоӈ$')\n",
        "earth = re.compile('миф$')\n",
        "animal = re.compile('ӈа$')\n",
        "pattern = '|'.join(list(final_glosses[final_glosses['Gloss'] == 'PRON:ANY']['Morph'])[0].split(', '))\n",
        "pron_aff = re.compile(f'({pattern})$')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BOZnF5AdFus"
      },
      "outputs": [],
      "source": [
        "list_of_patterns = {pers_aff: \"человек\", woman_aff: \"женщина\", cub_aff: \"детёныш\",\n",
        "                    kid_aff: \"ребёнок\", female: \"самка\",\n",
        "                    month: \"месяц\", earth: \"земля\", animal: \"зверь\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQUPCpx3Y-Dm"
      },
      "outputs": [],
      "source": [
        "def find_complex_word(line):\n",
        "    for patt in list_of_patterns:\n",
        "        res = re.search(patt, line)\n",
        "        if res:\n",
        "            if res.span()[0] > 0:\n",
        "                return True, patt\n",
        "    return False, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87lXlknIltyI"
      },
      "outputs": [],
      "source": [
        "def form_dictionary(line, stem, affixes, dictionary):\n",
        "    for aff, gloss in affixes.items():\n",
        "        dictionary['affixes'][aff] = gloss\n",
        "    dictionary['nv_stem'] = stem\n",
        "    dictionary['metadata'] = None if isinstance(line.metadata, float) else line.metadata\n",
        "    return dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbm7Rfivd0ww"
      },
      "outputs": [],
      "source": [
        "def locative(line, stem_dictionary, dictionary, pos):\n",
        "    nv = line.nv.lower().replace('\\xad', '')\n",
        "    if re.search('(?:место.+|.*место)', line.ru) or 'место' in str(line.metadata) or 'участок' in str(line.metadata):\n",
        "        if nv[:-1] in stem_dictionary and 'VERB' in stem_dictionary[nv[:-1]]['pos']:\n",
        "            dictionary = form_dictionary(line, nv[:-1], {'NMN:L': 'ф'}, dictionary)\n",
        "            stem_dictionary[nv[:-1]]['ru'].update(set([x for x in line.ru.split(';')]))\n",
        "            stem_dictionary[nv[:-1]]['idx'].add(pos)\n",
        "            stem_dictionary[nv[:-1]]['pos'].add(line.pos_tag)\n",
        "            return True, stem_dictionary, dictionary\n",
        "\n",
        "        # аттрибутив + локатив?\n",
        "        atr, stem = sub_aff(nv[:-1], atr_aff)\n",
        "        if atr:\n",
        "            dictionary = form_dictionary(line, stem, {'NMN:L': 'ф'}, dictionary)\n",
        "            if stem not in stem_dictionary:\n",
        "                stem_dictionary[stem] =  {'ru': set(), 'idx': set(), 'pos': set()}\n",
        "            else:\n",
        "                dictionary['affixes']['ATR'] = atr.group()\n",
        "            stem_dictionary[stem]['ru'].update(set([x for x in line.ru.split(';')]))\n",
        "            stem_dictionary[stem]['idx'].add(pos)\n",
        "            stem_dictionary[stem]['pos'].add(line.pos_tag)\n",
        "            return True, stem_dictionary, dictionary\n",
        "    return False, stem_dictionary, dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Zj7NDLadaD4"
      },
      "outputs": [],
      "source": [
        "def process_noun(line, stem_dictionary, pos):\n",
        "    dictionary = {'nv_word': '', 'ru_word': set(), 'nv_stem': '',\n",
        "                  'affixes': dict(), 'metadata': '', 'pos': None}\n",
        "\n",
        "    nv = line.nv.lower().replace('\\xad', '')\n",
        "    dictionary['nv_word'] = nv\n",
        "    dictionary['pos'] = line.pos_tag\n",
        "    traduction = line.ru\n",
        "    dictionary['ru_word'].update(set([x for x in traduction.split(';')]))\n",
        "\n",
        "    # поиск локативных номмнализаций\n",
        "    if nv[-1] == 'ф':\n",
        "        check, stem_dictionary, dictionary = locative(line, stem_dictionary, dictionary, pos)\n",
        "        if check:\n",
        "            return stem_dictionary, dictionary\n",
        "\n",
        "    # поиск актантной номинализации\n",
        "    ind = list(final_glosses[final_glosses['Gloss'] == 'NMN:A']['Morph'])[0].split(', ')\n",
        "    pattern = re.compile(f\"({'|'.join(ind)})$\")\n",
        "\n",
        "    if nv.endswith(tuple(ind)):\n",
        "        new_word = re.sub(pattern, '', nv)\n",
        "        if new_word in stem_dictionary and 'VERB' in stem_dictionary[new_word]['pos']:\n",
        "            if cosine(line, new_word, stem_dictionary):\n",
        "                dictionary = form_dictionary(line, new_word, {'NMN:A': re.search(pattern, nv).group()}, dictionary)\n",
        "                stem_dictionary[new_word]['idx'].add(pos)\n",
        "                return stem_dictionary, dictionary\n",
        "        else:\n",
        "          #  атрибутив + номинализация\n",
        "            if re.search('ла$', new_word):\n",
        "                if new_word[:-2] in stem_dictionary and len(new_word[:-2]) > 1 and 'VERB' in stem_dictionary[new_word[:-2]]['pos']:\n",
        "                    new_word = re.sub('ла$', '', new_word)\n",
        "                    atr_affix = 'ла'\n",
        "                    if cosine(line, new_word, stem_dictionary):\n",
        "                        dictionary = form_dictionary(line, new_word,\n",
        "                        {'NMN:A': re.search(pattern, nv).group(),\n",
        "                          'ATR': atr_affix}, dictionary)\n",
        "                        stem_dictionary[new_word]['idx'].add(pos)\n",
        "\n",
        "                elif new_word[:-1] in stem_dictionary and 'VERB' in stem_dictionary[new_word[:-1]]['pos']:\n",
        "                    new_word = re.sub('а$', '', new_word)\n",
        "                    atr_affix = 'а'\n",
        "                    if cosine(line, new_word, stem_dictionary):\n",
        "                        dictionary = form_dictionary(line, new_word,\n",
        "                        {'NMN:A': re.search(pattern, nv).group(),\n",
        "                          'ATR': atr_affix}, dictionary)\n",
        "                        stem_dictionary[new_word]['idx'].add(pos)\n",
        "\n",
        "\n",
        "            elif re.search('а$', new_word):\n",
        "                if new_word[:-1] in stem_dictionary and 'VERB' in stem_dictionary[new_word[:-1]]['pos']:\n",
        "                    new_word = re.sub('а$', '', new_word)\n",
        "                    atr_affix = 'а'\n",
        "                    if cosine(line, new_word, stem_dictionary):\n",
        "                        dictionary = form_dictionary(line, new_word,\n",
        "                        {'NMN:A': re.search(pattern, nv).group(),\n",
        "                          'ATR': atr_affix}, dictionary)\n",
        "                        stem_dictionary[new_word]['idx'].add(pos)\n",
        "                        return stem_dictionary, dictionary\n",
        "\n",
        "    ind_aff = re.compile('[дт]ь?$')\n",
        "    ind, stem = sub_aff(nv, ind_aff)\n",
        "    if ind and stem in stem_dictionary and 'VERB' in stem_dictionary[stem]['pos']:\n",
        "        if cosine(line, stem, stem_dictionary, a=0.4):\n",
        "            dictionary = form_dictionary(line, stem,\n",
        "                        {'NMN:P': ind.group()}, dictionary)\n",
        "            stem_dictionary[stem]['idx'].add(pos)\n",
        "            return stem_dictionary, dictionary\n",
        "\n",
        "    if nv not in stem_dictionary:\n",
        "        dictionary = form_dictionary(line, nv, {}, dictionary)\n",
        "        stem_dictionary[nv] =  {'ru': set(), 'idx': set(), 'pos': set()}\n",
        "        stem_dictionary[nv]['ru'].update(set([x for x in line.ru.split(';')]))\n",
        "        stem_dictionary[nv]['idx'].add(pos)\n",
        "        stem_dictionary[nv]['pos'].add(line.pos_tag)\n",
        "    return stem_dictionary, dictionary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASzHSf0PMxbB"
      },
      "outputs": [],
      "source": [
        "mapping = {'нонӄ': 'детёныш', 'нивх': 'человек',\n",
        "           'нивӈ': 'человек', 'ниғвӈ': 'человек',\n",
        "           'умгу': 'женщина', 'р̌аӈӄ': 'женщина',\n",
        "           'р̌аӈӻ': 'женщина', 'р̌аӈӷ': 'женщина',\n",
        "           'оӻла': 'ребёнок', 'эӻлӈ': 'ребёнок',\n",
        "           'оғла': 'ребёнок', 'ола': 'ребёнок',\n",
        "           'аньӽ': 'самка', 'лоӈ': \"месяц\",\n",
        "           'миф': 'земля', 'ӈа': 'зверь'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ON2r9OJHipn2"
      },
      "outputs": [],
      "source": [
        "def fix_complex_words(line, stem_dictionary, pos):\n",
        "    dictionary = {'nv_word': '', 'ru_word': set(), 'nv_stem': '',\n",
        "                  'affixes': dict(), 'metadata': '', 'pos': None}\n",
        "\n",
        "    nv = line.nv.lower().replace('\\xad', '')\n",
        "    dictionary['nv_word'] = nv\n",
        "    traduction = line.ru\n",
        "    dictionary['ru_word'].update(set([x for x in traduction.split(';')]))\n",
        "    dictionary['pos'] = line.pos_tag\n",
        "\n",
        "    prefix = nv\n",
        "    new_stem = []\n",
        "    affixes = dict()\n",
        "    while True:\n",
        "        check, pattern = find_complex_word(prefix)\n",
        "        if not check:\n",
        "            break\n",
        "        suffix, prefix = sub_aff(prefix, pattern)\n",
        "        if suffix:\n",
        "            affixes[suffix.group()] = mapping[suffix.group()]\n",
        "            new_stem.append(suffix.group())\n",
        "    if prefix in stem_dictionary:\n",
        "        affixes[prefix] = stem_dictionary[prefix]['ru']\n",
        "    else:\n",
        "        affixes[prefix] = '?'\n",
        "    new_stem.append(prefix)\n",
        "    new_stem = '-'.join(new_stem[::-1])\n",
        "    dictionary = form_dictionary(line, new_stem, affixes, dictionary)\n",
        "    stem_dictionary[new_stem] =  {'ru': set(), 'idx': set(), 'pos': set()}\n",
        "    stem_dictionary[new_stem]['ru'].update(set([x for x in line.ru.split(';')]))\n",
        "    stem_dictionary[new_stem]['idx'].add(pos)\n",
        "    stem_dictionary[new_stem]['pos'].add(line.pos_tag)\n",
        "    return stem_dictionary, dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6_G0C2tHms3"
      },
      "outputs": [],
      "source": [
        "def fix_morphology(line, stem_dictionary, pos):\n",
        "    dictionary = {'nv_word': '', 'ru_word': set(), 'nv_stem': '',\n",
        "                  'affixes': dict(), 'metadata': '', 'pos': None}\n",
        "\n",
        "    nv = line.nv.lower().replace('\\xad', '')\n",
        "    dictionary['nv_word'] = nv\n",
        "    traduction = line.ru\n",
        "    dictionary['ru_word'].update(set([x for x in traduction.split(';')]))\n",
        "    dictionary['pos'] = line.pos_tag\n",
        "\n",
        "    stem = nv\n",
        "    pl, prefix = sub_aff(nv, pl_aff)\n",
        "    affixes = {}\n",
        "    if pl and prefix in stem_dictionary:\n",
        "        if cosine(line, prefix, stem_dictionary, a=0.2):\n",
        "            affixes = {'PL': pl.group()}\n",
        "            stem = prefix\n",
        "    elif pl:\n",
        "        nmn_a = re.search('[кӈ]$', prefix)\n",
        "        if nmn_a and prefix[:nmn_a.span()[0]] in stem_dictionary:\n",
        "            new_word = prefix[:nmn_a.span()[0]]\n",
        "            if cosine(line, new_word, stem_dictionary, a=0.2):\n",
        "                affixes = {'PL': pl.group(), 'NMN:A': nmn_a.group()}\n",
        "                prefix = new_word\n",
        "                stem = prefix\n",
        "        else:\n",
        "            check, pattern = find_complex_word(prefix)\n",
        "            if check:\n",
        "                word = re.search(pattern, prefix)\n",
        "                real_prefix = prefix[:word.span()[0]]\n",
        "                new_stem = real_prefix + '-' + word.group()\n",
        "                if new_stem in stem_dictionary:\n",
        "                    stem = new_stem\n",
        "                    affixes[word.group()] = mapping[word.group()]\n",
        "                    affixes['PL'] = pl.group()\n",
        "\n",
        "                elif real_prefix in stem_dictionary:\n",
        "                    stem = new_stem\n",
        "                    affixes[word.group()] = mapping[word.group()]\n",
        "                    affixes['PL'] = pl.group()\n",
        "\n",
        "    dictionary = form_dictionary(line, stem, affixes, dictionary)\n",
        "\n",
        "    if stem in stem_dictionary:\n",
        "        stem_dictionary[stem]['idx'].add(pos)\n",
        "    else:\n",
        "        stem_dictionary[stem] = {'ru': set(), 'idx': set(), 'pos': set()}\n",
        "        stem_dictionary[stem]['ru'].update(set([x for x in line.ru.split(';')]))\n",
        "        stem_dictionary[stem]['idx'].add(pos)\n",
        "        stem_dictionary[stem]['pos'].add(line.pos_tag)\n",
        "\n",
        "    return stem_dictionary, dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KfXghdFXOj5F"
      },
      "outputs": [],
      "source": [
        "def fix_syllables(line, pos):\n",
        "\n",
        "    dictionary = {'nv_word': '', 'ru_word': set(), 'nv_stem': '',\n",
        "                  'affixes': dict(), 'metadata': '', 'pos': None}\n",
        "\n",
        "    nv = line.nv.lower().replace('\\xad', '')\n",
        "    dictionary['nv_word'] = nv\n",
        "    traduction = line.ru\n",
        "    dictionary['ru_word'].update(set([x for x in traduction.split(';')]))\n",
        "    dictionary['nv_stem'] = nv\n",
        "    dictionary['metadata'] = None if isinstance(line.metadata, float) else line.metadata\n",
        "    dictionary['pos'] = line.pos_tag\n",
        "\n",
        "    return dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "1mytdXMHpfPv"
      },
      "outputs": [],
      "source": [
        "noun_final = []\n",
        "complex_words = []\n",
        "two_sillables = []\n",
        "morphology_words = []\n",
        "for pos, line in enumerate(final_df[final_df['pos_tag'] == 'NOUN'].itertuples(), len(adj_final)):\n",
        "    if len(line.nv.split()) > 1:\n",
        "        # two_sillables.append((pos, line))\n",
        "        dictionary = fix_syllables(line, pos)\n",
        "        noun_final.append(dictionary)\n",
        "        continue\n",
        "\n",
        "    if find_complex_word(line.nv)[0]:\n",
        "        # complex_words.append(((pos, line)))\n",
        "        stem_dictionary, dictionary = fix_complex_words(line, stem_dictionary, pos)\n",
        "        noun_final.append(dictionary)\n",
        "        continue\n",
        "    if re.search(pl_aff, line.nv):\n",
        "        # morphology_words.append(line)\n",
        "        stem_dictionary, dictionary = fix_morphology(line, stem_dictionary, pos)\n",
        "        noun_final.append(dictionary)\n",
        "        continue\n",
        "\n",
        "    if re.search('(?:[кг][ўу][р̌рт]|[тд]о[хӽ]|ух)', line.nv):\n",
        "        # morphology_words.append(line)\n",
        "        stem_dictionary, dictionary = fix_morphology(line, stem_dictionary, pos)\n",
        "        noun_final.append(dictionary)\n",
        "        continue\n",
        "\n",
        "    if re.search(pron_aff, line.nv):\n",
        "        # morphology_words.append(line)\n",
        "        stem_dictionary, dictionary = fix_morphology(line, stem_dictionary, pos)\n",
        "        noun_final.append(dictionary)\n",
        "        continue\n",
        "\n",
        "    stem_dictionary, dictionary = process_noun(line, stem_dictionary, pos)\n",
        "    noun_final.append(dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNi-UeJXJbSN"
      },
      "outputs": [],
      "source": [
        "len(noun_final)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiIAs2GEeHhC"
      },
      "source": [
        "### Прочее"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CKEodH4uePNw"
      },
      "outputs": [],
      "source": [
        "def process_pron(line, stem_dictionary, pos):\n",
        "    dictionary = {'nv_word': '', 'ru_word': set(), 'nv_stem': '',\n",
        "                  'affixes': dict(), 'metadata': '', 'pos': None}\n",
        "\n",
        "    nv = line.nv.lower().replace('\\xad', '')\n",
        "    dictionary['nv_word'] = nv\n",
        "    dictionary['pos'] = line.pos_tag\n",
        "    traduction = line.ru\n",
        "    dictionary['ru_word'].update(set([x for x in traduction.split(';')]))\n",
        "\n",
        "    if 'лу' in nv:\n",
        "        stem = re.sub('лу', '', nv)\n",
        "        stem = re.sub(' ', '', stem)\n",
        "        dictionary['affixes']['INDEF'] =' лу'\n",
        "    else:\n",
        "        stem = nv\n",
        "    if stem not in stem_dictionary:\n",
        "        stem_dictionary[stem] = {'ru': set(), 'idx': set(), 'pos': set()}\n",
        "\n",
        "    stem_dictionary[stem]['ru'].update(set([x for x in traduction.split(';')]))\n",
        "    stem_dictionary[stem]['idx'].add(pos)\n",
        "    stem_dictionary[stem]['pos'].add(line.pos_tag)\n",
        "\n",
        "    dictionary['nv_stem'] = stem\n",
        "    dictionary['metadata'] = None if isinstance(line.metadata, float) else line.metadata\n",
        "\n",
        "    return stem_dictionary, dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oA2h-7CdeREv"
      },
      "outputs": [],
      "source": [
        "pron_final = []\n",
        "for pos, line in enumerate(final_df[final_df['pos_tag'] == 'PRON'].itertuples(), len(noun_final)):\n",
        "    stem_dictionary, dictionary = process_pron(line, stem_dictionary, pos)\n",
        "    pron_final.append(dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b28MUusvc-Py"
      },
      "outputs": [],
      "source": [
        "sconj_final = []\n",
        "for pos, line in enumerate(final_df[final_df['pos_tag'] == 'SCONJ'].itertuples(), len(pron_final)):\n",
        "    stem_dictionary, dictionary = process_pron(line, stem_dictionary, pos)\n",
        "    sconj_final.append(dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8vLIc18nRdx"
      },
      "outputs": [],
      "source": [
        "def adv(line, stem_dictionary, pos):\n",
        "    dictionary = {'nv_word': '', 'ru_word': set(), 'nv_stem': '',\n",
        "                  'affixes': dict(), 'metadata': '', 'pos': None}\n",
        "\n",
        "    nv = line.nv.lower().replace('\\xad', '')\n",
        "    dictionary['nv_word'] = nv\n",
        "    dictionary['pos'] = line.pos_tag\n",
        "    traduction = line.ru\n",
        "    dictionary['ru_word'].update(set([x for x in traduction.split(';')]))\n",
        "\n",
        "    affixes = {}\n",
        "    stem = nv\n",
        "\n",
        "    if re.search('л[уо]$', stem):\n",
        "        patt = re.search('л[уо]$', stem).group()\n",
        "        stem = re.sub(patt, '', nv)\n",
        "        stem = re.sub(' ', '', stem)\n",
        "        affixes['INDEF'] = patt\n",
        "\n",
        "    if re.search('[гк]у[р̌рт]$', stem):\n",
        "        new_word = re.sub('гур̌', '', stem)\n",
        "        if new_word in stem_dictionary:\n",
        "            affixes['CAUS'] = re.search('[гк]у(?=[р̌рт]$)', stem)\n",
        "            affixes['CONV.3.SG'] = re.search('[р̌рт]$', stem).group()\n",
        "            stem = new_word\n",
        "\n",
        "    if re.search('[тд]оӽ$', stem):\n",
        "        new_word = re.sub('доӽ', '', stem)\n",
        "        if new_word in stem_dictionary:\n",
        "            affixes['DAT'] = re.search('[тд]оӽ$', stem).group()\n",
        "            stem = new_word\n",
        "\n",
        "    if re.search('ра$', stem):\n",
        "        new_word = re.sub('ра', '', stem)\n",
        "        if new_word in stem_dictionary:\n",
        "            affixes['EMPH.3.SG'] ='ра'\n",
        "            stem = new_word\n",
        "\n",
        "    if re.search('ух$', stem):\n",
        "        new_word = re.sub('ух', '', stem)\n",
        "        if new_word in stem_dictionary:\n",
        "            affixes['ABL'] ='ух'\n",
        "            stem = new_word\n",
        "\n",
        "    if re.search('ӿагин$', stem):\n",
        "        new_word = re.sub('ӿагин', '', stem)\n",
        "        if new_word in stem_dictionary:\n",
        "            affixes['PRON:ANY'] ='ӿагин'\n",
        "            stem = new_word\n",
        "\n",
        "    dictionary = form_dictionary(line, stem, affixes, dictionary)\n",
        "    if stem not in stem_dictionary:\n",
        "        stem_dictionary[stem] = {'ru': set(), 'idx': set(), 'pos': set()}\n",
        "\n",
        "    stem_dictionary[stem]['ru'].update(set([x for x in traduction.split(';')]))\n",
        "    stem_dictionary[stem]['idx'].add(pos)\n",
        "    stem_dictionary[stem]['pos'].add(line.pos_tag)\n",
        "\n",
        "    return stem_dictionary, dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOVKTEQ3l39q"
      },
      "outputs": [],
      "source": [
        "adv_final = []\n",
        "for pos, line in enumerate(final_df[final_df['pos_tag'] == 'ADV'].itertuples(), len(sconj_final)):\n",
        "    stem_dictionary, dictionary = adv(line, stem_dictionary, pos)\n",
        "    adv_final.append(dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4-d2m5pn_yl"
      },
      "outputs": [],
      "source": [
        "adp_final = []\n",
        "for pos, line in enumerate(final_df[final_df['pos_tag'] == 'ADP'].itertuples(), len(adv_final)):\n",
        "    stem_dictionary, dictionary = adv(line, stem_dictionary, pos)\n",
        "    adp_final.append(dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hc6gU9xHoRZa"
      },
      "outputs": [],
      "source": [
        "cconj_final = []\n",
        "for pos, line in enumerate(final_df[final_df['pos_tag'] == 'CCONJ'].itertuples(), len(adp_final)):\n",
        "    stem_dictionary, dictionary = adv(line, stem_dictionary, pos)\n",
        "    cconj_final.append(dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9BI-uoWoc_G"
      },
      "outputs": [],
      "source": [
        "det_final = []\n",
        "for pos, line in enumerate(final_df[final_df['pos_tag'] == 'DET'].itertuples(), len(cconj_final)):\n",
        "    stem_dictionary, dictionary = adv(line, stem_dictionary, pos)\n",
        "    det_final.append(dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzJSirkIomHf"
      },
      "outputs": [],
      "source": [
        "intj_final = []\n",
        "for pos, line in enumerate(final_df[final_df['pos_tag'] == 'INTJ'].itertuples(), len(det_final)):\n",
        "    stem_dictionary, dictionary = adv(line, stem_dictionary, pos)\n",
        "    intj_final.append(dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1DpIA-Qozu0"
      },
      "outputs": [],
      "source": [
        "num_final = []\n",
        "for pos, line in enumerate(final_df[final_df['pos_tag'] == 'NUM'].itertuples(), len(intj_final)):\n",
        "    stem_dictionary, dictionary = adv(line, stem_dictionary, pos)\n",
        "    num_final.append(dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UnitWHPo3zN"
      },
      "outputs": [],
      "source": [
        "part_final = []\n",
        "for pos, line in enumerate(final_df[final_df['pos_tag'] == 'PART'].itertuples(), len(num_final)):\n",
        "    stem_dictionary, dictionary = adv(line, stem_dictionary, pos)\n",
        "    part_final.append(dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJncAolmo8ty"
      },
      "outputs": [],
      "source": [
        "propn_final = []\n",
        "for pos, line in enumerate(final_df[final_df['pos_tag'] == 'PROPN'].itertuples(), len(part_final)):\n",
        "    stem_dictionary, dictionary = adv(line, stem_dictionary, pos)\n",
        "    propn_final.append(dictionary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULPed-nLpCh6"
      },
      "outputs": [],
      "source": [
        "final = verb_final + adj_final + noun_final + pron_final + sconj_final + adv_final + adp_final + cconj_final + det_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaIpcCdPpXxa"
      },
      "outputs": [],
      "source": [
        "final = final + intj_final + num_final + part_final + propn_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFI1JREHpd5l"
      },
      "outputs": [],
      "source": [
        "len(final)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfkkUMVKp45b"
      },
      "outputs": [],
      "source": [
        "final[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kb2Fsw9wptPo"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def set_default(obj):\n",
        "    if isinstance(obj, set):\n",
        "        return list(obj)\n",
        "    raise TypeError\n",
        "\n",
        "with open('data.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(final, f, ensure_ascii=False, indent=6, default=set_default)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3xQjkYyqZtZ"
      },
      "outputs": [],
      "source": [
        "with open('stem.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(stem_dictionary, f, ensure_ascii=False, indent=3, default=set_default)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QxwRYteHEgD"
      },
      "source": [
        "# Данные"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQR7xtevIZA0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZVFBLxDH5xK"
      },
      "outputs": [],
      "source": [
        "!pip --quiet install gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rfvvt-L2IQSW"
      },
      "outputs": [],
      "source": [
        "!gdown --folder https://drive.google.com/drive/folders/1YBLW10W3q3-wa5Mx4CA6-P4Qvs5ish9d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fPUK09NgIqly"
      },
      "outputs": [],
      "source": [
        "path = '/content/датасет'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6tSk0htvIaC8"
      },
      "outputs": [],
      "source": [
        "texts = ['/'.join([path, x]) for x in os.listdir(path)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TRZXUE4tIzHY"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "for text in texts:\n",
        "    with open(text, 'r', encoding='utf8') as file:\n",
        "      # f = file.readlines()\n",
        "      # data.extend(f)\n",
        "      new_dict = {'segmented': '', 'glossed': '', 'translation': None, 'metadata': None}\n",
        "      for string in file:\n",
        "          if re.findall('\\d+(\\_\\d+)*=', string):\n",
        "              new_dict['translation'] = re.search('(?<=\\d=[\\t ]).*', string).group()\n",
        "              data.append(new_dict)\n",
        "              new_dict = {'segmented': '', 'glossed': '', 'translation': None, 'metadata': None}\n",
        "              continue\n",
        "          if re.findall('\\d+\\>', string):\n",
        "              substring = re.search('(?<=\\>[ \\t]).*', string).group()\n",
        "              substring = re.sub('\\t+', '\\t', substring)\n",
        "              new_dict['segmented'] += '\\t'.join([substring])\n",
        "              continue\n",
        "          if re.findall('\\d+\\<', string):\n",
        "              substring = re.search('(?<=\\<[ \\t]).*', string).group()\n",
        "              substring = re.sub('\\t+', '\\t', substring)\n",
        "              new_dict['glossed'] += '\\t'.join([substring])\n",
        "          if re.findall('#', string):\n",
        "              string = re.sub('(?<=#) *', '', string)\n",
        "              if new_dict['metadata'] is None:\n",
        "                  new_dict['metadata'] = '\\n'.join([re.search('(?<=#).*', string).group()])\n",
        "              else:\n",
        "                  new_dict['metadata'] += '\\n' + '\\n'.join([re.search('(?<=#).*', string).group()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N84_Ntg3MtEF"
      },
      "outputs": [],
      "source": [
        "data[125]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Few-shot: морфемная сегментация"
      ],
      "metadata": {
        "id": "mtGXQGz1Fnh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для задачи морфемной сегментации было проведено разделение выборки на пословную выборку.\n",
        "\n",
        "Для обеспечения минимального пересечение между тренировочной и тестовой выборки была написана функция с гиперпараметром. Эмпирически был подобран минимальное возможное пересечение – 25%.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "P2UiJjMgRGzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -L -o final_glosses.csv 'https://docs.google.com/spreadsheets/d/19045IoPzWSiTvmZC3zQIS1vqRYpwazQPCKTEov2dCcU/export?exportFormat=csv'"
      ],
      "metadata": {
        "id": "xVob9PyedEU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_glosses = pd.read_csv('final_glosses.csv').drop(['Category'], axis=1)"
      ],
      "metadata": {
        "id": "ZWwb7y2T4Jk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_glosses"
      ],
      "metadata": {
        "id": "FCS_viYsdX6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "morph_gloss = {}\n",
        "for key in final_glosses.itertuples():\n",
        "    morphemes = key.Morph.split(', ')\n",
        "    for morph in morphemes:\n",
        "        morph_gloss[morph] = key.Gloss"
      ],
      "metadata": {
        "id": "wcSE8v1hdREP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -L -o data.json \"https://drive.google.com/uc?export=download&id=1lqJPVhTz1F_hfCPj65rfu1s7S52rvwat\""
      ],
      "metadata": {
        "id": "9OVBZ9ZsqUcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "f = open('/content/data.json', 'r', encoding='utf8')\n",
        "data = json.load(f)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "Bfme91HbqYic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data)"
      ],
      "metadata": {
        "id": "efWNgMXMqgPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# разбиение выборки на слова для few-shot сегментации\n",
        "\n",
        "all_words = []\n",
        "all_labels = []\n",
        "for sent in data:\n",
        "    for word in sent['segmented'].split('\\t'):\n",
        "        word = re.sub('[\\\"\\«\\»,\\.\\(\\)\\?\\!\\[А-Я:\\]]+', '', word.lower())\n",
        "        orig = word.replace('-', '')\n",
        "        if re.findall('(?<![ˇ’ʻ‘ʼ\\'р̌’‘ӻӿӃӾЧА-яёЁӽӈғӄӷ])[ӷр̌ӻӿа-яёӽӈғӄ](?![р̌’ʻ‘\\'ʼӻғӿА-яЁёӽӈа-яˇӄӷ])', orig):\n",
        "            continue\n",
        "        if re.findall('\"[ˇ’ʻ‘ʼ\\'р̌’‘ӻӿӃӾЧА-яёЁӽӈғӄӷ]{2}\"', orig):\n",
        "            continue\n",
        "        orig = re.sub('[\\\"\\«\\»,\\.\\(\\)\\?\\!\\[А-Я:\\]]+', '', orig.lower())\n",
        "        if orig != '':\n",
        "            all_words.append(orig)\n",
        "            all_labels.append(word)"
      ],
      "metadata": {
        "id": "3gKrLjCZuQv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [{key: value} for key, value in zip(all_words, all_labels)]"
      ],
      "metadata": {
        "id": "kBiwAx2x4dyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "id": "L-5GyirfEaWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def split_with_overlap_limit(dataset, train_ratio=0.5, max_overlap_ratio=0.2, max_attempts=100):\n",
        "    \"\"\"\n",
        "    Разделяет данные на обучающую и тестовую выборки с ограничением на пересечение.\n",
        "    \"\"\"\n",
        "    for attempt in range(max_attempts):\n",
        "        random.shuffle(dataset)\n",
        "        # Случайное разделение\n",
        "        random.shuffle(dataset)\n",
        "        train_size = int(len(all_words) * train_ratio)\n",
        "        train = dataset[:train_size]\n",
        "        test = dataset[train_size:]\n",
        "\n",
        "        train_set = set([list(word.keys())[0] for word in train])\n",
        "        test_set = set([list(word.keys())[0] for word in test])\n",
        "\n",
        "        # Проверка пересечения\n",
        "        overlap = train_set.intersection(test_set)\n",
        "        overlap_ratio = len(overlap) / len(test_set)\n",
        "\n",
        "        # Если пересечение в пределах допустимого, завершаем\n",
        "        if overlap_ratio <= max_overlap_ratio:\n",
        "            return train, test\n",
        "\n",
        "    raise ValueError(\"Не удалось разделить данные с заданным ограничением на пересечение.\")\n",
        "\n",
        "\n",
        "# Разделение с ограничением на пересечение\n",
        "train, test = split_with_overlap_limit(dataset, train_ratio=0.3, max_overlap_ratio=0.25)\n",
        "\n",
        "print(\"Обучающая выборка:\", train)\n",
        "print(\"Тестовая выборка:\", test)"
      ],
      "metadata": {
        "id": "f4EpCMW8Xm9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test)"
      ],
      "metadata": {
        "id": "3Dov_U2oEfSy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Данная функция, вдохновленная метрикой chrF++, обеспечивает ретрив релевантных примеров. По максимально совпадающим биграммам выбираются до 10 актуальных примеров, а также выделяются морфемы, которые, возможно, встречаются в тестовом слове (target word)"
      ],
      "metadata": {
        "id": "U9nv8qb1SUN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ngrams(word, n=2):\n",
        "    return [word[i:i+n] for i in range(len(word) - n + 1)]\n",
        "\n",
        "def calculate_chrf(true_word, candidate_word, n=2, beta=1):\n",
        "    true_ngrams = get_ngrams(true_word, n)\n",
        "    candidate_ngrams = get_ngrams(candidate_word, n)\n",
        "\n",
        "    true_counts = Counter(true_ngrams)\n",
        "    candidate_counts = Counter(candidate_ngrams)\n",
        "\n",
        "    overlap = sum(min(true_counts[ng], candidate_counts[ng]) for ng in candidate_counts if ng in true_counts)\n",
        "\n",
        "    precision = overlap / len(candidate_ngrams) if candidate_ngrams else 0\n",
        "    recall = overlap / len(true_ngrams) if true_ngrams else 0\n",
        "\n",
        "    if precision == 0 or recall == 0:\n",
        "        return 0\n",
        "\n",
        "    chrf = ((1 + beta**2) * precision * recall) / (recall + beta**2 * precision)\n",
        "    return chrf"
      ],
      "metadata": {
        "id": "wL8FEQD_L2Ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_word = list(test[148].keys())[0]"
      ],
      "metadata": {
        "id": "Qfn2P6sYbndB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test[148]"
      ],
      "metadata": {
        "id": "Y-aYThmheOst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "for word in train:\n",
        "    cand, segm = list(word.items())[0]\n",
        "    chrf_score = calculate_chrf(target_word, cand)\n",
        "    results.append((cand, segm, chrf_score))\n",
        "\n",
        "results.sort(key=lambda x: x[2], reverse=True)\n",
        "\n",
        "examples = results[:30]\n",
        "glosses = set()\n",
        "for word, segm, score in sorted(set(examples), key=lambda x: -x[2]):\n",
        "    morphemes = segm.split('-')\n",
        "    for m in morphemes:\n",
        "        if m in morph_gloss:\n",
        "            glosses.add(f'{m}={morph_gloss[m]}')\n",
        "    print(f\"Слово: {word}, разделение: {segm} chrF++: {score:.3f}\")"
      ],
      "metadata": {
        "id": "KPwbgofoMG8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Системный промпт для Гигачата"
      ],
      "metadata": {
        "id": "395SHY9t9h_r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRRUR2cV8owj"
      },
      "outputs": [],
      "source": [
        "# prompt = \"\"\"### Ты – выдающийся лингвист, специализирующийся в морфологии нивхского языка.\n",
        "\n",
        "# #### Задача\n",
        "# Отглосировать слово, то есть выделить составляющие его морфемы и разделить их дефисами ('-'). В качестве примера будет дано несколько уже отглоссированных слов.\n",
        "\n",
        "# #### Инструкции\n",
        "# 1. Проанализируй структуру предложенного слова.\n",
        "# 2. Найди в нем отдельные морфемы (корень, суффиксы, окончания) и раздели их дефисами.\n",
        "# 4. Не изменяй символы в словах: все символы должны быть точно такими же, как в оригинале. Не удаляй буквы и не вставляй другие.\n",
        "# 5. Возможные морфемы могут помочь, но не всегда присутствуют в оригинальном слове.\n",
        "# 6. Слово из твоего ответ без дефисов должны быть идентичным оригинальному.\n",
        "\n",
        "# #### Формат ответа\n",
        "# - Результат должен содержать оригинальное слово, но со всеми выявленными морфемами, разделенными дефисами.\n",
        "# - Тебе необходимо ответить только одним словом, разделенным на дефисы\n",
        "\n",
        "# #### Пример\n",
        "# Раздели следующие слово на морфемы: `ӽаудь`\n",
        "\n",
        "# Другие слова:\n",
        "# `ғаудь`: `ғау-дь`\n",
        "# `ӽаугудьғу`: `ӽау-гу-дь-ғу`\n",
        "# `ӄ’аудь`: `ӄ’ау-дь`\n",
        "# `ӄаудь`: `ӄау-дь`\n",
        "\n",
        "# Возможные морфемы:\n",
        "# - 'ӄ’ау': AUX:NEG\n",
        "# - 'дь': IND\n",
        "# - 'ғу': PL\n",
        "\n",
        "# ----------------\n",
        "# Твой ответ: `ӽау-дь`\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Ты – выдающийся лингвист, специализирующийся в морфологии нивхского языка.\n",
        "\n",
        "#### Задача\n",
        "Отглосировать слово, то есть выделить составляющие его морфемы и разделить их дефисами ('-').\n",
        "\n",
        "#### Инструкция\n",
        "1. Проанализируй структуру предложенного слова.\n",
        "2. Найди в нём отдельные морфемы (корень, аффиксы, окончания) и раздели их дефисами.\n",
        "3. Морфемы могут отсутствовать в некоторых случаях; учитывай этот факт при анализе.\n",
        "4. Сохраняй неизменными все символы в словах.\n",
        "\n",
        "#### Формат ответа\n",
        "Ответ должен содержать только одно слово, которое будет оригинальным словом, разделённым дефисами согласно найденным морфемам.\n",
        "\n",
        "#### Пример работы\n",
        "----------------\n",
        "Раздели следующие слово на морфемы: `ӽаудь`\n",
        "\n",
        "Другие слова:\n",
        "`ғаудь`: `ғау-дь`\n",
        "`ӽаугудьғу`: `ӽау-гу-дь-ғу`\n",
        "`ӄ’аудь`: `ӄ’ау-дь`\n",
        "`ӄаудь`: `ӄау-дь`\n",
        "\n",
        "Возможные морфемы:\n",
        "- 'ӄ’ау': AUX:NEG\n",
        "- 'дь': IND\n",
        "- 'ғу': PL\n",
        "\n",
        "Твой ответ: `ӽау-дь`\"\"\""
      ],
      "metadata": {
        "id": "wdkPBVvqJLMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$chrF$++ $= \\frac {(1 + \\beta^2) \\cdot P \\cdot R} {R + β^2 \\cdot P} $\n",
        "\n",
        "где:\n",
        "\n",
        "$P$ – precision (доля совпадающих n-грамм в предсказании относительно всех n-грамм в предсказании).\n",
        "\n",
        "$R$ – recall (доля совпадающих n-грамм в истинной строке относительно всех n-грамм в истинной строке).\n",
        "\n",
        "$\\beta$ – параметр, который регулирует важность recall относительно precision.\n"
      ],
      "metadata": {
        "id": "K5Niv5U393iq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ngrams(word, n=2):\n",
        "    # получение n-грам\n",
        "    return [word[i:i+n] for i in range(len(word) - n + 1)]\n",
        "\n",
        "def calculate_chrf(true_word, candidate_word, n=2, beta=1):\n",
        "    true_ngrams = get_ngrams(true_word, n)\n",
        "    candidate_ngrams = get_ngrams(candidate_word, n)\n",
        "\n",
        "    true_counts = Counter(true_ngrams)\n",
        "    candidate_counts = Counter(candidate_ngrams)\n",
        "\n",
        "    overlap = sum(min(true_counts[ng], candidate_counts[ng]) for ng in candidate_counts if ng in true_counts)\n",
        "\n",
        "    precision = overlap / len(candidate_ngrams) if candidate_ngrams else 0\n",
        "    recall = overlap / len(true_ngrams) if true_ngrams else 0\n",
        "\n",
        "    if precision == 0 or recall == 0:\n",
        "        return 0\n",
        "\n",
        "    chrf = ((1 + beta**2) * precision * recall) / (recall + beta**2 * precision)\n",
        "    return chrf"
      ],
      "metadata": {
        "id": "GFVY0kut9wir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upmtM4S-_mWt"
      },
      "outputs": [],
      "source": [
        "def format_prompt(target_word):\n",
        "    prompt = \"\"\"Раздели следующие слово на морфемы: `{0}`\n",
        "\n",
        "Похожие слова:\n",
        "{1}\n",
        "\n",
        "Возможные морфемы:\n",
        "{2}\"\"\"\n",
        "\n",
        "    glosses = set()\n",
        "    final_examples = []\n",
        "    results = []\n",
        "    # высчитываем метрику chrF++ для таргета по трейну\n",
        "    for word in train:\n",
        "        cand, segm = list(word.items())[0]\n",
        "        chrf_score = calculate_chrf(target_word, cand)\n",
        "        results.append((cand, segm, chrf_score))\n",
        "\n",
        "    # берем 30 ближайших слов\n",
        "    results.sort(key=lambda x: x[2], reverse=True)\n",
        "    examples = sorted(set(results[:30]), key=lambda x: -x[2])\n",
        "\n",
        "    for word, segm, score in examples:\n",
        "        morphemes = segm.split('-')\n",
        "        for m in morphemes:\n",
        "            if m in morph_gloss:\n",
        "                glosses.add(f'{m}={morph_gloss[m]}')\n",
        "\n",
        "        final_examples.append(f\"Слово: {word}, разделение: {segm}\")\n",
        "        poss_glosses = '- ' + '\\n- '.join(list(glosses)[:3])\n",
        "    final_examples = '\\n'.join(final_examples[:5])\n",
        "    new_prompt = prompt.format(target_word, final_examples, poss_glosses)\n",
        "    return new_prompt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_prompt = format_prompt('ӿоӄот')\n",
        "print(target_prompt)"
      ],
      "metadata": {
        "id": "IR_mrIMcEICt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASPAzt73Skky"
      },
      "outputs": [],
      "source": [
        "!pip --quiet install gigachat"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "token = userdata.get('Giga_TOKEN')"
      ],
      "metadata": {
        "id": "kFQpusp783yY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://ngw.devices.sberbank.ru:9443/api/v2/oauth\"\n",
        "\n",
        "payload = 'scope=GIGACHAT_API_PERS'\n",
        "headers = {\n",
        "    'Content-Type': 'application/x-www-form-urlencoded',\n",
        "    'Accept': 'application/json',\n",
        "    'RqUID': '1777f9ac-7f6e-4632-9a84-24374af0adb3',\n",
        "    'Authorization': f'Basic {token}'\n",
        "}\n",
        "\n",
        "response = requests.request(\"POST\", url, headers=headers, data=payload, verify=False)\n",
        "\n",
        "access_token = response.json()['access_token']"
      ],
      "metadata": {
        "id": "ISsXi0ZV8YbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib3\n",
        "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)"
      ],
      "metadata": {
        "id": "NTAEjOk4e6Bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://gigachat.devices.sberbank.ru/api/v1/chat/completions\""
      ],
      "metadata": {
        "id": "fzUQ8BKn-sWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_giga_answers(target_word, access_token):\n",
        "    headers = {\n",
        "    'Content-Type': 'application/json',\n",
        "    'Accept': 'application/json',\n",
        "    'Authorization': f'Bearer {access_token}'}\n",
        "\n",
        "    payload = {\n",
        "    \"model\": \"GigaChat\",\n",
        "    \"messages\":\n",
        "            [{\"role\": \"system\",\n",
        "              \"content\": prompt}],\n",
        "    \"profanity_check\": True,\n",
        "    \"max_tokens\": 15}\n",
        "\n",
        "    target_prompt = format_prompt(target_word)\n",
        "    payload['messages'].append({'role': 'user',\n",
        "                                'content': target_prompt})\n",
        "\n",
        "    response = requests.request(\"POST\", url, headers=headers, data=json.dumps(payload), verify=False)\n",
        "    return response"
      ],
      "metadata": {
        "id": "hUlhZZo8_7fL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "import time"
      ],
      "metadata": {
        "id": "JUAzvPjQ-J92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция для генерации нового токена\n",
        "def generate_access_token(token):\n",
        "    url = \"https://ngw.devices.sberbank.ru:9443/api/v2/oauth\"\n",
        "    payload = 'scope=GIGACHAT_API_PERS'\n",
        "\n",
        "    headers = {\n",
        "    'Content-Type': 'application/x-www-form-urlencoded',\n",
        "    'Accept': 'application/json',\n",
        "    'RqUID': '1777f9ac-7f6e-4632-9a84-24374af0adb3',\n",
        "    'Authorization': f'Basic {token}'\n",
        "}\n",
        "\n",
        "    print(\"\\nГенерация нового токена...\\n\")\n",
        "    response = requests.request(\"POST\", url, headers=headers, data=payload, verify=False)\n",
        "    token = response.json()['access_token']\n",
        "    return token"
      ],
      "metadata": {
        "id": "_DdgOOkDHSNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test)"
      ],
      "metadata": {
        "id": "9t6e3xre3O4u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_preds = []"
      ],
      "metadata": {
        "id": "idQdJfuuDc4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "access_token = generate_access_token(token)\n",
        "last_token_time = time.time()\n",
        "token_lifetime = 25 * 60\n",
        "final_data = {}\n",
        "for word in tqdm.tqdm(test):\n",
        "    if time.time() - last_token_time >= token_lifetime:\n",
        "        access_token = generate_access_token(token)\n",
        "        last_token_time = time.time()\n",
        "        print('\\nНовый токен\\n')\n",
        "\n",
        "    final_data = {}\n",
        "    word, true_label = list(word.items())[0]\n",
        "    response = get_giga_answers(word, access_token)\n",
        "    pred = response.json()['choices'][0]['message']['content']\n",
        "    final_data['word'] = word\n",
        "    final_data['true_label'] = true_label\n",
        "    final_data['pred_label'] = pred\n",
        "    all_preds.append(final_data)"
      ],
      "metadata": {
        "id": "6aYmeBzaKGoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('preds_2_prompt.json', 'w', encoding='utf8')\n",
        "new_file = json.dumps(all_preds)\n",
        "f.write(new_file)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "UwN7Cs0wJfi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_preds)"
      ],
      "metadata": {
        "id": "G3mbJwkmDIQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = pd.DataFrame(all_preds)"
      ],
      "metadata": {
        "id": "z0VdYj28Z3-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds"
      ],
      "metadata": {
        "id": "aVC5Z6hrZ7eN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_drop = list(preds[preds['word'].str.contains('(?<![ˇ’ʻ‘ʼ\\'р̌’‘ӻӿӃӾЧА-яёЁӽӈғӄӷ])[ӷр̌ӻӿа-яёӽӈғӄ](?![р̌’ʻ‘\\'ʼӻғӿА-яЁёӽӈа-яˇӄӷ])')].index)"
      ],
      "metadata": {
        "id": "-Kxy2XzHbUhc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_drop.extend(list(preds[preds['word'].str.contains('\"[ˇ’ʻ‘ʼ\\'р̌’‘ӻӿӃӾЧА-яёЁӽӈғӄӷ]{2}\"')].index))"
      ],
      "metadata": {
        "id": "GsOGUDrZcXK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = preds.drop(to_drop)"
      ],
      "metadata": {
        "id": "74gobXXxbyWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds"
      ],
      "metadata": {
        "id": "pfdjr4Ojcwe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds[preds['true_label'] == preds['pred_label']]"
      ],
      "metadata": {
        "id": "byOJc08PKJxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(list(preds['true_label']), list(preds['pred_label']))"
      ],
      "metadata": {
        "id": "j7OUk4lMaAbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Word accuracy: {accuracy*100:.2f}%')"
      ],
      "metadata": {
        "id": "HfqWOhnqaS0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_data = [x.split('-') for x in list(preds['true_label'])]\n",
        "true_data[0]"
      ],
      "metadata": {
        "id": "0esfIK0urB65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_data = [x.split('-') for x in list(preds['pred_label'])]\n",
        "predicted_data[0]"
      ],
      "metadata": {
        "id": "FnE_R3J4rLaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_overall_accuracy(true_data, predicted_data):\n",
        "    \"\"\"\n",
        "    Вычисляет общую точность морфем для множества слов.\n",
        "    \"\"\"\n",
        "    total_correct = 0\n",
        "    total_count = 0\n",
        "\n",
        "    for true_segments, predicted_segments in zip(true_data, predicted_data):\n",
        "        total_correct += sum(1 for true, pred in zip(true_segments, predicted_segments) if true == pred)\n",
        "        total_count += len(true_segments)\n",
        "\n",
        "    return total_correct / total_count if total_count > 0 else 0.0\n",
        "\n",
        "\n",
        "overall_accuracy = calculate_overall_accuracy(true_data, predicted_data)\n",
        "print(f\"Overall Morpheme Accuracy: {overall_accuracy:.2%}\")"
      ],
      "metadata": {
        "id": "QNTkmBTeq6pC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def compute_chrf_morpheme_as_ngram(true_segments, predicted_segments, beta=2):\n",
        "    \"\"\"\n",
        "    Вычисляет метрику chrF++ для сегментированных данных, где морфемы рассматриваются как n-граммы.\n",
        "    \"\"\"\n",
        "    def compute_fscore(precision, recall, beta):\n",
        "        \"\"\" Вычисляет F-меру. \"\"\"\n",
        "        if precision + recall == 0:\n",
        "            return 0.0\n",
        "        return (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n",
        "\n",
        "    ref_morphemes = Counter(true_segments)\n",
        "    hyp_morphemes = Counter(predicted_segments)\n",
        "\n",
        "    # совпадающие морфемы\n",
        "    common_morphemes = ref_morphemes & hyp_morphemes\n",
        "    matches = sum(common_morphemes.values())\n",
        "\n",
        "    # общее количество морфем\n",
        "    total_ref_morphemes = sum(ref_morphemes.values())\n",
        "    total_hyp_morphemes = sum(hyp_morphemes.values())\n",
        "\n",
        "    # точность и полнота\n",
        "    precision = matches / total_hyp_morphemes if total_hyp_morphemes > 0 else 0\n",
        "    recall = matches / total_ref_morphemes if total_ref_morphemes > 0 else 0\n",
        "\n",
        "    # вычисление F-меры\n",
        "    chrf_score = compute_fscore(precision, recall, beta)\n",
        "    return chrf_score"
      ],
      "metadata": {
        "id": "9DFV4F6Rr8Eo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_overall_chrf_morpheme_as_ngram(true_data, predicted_data, beta=2):\n",
        "    \"\"\"\n",
        "    Вычисляет средний chrF++ для множества сегментированных данных.\n",
        "    \"\"\"\n",
        "    scores = [\n",
        "        compute_chrf_morpheme_as_ngram(true_segments, predicted_segments, beta)\n",
        "        for true_segments, predicted_segments in zip(true_data, predicted_data)\n",
        "    ]\n",
        "    return sum(scores) / len(scores) if scores else 0.0\n",
        "\n",
        "\n",
        "overall_chrf = compute_overall_chrf_morpheme_as_ngram(true_data, predicted_data)"
      ],
      "metadata": {
        "id": "H8a6wTwRtQvi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Word accuracy: {accuracy:.2%}')\n",
        "print(f\"Morpheme Accuracy: {overall_accuracy:.2%}\")\n",
        "print(f\"Overall chrF++ Score for Morpheme-as-ngram Data: {overall_chrf:.2%}\")"
      ],
      "metadata": {
        "id": "TRD6HF_Erm_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = {idx: [list(key_dict.items())[0][0], list(key_dict.items())[0][1]] for idx, key_dict in enumerate(train)}\n",
        "df = pd.DataFrame(train_set, index=['word', 'segmentation']).T\n",
        "df.to_csv('train.csv')"
      ],
      "metadata": {
        "id": "DnS3S8YRoKKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Попытка исправить"
      ],
      "metadata": {
        "id": "vwg1jY6KlCpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_test = [{key.word: key.true_label} for key in preds[preds['true_label'] != preds['pred_label']].itertuples()]"
      ],
      "metadata": {
        "id": "urWIvrnpbBtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(preds[preds['true_label'] != preds['pred_label']].index))"
      ],
      "metadata": {
        "id": "0JapOt16LP4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "access_token = generate_access_token(token)\n",
        "last_token_time = time.time()\n",
        "token_lifetime = 25 * 60\n",
        "final_data = {}\n",
        "for word in tqdm.tqdm(new_test):\n",
        "    if time.time() - last_token_time >= token_lifetime:\n",
        "        access_token = generate_access_token(token)\n",
        "        last_token_time = time.time()\n",
        "        print('Новый токен')\n",
        "\n",
        "    final_data = {}\n",
        "    word, true_label = list(word.items())[0]\n",
        "    response = get_giga_answers(word, access_token, )\n",
        "    pred = response.json()['choices'][0]['message']['content']\n",
        "    final_data['word'] = word\n",
        "    final_data['true_label'] = true_label\n",
        "    final_data['pred_label'] = pred\n",
        "    all_preds.append(final_data)"
      ],
      "metadata": {
        "id": "gdc_DwnidNwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_preds = all_preds[2597:]\n",
        "all_preds = all_preds[:2597]"
      ],
      "metadata": {
        "id": "9OGKvMj3Uqzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_preds)"
      ],
      "metadata": {
        "id": "I14eTUHSUgxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(preds[preds['true_label'] != preds['pred_label']].index))"
      ],
      "metadata": {
        "id": "g2Tzc0AKoEw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iter = 0\n",
        "for idx in list(preds[preds['true_label'] != preds['pred_label']].index):\n",
        "    all_preds[idx] = new_preds[iter]\n",
        "    iter += 1"
      ],
      "metadata": {
        "id": "voQSxduIonqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_preds = pd.DataFrame(all_preds)\n",
        "new_preds"
      ],
      "metadata": {
        "id": "iCv0MHp-pIyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_drop = new_preds[new_preds['word'].str.contains('\"[ˇ’ʻ‘ʼ\\'р̌’‘ӻӿӃӾЧА-яёЁӽӈғӄӷ]{2}\"')].index"
      ],
      "metadata": {
        "id": "pDA_Yq_mvyIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_preds = new_preds.drop(to_drop)"
      ],
      "metadata": {
        "id": "Q993YSQWtZ6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_preds['true_label'] = new_preds['true_label'].str.replace('\"', '')\n",
        "new_preds['word'] = new_preds['word'].str.replace('\"', '')\n",
        "new_preds['true_label'] = new_preds['true_label'].str.replace('»', '')\n",
        "new_preds['pred_label'] = new_preds['pred_label'].str.replace('=', '-')\n",
        "new_preds['word'] = new_preds['word'].str.replace('»', '')\n",
        "\n",
        "new_preds"
      ],
      "metadata": {
        "id": "nJp35tFfpVhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(list(new_preds['true_label']), list(new_preds['pred_label']))\n",
        "accuracy"
      ],
      "metadata": {
        "id": "tD2Tgb-6pP5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_preds.to_csv('new_preds.csv')"
      ],
      "metadata": {
        "id": "Alk3nSBUXQVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Анализ ошибок модели (?)"
      ],
      "metadata": {
        "id": "qwcenkjpqguh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preds[new_preds['pred_label'] != preds['true_label']]"
      ],
      "metadata": {
        "id": "y2dOVjmfqu7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = [list(key)[0] for key in train]"
      ],
      "metadata": {
        "id": "9nGBFf_Xwqrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "errors = {'segm_error': 0,\n",
        "          'deleted_symb': 0,\n",
        "          'added_symb': 0,\n",
        "          'difficult_words': 0,\n",
        "          'preprocess_error': 0,\n",
        "          'other': 0}\n",
        "\n",
        "for word in new_preds[new_preds['pred_label'] != new_preds['true_label']].itertuples():\n",
        "    real_word = word.word\n",
        "    pred_word = word.pred_label.replace('-', '')\n",
        "\n",
        "    # ошибка разметки\n",
        "    if re.search('[A-z]+', real_word):\n",
        "        errors['preprocess_error'] += 1\n",
        "        continue\n",
        "\n",
        "    # просто ошибка в сегментации\n",
        "    if len(real_word) == len(pred_word):\n",
        "        if real_word == pred_word:\n",
        "            errors['segm_error'] += 1\n",
        "            continue\n",
        "\n",
        "    # удаление лишних символов\n",
        "    if len(real_word) > len(pred_word):\n",
        "        errors['deleted_symb'] += 1\n",
        "        continue\n",
        "\n",
        "    if len(real_word) < len(pred_word):\n",
        "        errors['added_symb'] += 1\n",
        "        continue\n",
        "\n",
        "    if real_word not in train_set:\n",
        "        errors['difficult_words'] += 1\n",
        "        continue\n",
        "\n",
        "    # прочее (замена символа в слове на другой символ)\n",
        "    else:\n",
        "        errors['other'] += 1"
      ],
      "metadata": {
        "id": "TRaGPIHrqk1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[float(f'{x / sum(errors.values()):.2f}') for x in errors.values()]"
      ],
      "metadata": {
        "id": "ICFnWWwgy3lM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Данные для круговой диаграммы\n",
        "labels = ['Сегментация', 'Удаление', 'Вставка', 'Разметка', 'Предобработка', 'Прочее']\n",
        "sizes = [float(f'{x / sum(errors.values()):.2f}') for x in errors.values()]  # Процентное соотношение\n",
        "colors = ['#607196', '#CCD7C5', '#C7A27C', '#EFD2CB', '#c2c2f0', '#EE9480', '#285943']  # Цветовая палитра\n",
        "explode = (0.1, 0, 0, 0, 0, 0)  # Выделение первого сегмента\n",
        "\n",
        "# Создание фигуры и осей\n",
        "plt.figure(figsize=(8, 8))  # Размер графика\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "# Построение круговой диаграммы\n",
        "plt.pie(\n",
        "    sizes,\n",
        "    explode=explode, # Выделяем первый сегмент\n",
        "    colors=colors,    # Цвета сегментов\n",
        "    autopct='%1.1f%%',  # Отображение процентов\n",
        "    shadow=False,      # Тень для объёма\n",
        "    startangle=90,    # Начальный угол поворота диаграммы\n",
        "    textprops={'fontsize': 9}, # Размер шрифта для подписей\n",
        "    pctdistance=1.09,\n",
        "    wedgeprops=dict(width=0.3)\n",
        ")\n",
        "\n",
        "# Обеспечиваем круглую форму диаграммы\n",
        "plt.axis('equal')\n",
        "\n",
        "# Добавляем заголовок\n",
        "plt.title('Типы ошибок в процентах', fontsize=16, fontweight='bold')\n",
        "plt.legend(labels, title=\"Категории\", loc=\"best\")\n",
        "\n",
        "# Отображение диаграммы\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7zWLr_VmyxIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Глоссирование как задача классификации"
      ],
      "metadata": {
        "id": "9OTBl1uGlc6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import tqdm\n",
        "import time\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "0O_zfESuGYe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -L -o final_glosses.csv 'https://docs.google.com/spreadsheets/d/19045IoPzWSiTvmZC3zQIS1vqRYpwazQPCKTEov2dCcU/export?exportFormat=csv'"
      ],
      "metadata": {
        "id": "aHH3WAAwGLS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_glosses = pd.read_csv('final_glosses.csv').drop(['Category'], axis=1)"
      ],
      "metadata": {
        "id": "3mTZKsQxGQTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -L -o data.json \"https://drive.google.com/uc?export=download&id=1UK3M9yhRbG59dxKUpo67DROx5KK84CVe\""
      ],
      "metadata": {
        "id": "t2grunR0GU_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -L -o stem.json \"https://drive.google.com/uc?export=download&id=1EBpjmHoy0Tj6kC1eWaQSV7LvjSrp0kc0\""
      ],
      "metadata": {
        "id": "JVAAa8Ybijb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "f = open('/content/stem.json', 'r', encoding='utf8')\n",
        "stems = json.load(f)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "EtwMJpaDioNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "morph_gloss = {}\n",
        "for key in final_glosses.itertuples():\n",
        "    morphemes = key.Morph.split(', ')\n",
        "    for morph in morphemes:\n",
        "        if morph not in morph_gloss:\n",
        "            morph_gloss[morph] = []\n",
        "        morph_gloss[morph].append(key.Gloss)"
      ],
      "metadata": {
        "id": "MvOjVdfpGfOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "f = open('/content/data.json', 'r', encoding='utf8')\n",
        "data = json.load(f)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "8GMYb-E5GlNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gold_segmented = [x['segmented'] for x in data]\n",
        "gold_labels = [x['glossed'] for x in data]\n",
        "gold_translation = [x['translation'] for x in data]"
      ],
      "metadata": {
        "id": "xrYYyGoJG0NU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gold_segmented[7]"
      ],
      "metadata": {
        "id": "gRUjbrvJHBEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gold_labels[7]"
      ],
      "metadata": {
        "id": "ASoJznm5I3ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gold_translation[7]"
      ],
      "metadata": {
        "id": "mHL1W5TrJo4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [{'segm': x, 'label': y, 'translation': z} for x, y, z in zip(gold_segmented, gold_labels, gold_translation)]"
      ],
      "metadata": {
        "id": "exwSzh9IKqk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(72)\n",
        "\n",
        "random.shuffle(dataset)"
      ],
      "metadata": {
        "id": "AXqvqfALLVpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "id": "By7xn45ULrEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = dataset[:200], dataset[200:]"
      ],
      "metadata": {
        "id": "hWj-E_IeLn3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train[25]['label'].replace('\\t', ' '))"
      ],
      "metadata": {
        "id": "n_zSfoXELxGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"Ты – выдающийся лингвист, специализирующийся в морфологии нивхского языка.\n",
        "\n",
        "#### Задача\n",
        "Определить соответствие уже выделенных морфем их грамматическим значениям.\n",
        "\n",
        "#### Инструкция\n",
        "1. Каждой части словоформы соотнеси русскую лемму или сооответствующую глоссу из предоставленного списка.\n",
        "2. Для определения глоссы леммы используй перевод.\n",
        "3. Сохраняй неизменными все символы в словах.\n",
        "\n",
        "#### Формат ответа\n",
        "Ответ должен содержать строку, с таким же количеством слов, как и в поданном на разбор предложении.\n",
        "\n",
        "#### Пример работы\n",
        "----------------\n",
        "Определи глоссы в следующем предложении:\n",
        "Иф сидь лаға ғар в ытык ғе, в ымык ғе сык ны т’а дь.\n",
        "\n",
        "Перевод:\n",
        "Что он ни требовал, мать с отцом всё делали.\n",
        "\n",
        "Список возможных глосс:\n",
        "- 'ғар': RES, IND:EMPH, CONV:SUBJ\n",
        "- 'в': REC\n",
        "- 'ғе': COM\n",
        "- 'т’а': USIT.1.PL, USIT.1.SG, USIT.2.PL, USIT.3.PL, PROH, COORD.1.PL\n",
        "- 'дь': NMN:P, IND\n",
        "----------------\n",
        "Твой ответ: Иф=он сидь=что лаға=требовать ғар=CONV:SUBJ в=REC ытык=отец ғе=COM в=REC ымык=мать ғе=COM сык=весь ны=делать т’а=USIT.3.PL дь=IND\"\"\""
      ],
      "metadata": {
        "id": "KAJFqW1wJrCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_access_token(token):\n",
        "    url = \"https://ngw.devices.sberbank.ru:9443/api/v2/oauth\"\n",
        "    payload = 'scope=GIGACHAT_API_PERS'\n",
        "\n",
        "    headers = {\n",
        "    'Content-Type': 'application/x-www-form-urlencoded',\n",
        "    'Accept': 'application/json',\n",
        "    'RqUID': '1777f9ac-7f6e-4632-9a84-24374af0adb3',\n",
        "    'Authorization': f'Basic {token}'\n",
        "}\n",
        "\n",
        "    print(\"\\nГенерация нового токена...\\n\")\n",
        "    response = requests.request(\"POST\", url, headers=headers, data=payload, verify=False)\n",
        "    token = response.json()['access_token']\n",
        "    return token"
      ],
      "metadata": {
        "id": "9ktutih0NznR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train[25]"
      ],
      "metadata": {
        "id": "2YrI1v5PO39t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_sentence(dataset):\n",
        "    final_data = []\n",
        "    # Разделяем сегментированный текст на токены и морфемы\n",
        "    for data in dataset:\n",
        "        segm_tokens = data['segm'].split('\\t')  # Разделение по табуляции\n",
        "        morphemes = [token.split('-') for token in segm_tokens]  # Разделение на морфемы\n",
        "\n",
        "        # Разделяем метки на токены и их грамматические метки\n",
        "        label_tokens = data['label'].split('\\t')  # Разделение по табуляции\n",
        "        labels = [label.split('-') for label in label_tokens]  # Разделение на метки\n",
        "\n",
        "        # Формируем структурированное представление\n",
        "        processed_data = {\n",
        "            \"text\": \" \".join(segm_tokens),  # Исходный текст\n",
        "            \"label_tokens\": \" \".join(label_tokens),\n",
        "            \"translation\": data['translation'],  # Перевод\n",
        "            \"tokens\": [\n",
        "                {\n",
        "                    \"token\": token,\n",
        "                    \"morphemes\": morpheme_list,\n",
        "                    \"labels\": label_list\n",
        "                }\n",
        "                for token, morpheme_list, label_list in zip(segm_tokens, morphemes, labels)\n",
        "            ]\n",
        "        }\n",
        "        final_data.append(processed_data)\n",
        "    return final_data"
      ],
      "metadata": {
        "id": "V_HSDDycVh-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = process_sentence(train)\n",
        "test_data = process_sentence(test)"
      ],
      "metadata": {
        "id": "YFeZsvQ-SZko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_retrieval(target_sentence, corpus, token_weight=0.5, morpheme_weight=0.8, top_n=3):\n",
        "\n",
        "    def count_matching_elements(target, candidate):\n",
        "        \"\"\"\n",
        "        Подсчитывает количество совпадающих элементов между целевым списком и кандидатом.\n",
        "        \"\"\"\n",
        "        target_counter = Counter(target)\n",
        "        candidate_counter = Counter(candidate)\n",
        "        common_elements = target_counter & candidate_counter\n",
        "        return sum(common_elements.values())\n",
        "\n",
        "    # Извлечение токенов и морфем из целевого предложенияx\n",
        "    target_tokens = [token[\"token\"] for token in target_sentence[\"tokens\"]]\n",
        "    target_morphemes = [morpheme for token in target_sentence[\"tokens\"] for morpheme in token[\"morphemes\"]]\n",
        "\n",
        "    # Подсчет совпадений для каждого предложения в корпусе\n",
        "    scored_sentences = []\n",
        "    for sentence in corpus:\n",
        "        # Извлечение токенов и морфем из предложения корпуса\n",
        "        sentence_tokens = [token[\"token\"] for token in sentence[\"tokens\"]]\n",
        "        sentence_morphemes = [morpheme for token in sentence[\"tokens\"] for morpheme in token[\"morphemes\"]]\n",
        "\n",
        "        # Подсчет совпадений токенов\n",
        "        token_score = count_matching_elements(target_tokens, sentence_tokens)\n",
        "\n",
        "        # Подсчет совпадений морфем\n",
        "        morpheme_score = count_matching_elements(target_morphemes, sentence_morphemes)\n",
        "\n",
        "        # Взвешенная сумма\n",
        "        total_score = (\n",
        "            token_score * token_weight +\n",
        "            morpheme_score * morpheme_weight\n",
        "        )\n",
        "\n",
        "        # Сохраняем результат\n",
        "        scored_sentences.append((sentence, total_score))\n",
        "\n",
        "    # Сортировка предложений по взвешенному скору\n",
        "    scored_sentences.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Возвращаем топ-N предложений\n",
        "    return [sentence for sentence, _ in scored_sentences[:top_n]]"
      ],
      "metadata": {
        "id": "-TGs_ZDdT1QU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data[25]['text']"
      ],
      "metadata": {
        "id": "A68dGRWtUyzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\n\\n'.join(['\\n'.join([x['text'], x['label_tokens']]) for x in weighted_retrieval(test_data[25], train_data, top_n=2)]) )"
      ],
      "metadata": {
        "id": "mFxMBaQnYU3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in weighted_retrieval(test_data[25], train_data, top_n=2):\n",
        "    print(x['text'], end='\\n')\n",
        "    print(x['label_tokens'], end='\\n\\n')"
      ],
      "metadata": {
        "id": "rV2b272vUjl2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data[3]"
      ],
      "metadata": {
        "id": "WgLuJDBjkBgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_label(sent):\n",
        "    text = re.sub('[\\.\\,]+', '', sent['text']).replace('-', ' ').split()\n",
        "    label_tokens = sent['label_tokens'].replace('-', ' ').split()\n",
        "    final_label = ' '.join([f'{x}={y}' for x, y in zip(text, label_tokens)])\n",
        "    return final_label"
      ],
      "metadata": {
        "id": "LnNk9pzIj-Vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_label(test_data[3])"
      ],
      "metadata": {
        "id": "1FPs8yw5lMrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Необходимая для ретрива словарных данных функция\n",
        "\"\"\"\n",
        "\n",
        "def extract_stems(tokens):\n",
        "    \"\"\"\n",
        "    Извлекает основы (стемы) из токенов.\n",
        "    \"\"\"\n",
        "    stems = set()\n",
        "    for token in tokens:\n",
        "        # Берем первую морфему как основу (или можно использовать более сложную логику)\n",
        "        if token[\"morphemes\"]:\n",
        "            stems.add(token[\"morphemes\"][0])  # Первая морфема считается основой\n",
        "    return list(stems)\n",
        "\n",
        "\n",
        "def retrieve_from_stem_dictionary(target_sentence, stem_dictionary):\n",
        "    \"\"\"\n",
        "    Выполняет ретрив из словаря основ\n",
        "    \"\"\"\n",
        "    # Извлечение основ из целевого предложения\n",
        "    target_tokens = target_sentence[\"tokens\"]\n",
        "    target_stems = extract_stems(target_tokens)\n",
        "\n",
        "    # Поиск совпадений в словаре\n",
        "    results = []\n",
        "    for stem in target_stems:\n",
        "        if stem in stem_dictionary:\n",
        "            trads = '; '.join(stem_dictionary[stem]['ru'])\n",
        "            results.append(f'{stem}: {trads}')\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "QSezu9bNiY4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_segm_prompt(sent, random_sample=False):\n",
        "    prompt = \"\"\"Определи глоссы в следующем предложении:\n",
        "{0}\n",
        "\n",
        "Перевод:\n",
        "{1}\n",
        "\n",
        "Похожие примеры:\n",
        "{2}\n",
        "\n",
        "Список возможных глосс:\n",
        "{3}\n",
        "\n",
        "Русские основы слов:\n",
        "{4}\n",
        "\"\"\"\n",
        "    segm_sent = sent['text'].replace('-', ' ')\n",
        "    translation = sent['translation']\n",
        "    if random_sample:\n",
        "        top_sent = random.sample(train_data, 2)\n",
        "    else:\n",
        "        top_sent = weighted_retrieval(sent, train_data, top_n=2)\n",
        "    format_top = []\n",
        "    for sent in top_sent:\n",
        "\n",
        "        text = sent['text']\n",
        "        label = make_label(sent)\n",
        "        format_top.append(f'{text}\\n{label}')\n",
        "\n",
        "    format_top = '\\n\\n'.join(format_top)\n",
        "    all_morphemes = []\n",
        "    for x in test_data[25]['tokens']:\n",
        "        for m in x['morphemes']:\n",
        "            if m in morph_gloss:\n",
        "                all_morphemes.append(f\"{m}: {', '.join(morph_gloss[m])}\")\n",
        "\n",
        "    array = '\\n'.join(all_morphemes)\n",
        "    rus_stem = '\\n'.join(retrieve_from_stem_dictionary(sent, stems))\n",
        "    prompt = prompt.format(segm_sent, translation, format_top, array, rus_stem)\n",
        "    return prompt"
      ],
      "metadata": {
        "id": "gol9MCYVV9t0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(format_segm_prompt(test_data[25], random_sample=True))"
      ],
      "metadata": {
        "id": "OroYfjduaJgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://gigachat.devices.sberbank.ru/api/v1/chat/completions\""
      ],
      "metadata": {
        "id": "e5J5FA2TbuWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_giga_answers_segmented(sentence, access_token, random_sample=False):\n",
        "    headers = {\n",
        "    'Content-Type': 'application/json',\n",
        "    'Accept': 'application/json',\n",
        "    'Authorization': f'Bearer {access_token}'}\n",
        "\n",
        "    payload = {\n",
        "    \"model\": \"GigaChat\",\n",
        "    \"messages\":\n",
        "            [{\"role\": \"system\",\n",
        "              \"content\": system_prompt}],\n",
        "    \"profanity_check\": True,\n",
        "    \"max_tokens\": 200}\n",
        "\n",
        "    if random_sample:\n",
        "        target_prompt = format_segm_prompt(sentence, random_sample)\n",
        "    else:\n",
        "        target_prompt = format_segm_prompt(sentence)\n",
        "    payload['messages'].append({'role': 'user',\n",
        "                                'content': target_prompt})\n",
        "\n",
        "    response = requests.request(\"POST\", url, headers=headers, data=json.dumps(payload), verify=False)\n",
        "    return response"
      ],
      "metadata": {
        "id": "YouBQNUbavy5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "token = userdata.get('Giga_TOKEN')"
      ],
      "metadata": {
        "id": "IK6dI1Dha8Ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_preds = []"
      ],
      "metadata": {
        "id": "W1l6UU8Tb7hG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "access_token = generate_access_token(token)\n",
        "last_token_time = time.time()\n",
        "token_lifetime = 25 * 60\n",
        "final_data = {}\n",
        "\n",
        "for sent in tqdm.tqdm(test_data):\n",
        "    if time.time() - last_token_time >= token_lifetime:\n",
        "        access_token = generate_access_token(token)\n",
        "        last_token_time = time.time()\n",
        "        print('\\nНовый токен\\n')\n",
        "\n",
        "    final_data = {}\n",
        "    segment = sent['text']\n",
        "    true_label = sent['label_tokens']\n",
        "\n",
        "    response = get_giga_answers_segmented(sent, access_token)\n",
        "    pred = response.json()['choices'][0]['message']['content']\n",
        "    final_data['sent'] = segment\n",
        "    final_data['true_label'] = make_label(sent)\n",
        "    final_data['pred_label'] = pred\n",
        "    all_preds.append(final_data)"
      ],
      "metadata": {
        "id": "f-Sw7aKrVyOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_sample_preds = []"
      ],
      "metadata": {
        "id": "d01f83T8vK4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "access_token = generate_access_token(token)\n",
        "last_token_time = time.time()\n",
        "token_lifetime = 25 * 60\n",
        "final_data = {}\n",
        "\n",
        "for sent in tqdm.tqdm(test_data):\n",
        "    if time.time() - last_token_time >= token_lifetime:\n",
        "        access_token = generate_access_token(token)\n",
        "        last_token_time = time.time()\n",
        "        print('\\nНовый токен\\n')\n",
        "\n",
        "    final_data = {}\n",
        "    segment = sent['text']\n",
        "    true_label = sent['label_tokens']\n",
        "\n",
        "    response = get_giga_answers_segmented(sent, access_token, random_sample=True)\n",
        "    pred = response.json()['choices'][0]['message']['content']\n",
        "    final_data['sent'] = segment\n",
        "    final_data['true_label'] = make_label(sent)\n",
        "    final_data['pred_label'] = pred\n",
        "    random_sample_preds.append(final_data)"
      ],
      "metadata": {
        "id": "mddWDPcLvJ-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_preds[100]"
      ],
      "metadata": {
        "id": "yarP6HwvjEPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu"
      ],
      "metadata": {
        "id": "YUoyWAsFp0PR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sacrebleu"
      ],
      "metadata": {
        "id": "werSxE-RqYu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sacrebleu.metrics import CHRF"
      ],
      "metadata": {
        "id": "uaLh2U5Qpx_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_chrf(true_label, pred_label):\n",
        "    \"\"\"\n",
        "    Вычисляет метрику chrF между true_label и pred_label.\n",
        "\n",
        "    :param true_label: Истинная метка (str).\n",
        "    :param pred_label: Предсказанная метка (str).\n",
        "    :return: Значение chrF (float).\n",
        "    \"\"\"\n",
        "    chrf = CHRF()\n",
        "    score = chrf.corpus_score([pred_label], [[true_label]])\n",
        "    return score.score"
      ],
      "metadata": {
        "id": "W8IMuSD4pwJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_chrf(all_preds[100]['true_label'], all_preds[100]['pred_label'])"
      ],
      "metadata": {
        "id": "_v9QF2lFtr7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_metric = 0\n",
        "for pred in all_preds:\n",
        "    final_metric += compute_chrf(pred['true_label'], pred['pred_label'])\n",
        "\n",
        "print(f'ChrF metric for segmentation: {final_metric/len(all_preds):.2f}')"
      ],
      "metadata": {
        "id": "sZG7wbMMt7ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_preds[317]"
      ],
      "metadata": {
        "id": "_P5JRk8vyU9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_sample_preds[317]"
      ],
      "metadata": {
        "id": "B0QYyJ_9x09F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_chrf(all_preds[317]['true_label'], all_preds[317]['pred_label'])"
      ],
      "metadata": {
        "id": "DQe2ARV7zBx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compute_chrf(random_sample_preds[317]['true_label'], random_sample_preds[317]['pred_label'])"
      ],
      "metadata": {
        "id": "PVh8o2iPx97k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_metric = 0\n",
        "for pred in random_sample_preds:\n",
        "    final_metric += compute_chrf(pred['true_label'], pred['pred_label'])\n",
        "\n",
        "print(f'ChrF metric for segmentation: {final_metric/len(all_preds):.2f}')"
      ],
      "metadata": {
        "id": "69-Y4y9JyRzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def compute_morpheme_metrics(true_labels, pred_labels):\n",
        "\n",
        "    def split_into_morphemes(label):\n",
        "        \"\"\"Разбивает текст на морфемы.\"\"\"\n",
        "        morphemes = []\n",
        "        for token in label.split():\n",
        "            morphemes.extend(token.split('-'))  # Разделяем по \"-\"\n",
        "        return morphemes\n",
        "\n",
        "    total_matches = 0\n",
        "    total_true_morphemes = 0\n",
        "    total_pred_morphemes = 0\n",
        "\n",
        "    for true_label, pred_label in zip(true_labels, pred_labels):\n",
        "        # Извлечение морфем\n",
        "        true_morphemes = split_into_morphemes(true_label)\n",
        "        pred_morphemes = split_into_morphemes(pred_label)\n",
        "\n",
        "        # Подсчет совпадений\n",
        "        true_counter = Counter(true_morphemes)\n",
        "        pred_counter = Counter(pred_morphemes)\n",
        "        common_morphemes = true_counter & pred_counter\n",
        "        matches = sum(common_morphemes.values())\n",
        "\n",
        "        # Обновление общих счетчиков\n",
        "        total_matches += matches\n",
        "        total_true_morphemes += len(true_morphemes)\n",
        "        total_pred_morphemes += len(pred_morphemes)\n",
        "\n",
        "    # Вычисление точности и полноты\n",
        "    precision = total_matches / total_pred_morphemes if total_pred_morphemes > 0 else 0\n",
        "    recall = total_matches / total_true_morphemes if total_true_morphemes > 0 else 0\n",
        "\n",
        "    # Вычисление F-меры\n",
        "    f_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    return round(precision, 3), round(recall, 3), round(f_score, 3)"
      ],
      "metadata": {
        "id": "46HyUhCV7dkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_all_preds = [x['true_label'] for x in all_preds]\n",
        "pred_all_preds = [x['pred_label'] for x in all_preds]\n",
        "compute_morpheme_metrics(true_all_preds, pred_all_preds)"
      ],
      "metadata": {
        "id": "RurIOX6b7gAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_labels = [x['true_label'] for x in random_sample_preds]\n",
        "pred_labels = [x['pred_label'] for x in random_sample_preds]\n",
        "compute_morpheme_metrics(true_labels, pred_labels)"
      ],
      "metadata": {
        "id": "bPQlzE-R79qP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "OBmnwxW5-nWN",
        "_QxwRYteHEgD",
        "vwg1jY6KlCpI"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}