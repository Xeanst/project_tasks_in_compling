{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "c_fdnVvlszwF",
        "EaYPNA5Fs3EF",
        "ssn_9S9zFbKd",
        "pNSyz_TwKm4D",
        "SD2AG0JJFo6x",
        "cw9bfRPyKQN0",
        "iyYs_7HBs7Ld",
        "JYQyoSrQs-Qd",
        "o6snl-N94Ded",
        "BuN6uplHTVrC"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Вход на Hugging Face"
      ],
      "metadata": {
        "id": "hhmbynbEP5pj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi, interpreter_login\n",
        "\n",
        "interpreter_login()\n",
        "username = HfApi().whoami()[\"name\"]\n",
        "REPO_NAME = f\"{username}/RuSENTNE_project\"\n",
        "\n",
        "print(f\"Homework repository: '{REPO_NAME}'\")"
      ],
      "metadata": {
        "id": "BU_dfyN-P-Co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Загрузка датасета"
      ],
      "metadata": {
        "id": "c_fdnVvlszwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://raw.githubusercontent.com/dialogue-evaluation/RuSentNE-evaluation/main/train_data.csv\n",
        "!wget -q https://raw.githubusercontent.com/dialogue-evaluation/RuSentNE-evaluation/main/validation_data_labeled.csv\n",
        "!wget -q https://raw.githubusercontent.com/dialogue-evaluation/RuSentNE-evaluation/main/final_data.csv"
      ],
      "metadata": {
        "id": "Ex1VlRq6tD0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "train = pd.read_csv('train_data.csv', sep='\\t')\n",
        "validation = pd.read_csv('validation_data_labeled.csv', sep='\\t')\n",
        "test = pd.read_csv('final_data.csv', sep='\\t')"
      ],
      "metadata": {
        "id": "z3PM8_0ttk-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Перефразирование обучающей выборки"
      ],
      "metadata": {
        "id": "EaYPNA5Fs3EF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загружаем модель для перефразирования."
      ],
      "metadata": {
        "id": "4cOVE6_bYjhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "MODEL_NAME = 'cointegrated/rut5-base-paraphraser'\n",
        "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
        "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
        "model.cuda();\n",
        "model.eval();\n",
        "\n",
        "def paraphrase(text, beams=5, grams=4, do_sample=False):\n",
        "    x = tokenizer(text, return_tensors='pt', padding=True).to(model.device)\n",
        "    max_size = int(x.input_ids.shape[1] * 1.5 + 10)\n",
        "    out = model.generate(**x, encoder_no_repeat_ngram_size=grams, num_beams=beams, max_length=max_size, do_sample=do_sample)\n",
        "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "print(paraphrase('Каждый охотник желает знать, где сидит фазан.'))\n",
        "# Все охотники хотят знать где фазан сидит.\n",
        "\n"
      ],
      "metadata": {
        "id": "u2BeontAt8Bz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверим, как работает модель, на примере из датасета"
      ],
      "metadata": {
        "id": "cOk5SI-KYpmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(paraphrase('Владислав первым заметил возгорание и начал тушить его.'))"
      ],
      "metadata": {
        "id": "wid21x6fvj5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Перефразируем предложения положительного и отрицательного класса из выборки:"
      ],
      "metadata": {
        "id": "eGXmB3diZD6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "progress_bar = tqdm(range(1863))\n",
        "\n",
        "new_sentences = []\n",
        "for i, sentence in enumerate(train[\"sentence\"]):\n",
        "  if train[\"label\"][i] != 0:\n",
        "    new_sent = paraphrase(sentence)\n",
        "    progress_bar.update(1)\n",
        "    if new_sent != sentence:\n",
        "      new_row = {\"sentence\": new_sent,\n",
        "                 \"entity\": train[\"entity\"][i],\n",
        "                  \"entity_tag\": train[\"entity_tag\"][i],\n",
        "                  \"entity_pos_start_rel\": train[\"entity_pos_start_rel\"][i],\n",
        "                  \"entity_pos_end_rel\": train[\"entity_pos_end_rel\"][i],\n",
        "                  \"label\": train[\"label\"][i]}\n",
        "      new_sentences.append(new_row)"
      ],
      "metadata": {
        "id": "gsPKC1iI5kCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создадим датафрейм с новыми предложениями."
      ],
      "metadata": {
        "id": "xLpiDRkeZhXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extension = pd.DataFrame(new_sentences, columns=['sentence', 'entity', 'entity_tag', 'entity_pos_start_rel', 'entity_pos_end_rel', 'label'])"
      ],
      "metadata": {
        "id": "0R2LibOy0nS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extension.head()"
      ],
      "metadata": {
        "id": "I4AbqD2V7jCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Сохраним полученные предложения в csv-файл"
      ],
      "metadata": {
        "id": "DrCSASzTZk08"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extension.to_csv('extension.csv', sep=\"\\t\", index=False)"
      ],
      "metadata": {
        "id": "ZPpEXCum4Y1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Иногда после перефразирования исходная сущность отсутствует в предложении"
      ],
      "metadata": {
        "id": "6SqVlXjZbbik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(extension[\"sentence\"][0])\n",
        "print(extension[\"entity\"][0])"
      ],
      "metadata": {
        "id": "7H-uFOGFazQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Предобработка данных"
      ],
      "metadata": {
        "id": "ssn_9S9zFbKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Расширение обучающей выборки"
      ],
      "metadata": {
        "id": "pNSyz_TwKm4D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Объединим изначальную обучающую выборку с перефразированными предложениями."
      ],
      "metadata": {
        "id": "M4yvv76nZ68W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extension = pd.read_csv('extension.csv', sep='\\t')\n",
        "train_extended = pd.concat([train, extension], ignore_index=True)\n",
        "train_extended.tail()"
      ],
      "metadata": {
        "id": "3PumsHJmFdn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Перемешаем полученную выборку"
      ],
      "metadata": {
        "id": "IxPYCecubovH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_extended = train_extended.sample(frac=1, random_state=22).reset_index(drop=True)\n",
        "train_extended.tail()"
      ],
      "metadata": {
        "id": "QDpUHd6wH6n0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Создание вопросов"
      ],
      "metadata": {
        "id": "SD2AG0JJFo6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pymorphy3"
      ],
      "metadata": {
        "id": "8F7aNGKmFo6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pymorphy3\n",
        "morph = pymorphy3.MorphAnalyzer()\n",
        "\n",
        "def question(df, sent='Как относятся к {}?', c='datv'):\n",
        "  sentences = []\n",
        "  for entity in df['entity'].values:\n",
        "    try:\n",
        "      dative_list = [pymorphy3.shapes.restore_capitalization(morph.parse(x)[0].inflect({c}).word, x) for x in entity.split()]\n",
        "      final_form = ' '.join(dative_list)\n",
        "    except AttributeError:\n",
        "      final_form = entity\n",
        "    sentences.append(sent.format(final_form))\n",
        "  return sentences\n",
        "\n",
        "train_extended['question'] = question(train_extended, 'Как относятся к {}?')\n",
        "validation['question'] = question(validation, 'Как относятся к {}?')\n",
        "test['question'] = question(test, 'Как относятся к {}?')\n",
        "validation.head()"
      ],
      "metadata": {
        "id": "sLxSuH5CFo61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Создание датасета"
      ],
      "metadata": {
        "id": "cw9bfRPyKQN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_dict = {-1: 0, 0: 1, 1: 2}\n",
        "train_extended['raw_label'] = train_extended[\"label\"]\n",
        "train_extended['label'] = train_extended[\"raw_label\"].map(label_dict)\n",
        "validation['raw_label'] = validation[\"label\"]\n",
        "validation['label'] = validation[\"raw_label\"].map(label_dict)"
      ],
      "metadata": {
        "id": "is4sk-mAGuO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasets transformers evaluate"
      ],
      "metadata": {
        "id": "gL00XGQhIJqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "dataset_dict = DatasetDict({\"train\": Dataset.from_pandas(train_extended),\n",
        "                            \"validation\": Dataset.from_pandas(validation),\n",
        "                            \"test\": Dataset.from_pandas(test)})"
      ],
      "metadata": {
        "id": "ZtRhraDjIQOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Обучение обычной модели"
      ],
      "metadata": {
        "id": "iyYs_7HBs7Ld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "SH_oF9YmtH4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузка модели"
      ],
      "metadata": {
        "id": "seHTNePr2uWg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"DeepPavlov/rubert-base-cased\", num_labels=3).to(device)"
      ],
      "metadata": {
        "id": "e1tDFG3Vo0Qc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Токенизация датасета"
      ],
      "metadata": {
        "id": "6w5be3EO2wvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"question\"], example[\"sentence\"])\n",
        "\n",
        "tokenized_dataset = dataset_dict.map(tokenize_function, batched=True)\n",
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "c0tPj_IvoJC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Параметры обучения"
      ],
      "metadata": {
        "id": "zYNb7q6i2zRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from transformers import DataCollatorWithPadding, TrainingArguments\n",
        "from transformers import Trainer\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "training_args = TrainingArguments(output_dir=f'{REPO_NAME}-base', push_to_hub=True, evaluation_strategy=\"epoch\")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    processing_class=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "KVVsLULUpsAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "loZ27ICSr-fs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub(f\"{REPO_NAME}-base\")"
      ],
      "metadata": {
        "id": "tx-_Qurgc9yV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Запишем предсказания модели на валидационной выборке"
      ],
      "metadata": {
        "id": "uMxWzpK63LLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def predict_labels(dataset):\n",
        "    output = trainer.predict(dataset)\n",
        "    logits, labels = output[:2]\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    reverse_label_dict = {v:k for k, v in label_dict.items()}\n",
        "    return [reverse_label_dict[x] for x in predictions]\n",
        "\n",
        "validation_predictions = predict_labels(tokenized_dataset[\"validation\"])\n",
        "print(len(validation_predictions))\n",
        "validation_predictions[:25]"
      ],
      "metadata": {
        "id": "ZGvuzUZzsFCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Оценим качество модели на валидационной выборке"
      ],
      "metadata": {
        "id": "FHP9Rx663UNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "def compute_metrics(preds, labels):\n",
        "    metric = evaluate.load(\"f1\")\n",
        "    return metric.compute(predictions=preds, references=labels, average=\"macro\")\n",
        "\n",
        "tokenized_dataset[\"validation\"] = tokenized_dataset[\"validation\"].add_column(\"predictions\", validation_predictions)\n",
        "f1_score_all = compute_metrics(tokenized_dataset[\"validation\"][\"predictions\"], tokenized_dataset[\"validation\"][\"raw_label\"])\n",
        "filtered_validation = tokenized_dataset[\"validation\"].filter(lambda example: example[\"raw_label\"]!=0)\n",
        "f1_score_filtered = compute_metrics(filtered_validation[\"predictions\"], filtered_validation[\"raw_label\"])\n",
        "print('Макро F1-мера{}{}.\\nМакро F1-мера для положительного и отрицательного классов{}{}.'.format(':'.ljust(54), round(f1_score_all['f1'], 2), ':'.ljust(10), round(f1_score_filtered['f1'], 2)))"
      ],
      "metadata": {
        "id": "i8-S9xnS0IW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теперь сохраним предсказания модели на тестовой выборке"
      ],
      "metadata": {
        "id": "lJJNPuri3aPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions = predict_labels(tokenized_dataset[\"test\"])\n",
        "print(len(test_predictions))\n",
        "test_predictions[1925:]"
      ],
      "metadata": {
        "id": "lVehYGNy0xAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(test_predictions).to_csv('RuSentNE_predictions_Trainer_base.zip', compression={'method': 'zip', 'archive_name': 'RuSentNE_predictions_Trainer_base.csv'}, index=False, header=False)"
      ],
      "metadata": {
        "id": "tlstzFlX02p0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "На тестовой выборке на платформе CodaLab для базовой модели было получено качество 54.21 по макро F1-мере для положительного и отрицательного классов, 64.77 - для трех классов."
      ],
      "metadata": {
        "id": "2oAY403mxhIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Обучение предобученной модели"
      ],
      "metadata": {
        "id": "JYQyoSrQs-Qd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "hpaSI6esMNun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попробуем обучить разные модели (предобученные на анализ тональности предложений) для анализа тональности именованных сущностей"
      ],
      "metadata": {
        "id": "K-dZcC223ibp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"blanchefort/rubert-base-cased-sentiment\")\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(\"blanchefort/rubert-base-cased-sentiment\", num_labels=3).to(device)\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"seara/rubert-base-cased-russian-sentiment\")\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(\"seara/rubert-base-cased-russian-sentiment\", num_labels=3).to(device)\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"r1char9/rubert-base-cased-russian-sentiment\")\n",
        "# model = AutoModelForSequenceClassification.from_pretrained(\"r1char9/rubert-base-cased-russian-sentiment\", num_labels=3).to(device)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny-sentiment-balanced\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"cointegrated/rubert-tiny-sentiment-balanced\", num_labels=3).to(device)"
      ],
      "metadata": {
        "id": "hv5xpazouvc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Токенизация"
      ],
      "metadata": {
        "id": "kxOeHh3W3y1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"question\"], example[\"sentence\"])\n",
        "\n",
        "tokenized_dataset = dataset_dict.map(tokenize_function, batched=True)\n",
        "tokenized_dataset"
      ],
      "metadata": {
        "id": "6EPTr4v6uzL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Параметры обучения"
      ],
      "metadata": {
        "id": "JfbGyDZA30r4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from transformers import DataCollatorWithPadding, TrainingArguments\n",
        "from transformers import Trainer\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "training_args = TrainingArguments(output_dir=f\"{REPO_NAME}-cointegrated\",push_to_hub=True, evaluation_strategy=\"epoch\", num_train_epochs=3)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    processing_class=tokenizer\n",
        ")"
      ],
      "metadata": {
        "id": "BSzAEyiPu7da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "E8Mx_XHb2kYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.push_to_hub(f\"{REPO_NAME}-seara\")"
      ],
      "metadata": {
        "id": "14dHsirA6iRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Предсказания на валидационной выборке"
      ],
      "metadata": {
        "id": "vJEj9rrA33tk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def predict_labels(dataset):\n",
        "    output = trainer.predict(dataset)\n",
        "    logits, labels = output[:2]\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    reverse_label_dict = {v:k for k, v in label_dict.items()}\n",
        "    return [reverse_label_dict[x] for x in predictions]\n",
        "\n",
        "validation_predictions = predict_labels(tokenized_dataset[\"validation\"])\n",
        "print(len(validation_predictions))\n",
        "print('{0}'.format(validation_predictions[:25]))"
      ],
      "metadata": {
        "id": "QJvDXNlqxVfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подсчёт качества"
      ],
      "metadata": {
        "id": "xeQ_oNW437DY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "\n",
        "def compute_metrics(preds, labels):\n",
        "    metric = evaluate.load(\"f1\")\n",
        "    return metric.compute(predictions=preds, references=labels, average=\"macro\")\n",
        "\n",
        "tokenized_dataset[\"validation\"] = tokenized_dataset[\"validation\"].add_column(\"predictions\", validation_predictions)\n",
        "f1_score_all = compute_metrics(tokenized_dataset[\"validation\"][\"predictions\"], tokenized_dataset[\"validation\"][\"raw_label\"])\n",
        "filtered_validation = tokenized_dataset[\"validation\"].filter(lambda example: example[\"raw_label\"]!=0)\n",
        "f1_score_filtered = compute_metrics(filtered_validation[\"predictions\"], filtered_validation[\"raw_label\"])\n",
        "print('Макро F1-мера{}{}.\\nМакро F1-мера для положительного и отрицательного классов{}{}.'.format(':'.ljust(54), round(f1_score_all['f1'], 2), ':'.ljust(10), round(f1_score_filtered['f1'], 2)))"
      ],
      "metadata": {
        "id": "XNMqXL8jMNuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Результаты для **r1char9/rubert-base-cased-russian-sentiment** на валидационной выборке:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Макро F1-мера:                                                     0.66.\n",
        "Макро F1-мера для положительного и отрицательного классов:         0.42.\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "9V74WnSrlEj6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Результаты для **seara/rubert-base-cased-russian-sentiment** на валидационной выборке:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Макро F1-мера:                                                     0.67.\n",
        "Макро F1-мера для положительного и отрицательного классов:         0.43.\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "MWeyGHQ1dGwr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Результаты для **blanchefort/rubert-base-cased-sentiment** на валидационной выборке:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Макро F1-мера:                                                     0.28.\n",
        "Макро F1-мера для положительного и отрицательного классов:         0.0.\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "sDi-bfz6Z7pR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Результаты для **cointegrated/rubert-tiny-sentiment-balanced** на валидационной выборке:\n",
        "\n",
        "```\n",
        "Макро F1-мера:                                                     0.55.\n",
        "Макро F1-мера для положительного и отрицательного классов:         0.39.\n",
        "```\n"
      ],
      "metadata": {
        "id": "emW7HSMF8KNY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Запишем предсказания моделей на тестовой выборке"
      ],
      "metadata": {
        "id": "Z921d8SU3-au"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions = predict_labels(tokenized_dataset[\"test\"])\n",
        "print(len(test_predictions))\n",
        "test_predictions[1925:]"
      ],
      "metadata": {
        "id": "rSE8oifGMNuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pd.Series(test_predictions).to_csv('RuSentNE_predictions_Trainer_2.zip', compression={'method': 'zip', 'archive_name': 'RuSentNE_predictions_Trainer.csv'}, index=False, header=False)\n",
        "# pd.Series(test_predictions).to_csv('RuSentNE_predictions_Trainer_3.zip', compression={'method': 'zip', 'archive_name': 'RuSentNE_predictions_Trainer.csv'}, index=False, header=False)\n",
        "#pd.Series(test_predictions).to_csv('RuSentNE_predictions_Trainer_4.zip', compression={'method': 'zip', 'archive_name': 'RuSentNE_predictions_Trainer.csv'}, index=False, header=False)\n",
        "pd.Series(test_predictions).to_csv('RuSentNE_predictions_Trainer_seara.zip', compression={'method': 'zip', 'archive_name': 'RuSentNE_predictions_Trainer.csv'}, index=False, header=False)"
      ],
      "metadata": {
        "id": "UN2yYS90y4bW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Промежуточные результаты моделей"
      ],
      "metadata": {
        "id": "o6snl-N94Ded"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "| Модель | F1(P,N)-macro| F1(P,N,0)-macro|\n",
        "|----------|----------|----------|\n",
        "| Модель без предобучения  | 54.21   | 64.77   |\n",
        "| seara/rubert-base-cased-russian-sentiment    |  52.37  |  63.13  |\n",
        "| r1char9/rubert-base-cased-russian-sentiment    | 8.97   | 32.82   |\n",
        "| blanchefort/rubert-base-cased-sentiment    | 14.05   | 36.41   |\n",
        "| cointegrated/rubert-tiny-sentiment-balanced    | 30.91   | 46.81   |"
      ],
      "metadata": {
        "id": "SOWeRvVw4HXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Предобученные модели на выборке без расширения"
      ],
      "metadata": {
        "id": "ngIPOtr9Utwe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Предобработка данных"
      ],
      "metadata": {
        "id": "BuN6uplHTVrC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pymorphy3\n",
        "!pip install -q datasets transformers evaluate"
      ],
      "metadata": {
        "id": "2mpb6mNeTVrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pymorphy3\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "morph = pymorphy3.MorphAnalyzer()\n",
        "\n",
        "def question(df):\n",
        "  sentences = []\n",
        "  for entity in df['entity'].values:\n",
        "    try:\n",
        "      dative_list = [pymorphy3.shapes.restore_capitalization(morph.parse(x)[0].inflect({'datv'}).word, x) for x in entity.split()]\n",
        "      final_form = ' '.join(dative_list)\n",
        "    except AttributeError:\n",
        "      final_form = entity\n",
        "    sentences.append(f'Как относятся к {final_form}?')\n",
        "  return sentences\n",
        "\n",
        "train['question'] = question(train)\n",
        "validation['question'] = question(validation)\n",
        "test['question'] = question(test)\n",
        "\n",
        "label_dict = {-1: 0, 0: 1, 1: 2}\n",
        "train['raw_label'] = train[\"label\"]\n",
        "train['label'] = train[\"raw_label\"].map(label_dict)\n",
        "validation['raw_label'] = validation[\"label\"]\n",
        "validation['label'] = validation[\"raw_label\"].map(label_dict)\n",
        "\n",
        "dataset_dict = DatasetDict({\"train\": Dataset.from_pandas(train),\n",
        "                            \"validation\": Dataset.from_pandas(validation),\n",
        "                            \"test\": Dataset.from_pandas(test)})"
      ],
      "metadata": {
        "id": "TgAm5VKcTVrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Обучение моделей"
      ],
      "metadata": {
        "id": "_GZqyATkUpAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "metadata": {
        "id": "N5JpKMLdUpAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "import os\n",
        "import numpy as np\n",
        "from transformers import DataCollatorWithPadding, TrainingArguments\n",
        "from transformers import Trainer\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "class MyTrainer():\n",
        "  def __init__(self, model, tokenizer, tokenized_dataset, training_arguments):\n",
        "    self.trainer = Trainer(\n",
        "        model,\n",
        "        training_arguments,\n",
        "        train_dataset=tokenized_dataset[\"train\"],\n",
        "        eval_dataset=tokenized_dataset[\"validation\"],\n",
        "        data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        "        processing_class=tokenizer\n",
        "    )\n",
        "\n",
        "  def training(self):\n",
        "    self.trainer.train()\n",
        "\n",
        "  def predict_labels(self, dataset):\n",
        "      output = self.trainer.predict(dataset)\n",
        "      logits, labels = output[:2]\n",
        "      predictions = np.argmax(logits, axis=-1)\n",
        "      reverse_label_dict = {v:k for k, v in label_dict.items()}\n",
        "      return [reverse_label_dict[x] for x in predictions]"
      ],
      "metadata": {
        "id": "FvtFCFHZXXLA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "results = defaultdict(lambda: defaultdict(float))\n",
        "model_names = [\"blanchefort/rubert-base-cased-sentiment\", \"seara/rubert-base-cased-russian-sentiment\", \"r1char9/rubert-base-cased-russian-sentiment\", \"cointegrated/rubert-tiny-sentiment-balanced\"]\n",
        "\n",
        "def tokenize_function(example):\n",
        "  return tokenizer(example[\"question\"], example[\"sentence\"])\n",
        "\n",
        "def compute_metrics(preds, labels):\n",
        "      metric = evaluate.load(\"f1\")\n",
        "      return metric.compute(predictions=preds, references=labels, average=\"macro\")\n",
        "\n",
        "def apply_to_model(model_name, training_arguments):\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3).to(device)\n",
        "  tokenized_dataset = dataset_dict.map(tokenize_function, batched=True)\n",
        "  trainer = MyTrainer(model, tokenizer, tokenized_dataset, training_arguments)\n",
        "  trainer.training()\n",
        "  validation_predictions = trainer.predict_labels(tokenized_dataset[\"validation\"])\n",
        "  tokenized_dataset[\"validation\"] = tokenized_dataset[\"validation\"].add_column(\"predictions\", validation_predictions)\n",
        "  f1_score_all = compute_metrics(tokenized_dataset[\"validation\"][\"predictions\"], tokenized_dataset[\"validation\"][\"raw_label\"])\n",
        "  filtered_validation = tokenized_dataset[\"validation\"].filter(lambda example: example[\"raw_label\"]!=0)\n",
        "  f1_score_filtered = compute_metrics(filtered_validation[\"predictions\"], filtered_validation[\"raw_label\"])\n",
        "  results[model_name][\"f1_PN0\"] = round(f1_score_all['f1'], 2)\n",
        "  results[model_name][\"f1_PN\"] = round(f1_score_filtered['f1'], 2)\n",
        "  test_predictions = trainer.predict_labels(tokenized_dataset[\"test\"])\n",
        "  pd.Series(test_predictions).to_csv('RuSentNE_predictions_{}.zip'.format(model_name[:4]), compression={'method': 'zip', 'archive_name': 'RuSentNE_predictions_Trainer.csv'}, index=False, header=False)\n",
        "\n",
        "for model_name in model_names:\n",
        "  # apply_to_model(model_name, TrainingArguments(output_dir='./results', eval_strategy=\"epoch\", num_train_epochs=3, weight_decay=0.01, learning_rate=1e-6))\n",
        "  apply_to_model(model_name, TrainingArguments(output_dir='./results', eval_strategy=\"epoch\", num_train_epochs=3))"
      ],
      "metadata": {
        "id": "6tgijmLUUpAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Результаты"
      ],
      "metadata": {
        "id": "1jaUdNRanUxG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Качество моделей при дефолтных значениях параметров weight_decay (0) и learning_rate (5e-5)**"
      ],
      "metadata": {
        "id": "8Rwzu1oYju0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "results_for_default_training_args = {\"blanchefort/rubert-base-cased-sentiment\": {\"f1_PN0\": 0.36, \"f1_PN\": 0.11},\n",
        "                                     \"seara/rubert-base-cased-russian-sentiment\": {\"f1_PN0\": 0.69, \"f1_PN\": 0.46},\n",
        "                                     \"r1char9/rubert-base-cased-russian-sentiment\t\": {\"f1_PN0\": 0.67, \"f1_PN\": 0.45},\n",
        "                                     \"cointegrated/rubert-tiny-sentiment-balanced\": {\"f1_PN0\": 0.54, \"f1_PN\": 0.33},\n",
        "}\n",
        "df_base_default = pd.DataFrame(results_for_default_training_args)\n",
        "df_base_default = df_base_default.transpose()\n",
        "f1_PN0_base = [0.28, 0.67, 0.66, 0.55]\n",
        "df_base_default['compare to base model\\'s f1_PN0'] = f1_PN0_base\n",
        "f1_PN_base = [0.0, 0.43, 0.42, 0.39]\n",
        "df_base_default['compare to base model\\'s  f1_PN'] = f1_PN_base\n",
        "df_base_default = df_base_default.iloc[:, [0, 2, 1, 3]]\n",
        "\n",
        "df_base_default"
      ],
      "metadata": {
        "id": "F103woVJjxTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Качество моделей при заданных значениях параметров weight_decay (0.01) и learning_rate (1e-6)** -- ЗНАЧИТЕЛЬНОЕ УХУДШЕНИЕ"
      ],
      "metadata": {
        "id": "HBzkYkasjmkj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_default_set = pd.DataFrame(results)\n",
        "df_default_set = df_default_set.transpose()\n",
        "f1_PN0_default = [0.36, 0.69, 0.67, 0.54]\n",
        "df_default_set['compare to default f1_PN0'] = f1_PN0_default\n",
        "f1_PN_default = [0.11, 0.46, 0.45, 0.33]\n",
        "df_default_set['compare to default f1_PN'] = f1_PN_default\n",
        "df_default_set = df_default_set.iloc[:, [0, 2, 1, 3]]\n",
        "\n",
        "df_default_set"
      ],
      "metadata": {
        "id": "_soU2nmze-V4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Результаты на CodaLab\n",
        "\n",
        "| Модель | F1(P,N)-macro| F1(P,N,0)-macro|\n",
        "|----------|----------|----------|\n",
        "| seara/rubert-base-cased-russian-sentiment    |  52.37  |  63.13  |\n",
        "| cointegrated/rubert-tiny-sentiment-balanced    | 30.91   | 46.81   |"
      ],
      "metadata": {
        "id": "9Eb_lS4cIIMY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ансамбль?"
      ],
      "metadata": {
        "id": "YSlMGV5UtBp1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_Trainer(model):\n",
        "  trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    processing_class=tokenizer\n",
        ")\n",
        "  return trainer"
      ],
      "metadata": {
        "id": "TpxQ9Kf2_LtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer1 = AutoTokenizer.from_pretrained(\"kravmar/RuSENTNE_project-base\")\n",
        "model1 = AutoModelForSequenceClassification.from_pretrained(\"kravmar/RuSENTNE_project-base\")\n",
        "trainer1 = create_Trainer(model1)\n",
        "tokenizer2 = AutoTokenizer.from_pretrained(\"kravmar/RuSENTNE_project-seara\")\n",
        "model2 = AutoModelForSequenceClassification.from_pretrained(\"kravmar/RuSENTNE_project-seara\")\n",
        "trainer2 = create_Trainer(model2)\n",
        "tokenizer3 = AutoTokenizer.from_pretrained(\"kravmar/RuSENTNE_project-cointegrated\")\n",
        "model3 = AutoModelForSequenceClassification.from_pretrained(\"kravmar/RuSENTNE_project-cointegrated\")\n",
        "trainer3 = create_Trainer(model3)\n",
        "models = [trainer2, trainer1, trainer3]\n",
        "tokenizers = [tokenizer2, tokenizer1, tokenizer3]\n",
        "\n",
        "def tokenize_function(example, tokenizer):\n",
        "    return tokenizer(example[\"question\"], example[\"sentence\"])\n",
        "\n",
        "def majority_voting(dataset, models, tokenizers):\n",
        "    predictions = []\n",
        "    reverse_label_dict = {v:k for k, v in label_dict.items()}\n",
        "    for i, model in enumerate(models):\n",
        "        tokenizer = tokenizers[i]\n",
        "        tokenized_dataset = [tokenize_function(i, tokenizer) for i in dataset]\n",
        "        #tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "        output = model.predict(tokenized_dataset)\n",
        "        logits, labels = output[:2]\n",
        "        preds = np.argmax(logits, axis=-1)\n",
        "        predictions.append(preds)\n",
        "    predictions = np.array(predictions)\n",
        "    final_predictions = [np.bincount(pred).argmax() for pred in predictions.T]\n",
        "    return [reverse_label_dict[x] for x in final_predictions]\n",
        "\n",
        "validation_predictions = majority_voting(dataset_dict['validation'], models, tokenizers)\n",
        "print(len(validation_predictions))\n",
        "validation_predictions[:25]"
      ],
      "metadata": {
        "id": "rHhfH7gJrVBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset[\"validation\"] = tokenized_dataset[\"validation\"].remove_columns([\"predictions\"])\n",
        "tokenized_dataset[\"validation\"] = tokenized_dataset[\"validation\"].add_column(\"predictions\", validation_predictions)\n",
        "f1_score_all = compute_metrics(tokenized_dataset[\"validation\"][\"predictions\"], tokenized_dataset[\"validation\"][\"raw_label\"])\n",
        "filtered_validation = tokenized_dataset[\"validation\"].filter(lambda example: example[\"raw_label\"]!=0)\n",
        "f1_score_filtered = compute_metrics(filtered_validation[\"predictions\"], filtered_validation[\"raw_label\"])\n",
        "print('Макро F1-мера{}{}.\\nМакро F1-мера для положительного и отрицательного классов{}{}.'.format(':'.ljust(54), round(f1_score_all['f1'], 2), ':'.ljust(10), round(f1_score_filtered['f1'], 2)))"
      ],
      "metadata": {
        "id": "GrpxbJdFEsT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions = majority_voting(dataset_dict['test'], models, tokenizers)\n",
        "print(len(test_predictions))\n",
        "test_predictions[1925:]"
      ],
      "metadata": {
        "id": "0m8eFBWXFCYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(test_predictions).to_csv('RuSentNE_predictions_Trainer_ensemble.zip', compression={'method': 'zip', 'archive_name': 'RuSentNE_predictions_Trainer_ensemble.csv'}, index=False, header=False)"
      ],
      "metadata": {
        "id": "W-PvUeFoGtoE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Результаты на CodaLab\n",
        "\n",
        "| Модель | F1(P,N)-macro| F1(P,N,0)-macro|\n",
        "|----------|----------|----------|\n",
        "| Ансамбль из трёх моделей  | 50.73     | 62.34   |"
      ],
      "metadata": {
        "id": "6raOt38RHhOq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Более сложный ансамбль с помощью Stacking"
      ],
      "metadata": {
        "id": "uPO8Xm5iwsjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def get_model_predictions(models, tokenizer, dataset):\n",
        "\n",
        "    predictions = []\n",
        "    for i, model in enumerate(models):\n",
        "        tokenized_dataset = dataset.map(lambda example: tokenize_function(example, tokenizer[i]), batched=True)\n",
        "\n",
        "        output = model.predict(tokenized_dataset)\n",
        "        logits, labels = output[:2]\n",
        "        preds = np.argmax(logits, axis=-1)\n",
        "        predictions.append(preds)\n",
        "\n",
        "    return np.stack(predictions, axis=1)\n",
        "\n",
        "def stacking_ensemble(models, tokenizers, dataset, labels):\n",
        "    preds = get_model_predictions(models, tokenizers, dataset)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(preds, labels, test_size=0.2, random_state=42)\n",
        "    meta_model = LogisticRegression(class_weight='balanced')\n",
        "    meta_model.fit(X_train, y_train)\n",
        "    y_pred = meta_model.predict(X_val)\n",
        "\n",
        "    accuracy = accuracy_score(y_val, y_pred)\n",
        "    print(f\"Meta-model accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "    return meta_model\n",
        "\n",
        "def predict_stacking_ensemble(meta_model, models, tokenizers, dataset):\n",
        "    logits = get_model_predictions(models, tokenizers, dataset)\n",
        "    final_predictions = meta_model.predict(logits)\n",
        "    reverse_label_dict = {v:k for k, v in label_dict.items()}\n",
        "    return [reverse_label_dict[x] for x in final_predictions]\n"
      ],
      "metadata": {
        "id": "sRoTaJOPUsX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_models = [model1, model2, model3]\n",
        "tokenizers = [tokenizer1, tokenizer2, tokenizer3]\n",
        "ensemble = stacking_ensemble(models, tokenizers, dataset_dict['train'], dataset_dict['train']['label'])"
      ],
      "metadata": {
        "id": "SbaAg_t2VTZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "validation_predictions = predict_stacking_ensemble(ensemble, models, tokenizers, dataset_dict['validation'])\n",
        "print(len(validation_predictions))\n",
        "validation_predictions[:25]"
      ],
      "metadata": {
        "id": "6sXd6_xFX1iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset[\"validation\"] = tokenized_dataset[\"validation\"].remove_columns([\"predictions\"])\n",
        "tokenized_dataset[\"validation\"] = tokenized_dataset[\"validation\"].add_column(\"predictions\", validation_predictions)\n",
        "f1_score_all = compute_metrics(tokenized_dataset[\"validation\"][\"predictions\"], tokenized_dataset[\"validation\"][\"raw_label\"])\n",
        "filtered_validation = tokenized_dataset[\"validation\"].filter(lambda example: example[\"raw_label\"]!=0)\n",
        "f1_score_filtered = compute_metrics(filtered_validation[\"predictions\"], filtered_validation[\"raw_label\"])\n",
        "print('Макро F1-мера{}{}.\\nМакро F1-мера для положительного и отрицательного классов{}{}.'.format(':'.ljust(54), round(f1_score_all['f1'], 2), ':'.ljust(10), round(f1_score_filtered['f1'], 2)))"
      ],
      "metadata": {
        "id": "_uk1Ivi9YxIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_predictions = predict_stacking_ensemble(ensemble, models, tokenizers, dataset_dict['test'])\n",
        "print(len(test_predictions))\n",
        "test_predictions[1925:]"
      ],
      "metadata": {
        "id": "zU6jL1wtY1Q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.Series(test_predictions).to_csv('RuSentNE_predictions_Trainer_ensemble_stack.zip', compression={'method': 'zip', 'archive_name': 'RuSentNE_predictions_Trainer_ensemble.csv'}, index=False, header=False)"
      ],
      "metadata": {
        "id": "Wlb5gbS3ZJcy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Результаты на CodaLab\n",
        "\n",
        "| Модель | F1(P,N)-macro| F1(P,N,0)-macro|\n",
        "|----------|----------|----------|\n",
        "| Ансамбль из трёх моделей  | 50.62     | 62.19   |"
      ],
      "metadata": {
        "id": "89sYwHaekP5v"
      }
    }
  ]
}