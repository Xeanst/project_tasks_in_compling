{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">–í–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å–ª–æ–≤</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ê–ª–≥–æ—Ä–∏—Ç–º word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–ª—è —Ä–∞–±–æ—Ç—ã —Å –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω—É–∂–µ–Ω –Ω–µ–∫–æ—Ç–æ—Ä—ã–π —Å–ø–æ—Å–æ–± –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ ‚Äî –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –≤ –≤–∏–¥–µ –Ω–∞–±–æ—Ä–∞ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤. –†–∞–Ω–µ–µ –º—ã —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–ª–∏ –º–µ—Ç–æ–¥—ã \"–º–µ—à–æ–∫ —Å–ª–æ–≤\" –∏ TF-IDF, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∑–≤–æ–ª—è—é—Ç –ø–æ–ª—É—á–∏—Ç—å —Å—á–µ—Ç–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä—ã.\n",
    "\n",
    "‚ùå –ü—Ä–∏–∑–Ω–∞–∫–æ–≤–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ –∏–º–µ–µ—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å, —Ä–∞–≤–Ω—É—é –º–æ—â–Ω–æ—Å—Ç–∏ —Å–ª–æ–≤–∞—Ä—è,\n",
    "—Ç.–µ. —Ç—ã—Å—è—á–∏ –∏ –¥–µ—Å—è—Ç–∫–∏ —Ç—ã—Å—è—á. –≠—Ç–∞ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —Ä–∞—Å—Ç—ë—Ç –≤–º–µ—Å—Ç–µ —Å —Ä–æ—Å—Ç–æ–º —Å–ª–æ–≤–∞—Ä—è.\n",
    "\n",
    "‚ùå –ù–∏–∫–∞–∫ –Ω–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –±–ª–∏–∑–æ—Å—Ç—å —Å–ª–æ–≤, –≤—Å–µ –≤–µ–∫—Ç–æ—Ä—ã –æ–¥–∏–Ω–∞–∫–æ–≤–æ –¥–∞–ª–µ–∫–∏ –¥—Ä—É–≥ –æ—Ç –¥—Ä—É–≥–∞ –≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–µ–≥–æ–¥–Ω—è –º—ã –ø–æ–∑–Ω–∞–∫–æ–º–∏–º—Å—è —Å–æ —Å–∂–∞—Ç—ã–º–∏ –≤–µ–∫—Ç–æ—Ä–Ω—ã–º–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è–º–∏ —Å–ª–æ–≤ (word embeddings).\n",
    "\n",
    "‚úÖ –°—Ç—Ä–æ—è—Ç—Å—è –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞—Ö —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –ø–æ—Ä—è–¥–∫–∞ –¥–µ—Å—è—Ç–∫–æ–≤ –∏ —Å–æ—Ç–µ–Ω.\n",
    "\n",
    "‚úÖ –î–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –±–ª–∏–∑–∫–∏—Ö —Å–ª–æ–≤ –±–ª–∏–∑–∫–∏ –∫–∞–∫ –≤–µ–∫—Ç–æ—Ä—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π –º–µ—Ä–µ)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://i.ibb.co/NVy0YMs/w2v-example.png\" width=\"350\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2vec** ‚Äî –≥—Ä—É–ø–ø–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π —Å–ª–æ–≤.\n",
    "\n",
    "[T. Mikolov, K. Chen, G. Corrado, J. Dean(2013) Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "\n",
    "–î–≤–µ –º–æ–¥–µ–ª–∏:\n",
    "- **Continuous Bag-of-Words (CBOW)** –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Ç–µ–∫—É—â–µ–µ —Å–ª–æ–≤–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–æ–∫—Ä—É–∂–∞—é—â–∏—Ö —Å–ª–æ–≤);\n",
    "\n",
    "- **Skip-gram** –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç (–æ–∫—Ä—É–∂–∞—é—â–∏–µ —Å–ª–æ–≤–∞) –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–µ–≥–æ —Å–ª–æ–≤–∞."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://i.ibb.co/WWfJ20B/cbow-skipgram.png\" width=\"750\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–±–µ –º–æ–¥–µ–ª–∏ —Ä–µ—à–∞—é—Ç –∑–∞–¥–∞—á—É –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å –ø–æ–º–æ—â—å—é –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω–∞ —Å –æ–¥–Ω–∏–º —Å–∫—Ä—ã—Ç—ã–º —Å–ª–æ–µ–º. –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ –≤ —Å–ª–æ–≤–∞—Ä–µ –º—ã –¥–æ–ª–∂–Ω—ã –ø–æ–ª—É—á–∏—Ç—å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å, —á—Ç–æ –æ–Ω–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –¥–∞–Ω–Ω–æ–º—É –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –∏–ª–∏ —Å–∞–º–æ —è–≤–ª—è–µ—Ç—Å—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º.\n",
    "\n",
    "–ü—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤. –ò–º–µ–Ω–Ω–æ –æ–Ω–∏ –≤–ø–æ—Å–ª–µ–¥—Å—Ç–≤–∏–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∫–∞–∫ –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è —Å–ª–æ–≤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://i.ibb.co/JsZrX2C/architecture.png\" width=\"350\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–†–∞—Å—Å–º–æ—Ç—Ä–∏–º –ø–æ–ª—É—á–µ–Ω–∏–µ —Å–∂–∞—Ç—ã—Ö –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ –º–æ–¥–µ–ª–∏ Skip-gram, –ø–æ—Å–∫–æ–ª—å–∫—É –æ–Ω–∞ —á–∞—â–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –Ω–∞ –ø—Ä–∞–∫—Ç–∏–∫–µ –∏ –¥–∞–µ—Ç –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –¢–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∞—è —á–∞—Å—Ç—å"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—É—Å—Ç—å –Ω–∞—à –∫–æ—Ä–ø—É—Å —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ –æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è. –†–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ ‚Äî 1, —Å–º–æ—Ç—Ä–∏–º –Ω–∞ –æ–¥–Ω–æ —Å–ª–æ–≤–æ —Å–ª–µ–≤–∞ –∏ –æ–¥–Ω–æ —Å–ª–æ–≤–æ —Å–ø—Ä–∞–≤–∞.\n",
    "\n",
    "üìå –ö–∞–∫–∏–º –±—É–¥–µ—Ç —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://i.ibb.co/w427P7D/corpus1.png\" width=\"700\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) –ù–∞ –≤—Ö–æ–¥ –º—ã –ø–æ–¥–∞–µ–º one-hot –≤–µ–∫—Ç–æ—Ä $x$ –¥–ª—è —Å–ª–æ–≤–∞ \"passes\" —Ä–∞–∑–º–µ—Ä–∞ $V$.\n",
    "\n",
    "2) –í—Ö–æ–¥–Ω–æ–π –≤–µ–∫—Ç–æ—Ä —É–º–Ω–æ–∂–∞–µ—Ç—Å—è –Ω–∞ –º–∞—Ç—Ä–∏—Ü—É —Å–æ —Å–ª—É—á–∞–π–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ $W_{input}$ —Ä–∞–∑–º–µ—Ä–∞ $V \\times N$, –≥–¥–µ $V$ ‚Äî —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è, $N$ ‚Äî —Ä–∞–∑–º–µ—Ä –±—É–¥—É—â–∏—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, –∫–æ—Ç–æ—Ä—ã–π —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ (–¥–ª—è –±–æ–ª—å—à–∏—Ö –º–æ–¥–µ–ª–µ–π 300). –ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è —Å—Ç—Ä–æ–∫–∏ —ç—Ç–æ–π –º–∞—Ç—Ä–∏—Ü—ã –±—É–¥—É—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å —Å–∂–∞—Ç—ã–º –≤–µ–∫—Ç–æ—Ä–∞–º —Å–ª–æ–≤.\n",
    "\n",
    "3) –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –ø–µ—Ä–µ–º–Ω–æ–∂–µ–Ω–∏—è –ø–æ–ª—É—á–∞–µ–º –≤–µ–∫—Ç–æ—Ä $h$ —Ä–∞–∑–º–µ—Ä–∞ $N$. –≠—Ç–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –æ —Å–ª–æ–≤–µ \"passes\".\n",
    "\n",
    "4) –ü–æ–ª—É—á–∏–≤—à–∏–π—Å—è —Ä–∞–Ω–µ–µ –≤–µ–∫—Ç–æ—Ä $h$ —É–º–Ω–æ–∂–∞–µ—Ç—Å—è –Ω–∞ –Ω–æ–≤—É—é –º–∞—Ç—Ä–∏—Ü—É —Å–æ —Å–ª—É—á–∞–π–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ $W_{output}^T$ —Ä–∞–∑–º–µ—Ä–∞ $N \\times V$.\n",
    "\n",
    "5) –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –ø–µ—Ä–µ–º–Ω–æ–∂–µ–Ω–∏—è –ø–æ–ª—É—á–∞–µ–º –Ω–µ–∫–æ—Ç–æ—Ä—ã–π –≤–µ–∫—Ç–æ—Ä —Ä–∞–∑–º–µ—Ä–∞ $V$. –ü—Ä–∏–º–µ–Ω—è–µ–º –∫ –Ω–µ–º—É —Ñ—É–Ω–∫—Ü–∏—é –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ softmax, –ø–æ—Å–ª–µ —á–µ–≥–æ –ø–æ–ª—É—á–∞–µ–º –Ω–æ–≤—ã–π –≤–µ–∫—Ç–æ—Ä  $y_{pred}$. –û–Ω —Å–æ–¥–µ—Ä–∂–∏—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ –±—ã—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –≤—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ–≤–∞ \"passes\".\n",
    "\n",
    "6) –ü–æ–¥—Å—á–∏—Ç–∞–µ–º –æ—à–∏–±–∫—É –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π. –î–ª—è —ç—Ç–æ–≥–æ –∏–∑ –≤–µ–∫—Ç–æ—Ä–∞ $y_{pred}$ –≤—ã—á—Ç–µ–º –≤–µ–∫—Ç–æ—Ä –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤ $y_{true}$.\n",
    "\n",
    "7) –ó–∞—Ç–µ–º –º—ã —Å–∫–ª–∞–¥—ã–≤–∞–µ–º –¥–≤–∞ –ø–æ–ª—É—á–∏–≤—à–∏—Ö—Å—è –≤–µ–∫—Ç–æ—Ä–∞ –∏ –ø–æ–ª—É—á–∞–µ–º —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–Ω—É—é –æ—à–∏–±–∫—É."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://i.ibb.co/rvVdjVZ/skipgram.png\" width=\"900\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–£–º–Ω–æ–∂–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∞ –Ω–∞ –≤–µ—Å–∞ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://i.ibb.co/L140YB7/sg-f-1.png\" width=\"650\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–£–º–Ω–æ–∂–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∞ (—Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è) –Ω–∞ –≤–µ—Å–∞ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://i.ibb.co/b7mkJz6/sg-f-2.png\" width=\"650\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ–¥—Å—á–µ—Ç —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –æ—à–∏–±–∫–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://i.ibb.co/JCvqJWN/sg-b-1.png\" width=\"750\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ–¥—Å—á–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –¥–ª—è $W_{input}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://i.ibb.co/cT1fKCx/sg-b-2.png\" width=\"900\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ–¥—Å—á–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –¥–ª—è $W^T_{output}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://i.ibb.co/8jBD7TP/sg-b-3.png\" width=\"550\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ (–≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://i.ibb.co/X4bMqZf/sg-b-4.png\" width=\"900\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå –í —á–µ–º –ø—Ä–æ–±–ª–µ–º–∞ —Ç–∞–∫–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —á–∞—Å—Ç—å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import decomposition\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (10,8)\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–ø—Ä–µ–¥–µ–ª–∏–º –Ω–∞—à –Ω–µ–±–æ–ª—å—à–æ–π –∫–æ—Ä–ø—É—Å."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'drink milk',\n",
    "    'drink cold water',\n",
    "    'drink cold cola',\n",
    "    'drink juice',\n",
    "    'drink cola',\n",
    "    'eat bacon',\n",
    "    'eat mango',\n",
    "    'eat cherry',\n",
    "    'eat apple',\n",
    "    'juice with sugar',\n",
    "    'cola with sugar',\n",
    "    'mango is fruit',\n",
    "    'apple is fruit',\n",
    "    'cherry is fruit',\n",
    "    'Berlin is Germany',\n",
    "    'Boston is USA',\n",
    "    'Mercedes from Germany',\n",
    "    'Mercedes is a car',\n",
    "    'Ford from USA',\n",
    "    'Ford is a car'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–∞–ø–∏—à–µ–º —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è. –†–∞–∑–¥–µ–ª–∏–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –ø–æ —Å–ª–æ–≤–∞–º (`.split()`) –∏ —É–¥–∞–ª–∏–º —Å–ª–æ–ø-—Å–ª–æ–≤–∞ (`not in stop_words`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = [token for token in text.split() if token not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–∏–º–µ–Ω–∏–º –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É –∫–æ –≤—Å–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º –∫–æ—Ä–ø—É—Å–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [preprocess(sentence) for sentence in corpus]\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–æ–∑–¥–∞–¥–∏–º —Å–ª–æ–≤–∞—Ä—å —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ `vocabulary` –∫–ª–∞—Å—Å–∞ `Counter`.\n",
    "\n",
    "–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ –æ—Å—É—â–µ—Å—Ç–≤–ª—è–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–∞ `update` (—Å–º. [–ø—Ä–∏–º–µ—Ä](https://www.geeksforgeeks.org/python-counter-update-method/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = Counter()\n",
    "for sentence in corpus:\n",
    "  vocabulary.update(sentence)\n",
    "print(f'–°–ª–æ–≤–∞—Ä—å —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤:\\n{vocabulary}')\n",
    "print(f'–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤:{len(vocabulary)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C–æ–∑–¥–∞–¥–∏–º —Å–ª–æ–≤–∞—Ä—å —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏ `word2id`: –∫–∞–∂–¥–æ–º—É —Å–ª–æ–≤—É (–∫–ª—é—á—É) –ø—Ä–∏—Å–≤–æ–∏–º –µ–≥–æ –ø–æ—Ä—è–¥–∫–æ–≤—ã–π –Ω–æ–º–µ—Ä (–∑–Ω–∞—á–µ–Ω–∏–µ) ‚Äî —Ä–∞–∑–º–µ—Ä —Ç–µ–∫—É—â–µ–≥–æ —Å–ª–æ–≤–∞—Ä—è."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {}\n",
    "for word in vocabulary:\n",
    "  word2id[word] = len(word2id)\n",
    "# word2id = {word: i for i, word in enumerate(vocabulary)}\n",
    "print(f'–°–ª–æ–≤–∞—Ä—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –∏ –∏–Ω–¥–µ–∫—Å–æ–≤:\\n{word2id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ skip-gram –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –Ω–∞ –≤—Ö–æ–¥ –ø–æ–¥–∞–≤–∞—Ç—å —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ –∏ –Ω–∞ –≤—ã—Ö–æ–¥–µ –ø–æ–ª—É—á–∞—Ç—å –æ–¥–Ω–æ –∏–∑ —Å–ª–æ–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤ –∑–∞–¥–∞–Ω–Ω–æ–º –¥–∏–∞–ø–∞–∑–æ–Ω–µ (`window`). –°–æ–∑–¥–∞–¥–∏–º –¥–∞—Ç–∞—Ñ—Ä–µ–π–º, –≥–¥–µ –±—É–¥–µ—Ç –¥–≤–µ –∫–æ–ª–æ–Ω–∫–∏: `Input` ‚Äî —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ, `Output` ‚Äî —Å–ª–æ–≤–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(corpus, window):\n",
    "    columns = ['Input', 'Output']\n",
    "    result = pd.DataFrame(columns = columns)\n",
    "    for sentence in corpus:\n",
    "        for i, input_word in enumerate(sentence):\n",
    "            for n in range(1, window+1):\n",
    "                # —Å–º–æ—Ç—Ä–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–ª–µ–≤–∞\n",
    "                if (i-n)>=0:\n",
    "                    output_word = sentence[i-n]\n",
    "                    result.loc[len(result)] = [input_word,output_word]\n",
    "                # —Å–º–æ—Ç—Ä–∏–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–ø—Ä–∞–≤–∞\n",
    "                if (i+n)<len(sentence):\n",
    "                    output_word = sentence[i+n]\n",
    "                    result.loc[len(result)] = [input_word,output_word]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_emb = prepare_data(corpus, window = 2)\n",
    "print(f\"Number of rows: {train_emb.shape[0]}\")\n",
    "train_emb.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í –ø–æ–ª—É—á–∏–≤—à–µ–º—Å—è –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ –∑–∞–º–µ–Ω–∏–º —Å–ª–æ–≤–∞ –Ω–∞ –∏—Ö –∏–Ω–¥–µ–∫—Å—ã –∏–∑ —Å–ª–æ–≤–∞—Ä—è `word2id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_emb.Input = train_emb.Input.map(word2id)\n",
    "train_emb.Output = train_emb.Output.map(word2id)\n",
    "train_emb.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–£–∫–∞–∂–µ–º —Ä–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏ —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 5\n",
    "vocab_size = len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ—Å–∫–æ–ª—å–∫—É –≤ –Ω–∞—à–µ–º –∫–æ—Ä–ø—É—Å–µ –º–∞–ª–æ –ø—Ä–∏–º–µ—Ä–æ–≤, –Ω–∞–º –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –≤–µ—Å–∞ –Ω–µ–±–æ–ª—å—à–∏–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏. –ú–∞—Ç—Ä–∏—Ü–∞ `W_input` —Å–æ–¥–µ—Ä–∂–∏—Ç –≤–µ—Å–∞ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è, –º–∞—Ç—Ä–∏—Ü–∞ `W_output` ‚Äî –≤–µ—Å–∞ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è.\n",
    "\n",
    "`Variable` ‚Äî —ç—Ç–æ –Ω–∞–¥—Å—Ç—Ä–æ–π–∫–∞ –Ω–∞–¥ `Tensor`, –∫–æ—Ç–æ—Ä–∞—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–¥—Å—á–∏—Ç—ã–≤–∞—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –¥–ª—è –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –æ—à–∏–±–∫–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "initrange = 0.5 / embedding_dim\n",
    "W_input = Variable(torch.randn(vocab_size, embedding_dim).uniform_(-initrange, initrange).float(),\n",
    "                   requires_grad=True) # V x N\n",
    "W_output = Variable(torch.randn(embedding_dim, vocab_size).uniform_(-initrange, initrange).float(),\n",
    "                    requires_grad=True) # N x V\n",
    "print(f\"W_input:\\n{W_input}\")\n",
    "print(f\"Shape of W_input: {W_input.shape}\")\n",
    "print(f\"W_output:\\n{W_output}\")\n",
    "print(f\"Shape of W_output:{W_output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- –û–ø—Ä–µ–¥–µ–ª–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –∏ —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è\n",
    "\n",
    "- –ó–∞–¥–∞–¥–∏–º —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å –¥–ª—è –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "learning_rate = 0.2\n",
    "lr_decay = 0.99\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss_hist = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–∞–ø–∏—à–µ–º —Ü–∏–∫–ª –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —ç–ø–æ—Ö.\n",
    "\n",
    "- –ü–µ—Ä–µ–≤–µ–¥–µ–º –¥–∞–Ω–Ω—ã–µ `x` –≤ one-hot —Ç–µ–∑–Ω–æ—Ä `input_tensor`\n",
    "\n",
    "- –°–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π: —É–º–Ω–æ–∂–∏–º –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–Ω–∑–æ—Ä –Ω–∞ –º–∞—Ç—Ä–∏—Ü—É –≤–µ—Å–æ–≤ `W_input` (–º–∞—Ç—Ä–∏—á–Ω–æ–µ —É–º–Ω–æ–∂–µ–Ω–∏–µ: `.mm()` –∏–ª–∏`@`)\n",
    "\n",
    "- –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π: —É–º–Ω–æ–∂–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è –Ω–∞ –º–∞—Ç—Ä–∏—Ü—É –≤–µ—Å–æ–≤ `W_output`\n",
    "\n",
    "- –ü—Ä–∏–º–µ–Ω–∏–º —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å, –∫–æ—Ç–æ—Ä—É—é –æ–ø—Ä–µ–¥–µ–ª–∏–ª–∏ —Ä–∞–Ω–µ–µ (`criterion`)\n",
    "\n",
    "- –ó–∞–ø—É—Å—Ç–∏–º –º–µ—Ç–æ–¥ –æ–±—Ä–∞—Ç–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –æ—à–∏–±–∫–∏\n",
    "\n",
    "- –ë—É–¥–µ–º —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ –æ–±–Ω–æ–≤–ª—è—Ç—å –≤–µ—Å–∞ –º–µ—Ç–æ–¥–æ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞, –æ–±–Ω–æ–≤–ª—è—è –∑–Ω–∞—á–µ–Ω–∏—è –≤–µ—Å–æ–≤ –º–∞—Ç—Ä–∏—Ü `W_input` –∏ `W_output`\n",
    "\n",
    "- –ò–∑–Ω–∞—á–∞–ª—å–Ω–æ –º—ã –∑–∞–¥–∞–ª–∏ –¥–æ–≤–æ–ª—å–Ω–æ –≤—ã—Å–æ–∫—É—é —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è. –ë—É–¥–µ–º —É–º–µ–Ω—å—à–∞—Ç—å —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–π 10-–π —ç–ø–æ—Ö–∏\n",
    "\n",
    "- –ë—É–¥–µ–º –≤—ã–≤–æ–¥–∏—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –¥–ª—è –∫–∞–∂–¥–æ–π 50-–π —ç–ø–æ—Ö–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for epoch in range(num_epochs):\n",
    "    for x,y in zip(torch.tensor([train_emb.Input.values]), torch.tensor([train_emb.Output.values])):\n",
    "\n",
    "        # one-hot –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ö–æ–¥–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞\n",
    "        input_tensor = F.one_hot(x).type(torch.Tensor)\n",
    "\n",
    "        # —Å–∫—Ä—ã—Ç—ã–π —Å–ª–æ–π 1 x N: —É–º–Ω–æ–∂–µ–Ω–∏–µ –≤—Ö–æ–¥–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–∞ –Ω–∞ –º–∞—Ç—Ä–∏—Ü—É W_input\n",
    "        h = input_tensor @ W_input\n",
    "        #h = input_tensor.mm(W_input)\n",
    "\n",
    "        # –≤—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π 1 x V: —É–º–Ω–æ–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è –Ω–∞ –º–∞—Ç—Ä–∏—Ü—É W_output\n",
    "        y_pred = h @ W_output\n",
    "        #y_pred = h.mm(W_output)\n",
    "\n",
    "        # —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å\n",
    "        loss = criterion(y_pred, y)\n",
    "\n",
    "        # –æ–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–∫–∏\n",
    "        loss.backward()\n",
    "\n",
    "        # –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ —Å –ø–æ–º–æ—â—å—é –º–µ—Ç–æ–¥–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ —Å–ø—É—Å–∫–∞\n",
    "        with torch.no_grad():\n",
    "            W_input[x, :] -= learning_rate * W_input.grad.data[x, :]\n",
    "            W_output -= learning_rate * W_output.grad.data\n",
    "            W_input.grad.data.zero_()\n",
    "            W_output.grad.data.zero_()\n",
    "\n",
    "    # –¥–ª—è –∫–∞–∂–¥–æ–π 10-–π —ç–ø–æ—Ö–∏ –±—É–¥–µ–º —É–º–µ–Ω—å—à–∞—Ç—å —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è\n",
    "    if epoch % 10 == 0:\n",
    "        learning_rate *= lr_decay\n",
    "\n",
    "    # –∑–∞–ø–∏—à–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å\n",
    "    loss_hist.append(loss)\n",
    "\n",
    "    # –≤—ã–≤–æ–¥ –æ—à–∏–±–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–π 50-–π —ç–ø–æ—Ö–∏\n",
    "    if epoch % 50 == 0:\n",
    "        print(f'Epoch {epoch}, loss = {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ–ª—É—á–∏–≤—à–∏–µ—Å—è –≤–µ–∫—Ç–æ—Ä—ã –∏–º–µ—é—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å 5. –ß—Ç–æ–±—ã –æ—Ç–æ–±—Ä–∞–∑–∏—Ç—å –∏—Ö –Ω–∞ –≥—Ä–∞—Ñ–∏–∫–µ, –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É–º–µ–Ω—å—à–∏—Ç—å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–æ 2. –°–¥–µ–ª–∞–µ–º —ç—Ç–æ —Å –ø–æ–º–æ—â—å—é [—Å–∏–Ω–≥—É–ª—è—Ä–Ω–æ–≥–æ —Ä–∞–∑–ª–æ–∂–µ–Ω–∏—è](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_input = W_input.detach().numpy()\n",
    "svd = decomposition.TruncatedSVD(n_components=2)\n",
    "W_input_dec = svd.fit_transform(W_input)\n",
    "x = W_input_dec[:,0]\n",
    "y = W_input_dec[:,1]\n",
    "plot = sns.scatterplot(x=x, y=y)\n",
    "\n",
    "for i in range(0, W_input_dec.shape[0]):\n",
    "     plot.text(x[i], y[i]+2e-2, list(vocabulary.keys())[i], horizontalalignment='center', size='small', color='black', weight='semibold');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå –ö–∞–∫–∏–µ –≤—ã–≤–æ–¥—ã –º–æ–∂–Ω–æ —Å–¥–µ–ª–∞—Ç—å –æ –ø–æ–ª—É—á–∏–≤—à–∏—Ö—Å—è –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è—Ö?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Word2id mapping:\\n{word2id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ—Å—á–∏—Ç–∞–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–∞–º–∏ —Å–ª–æ–≤ 'milk' –∏ 'water', 'milk' –∏ 'apple'.\n",
    "\n",
    "–ó–∞–ø–∏—à–µ–º –≤–µ–∫—Ç–æ—Ä—ã –¥–ª—è —ç—Ç–∏—Ö —Å–ª–æ–≤ –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º –≤ –º–∞—Ç—Ä–∏—Ü–µ `W_input`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_input = torch.Tensor(W_input)\n",
    "milk = W_input[word2id['milk'], :]\n",
    "water = W_input[word2id['water'], :]\n",
    "apple = W_input[word2id['apple'], :]\n",
    "milk_water = F.cosine_similarity(milk, water, dim=0)\n",
    "print(f\"Cosine similarity between words 'milk' and 'water': {round(milk_water.item(), 2)}\")\n",
    "milk_apple= F.cosine_similarity(milk, apple, dim=0)\n",
    "print(f\"Cosine similarity between words 'milk' and 'apple': {round(milk_apple.item(), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ—Å–∫–æ–ª—å–∫—É –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–π word2vec —Ä–µ—à–∞–µ—Ç –∑–∞–¥–∞—á—É –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏, –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏ –ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞—é—Ç—Å—è –≤–µ—Å–∞ –¥–ª—è –≤—Å–µ—Ö —Å–ª–æ–≤ –≤ —Å–ª–æ–≤–∞—Ä–µ. –ß—Ç–æ–±—ã —Å–æ–∫—Ä–∞—Ç–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π, –±—ã–ª–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–∏–Ω–∞—Ä–Ω—É—é –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ï—Å–ª–∏ —Ä–∞–Ω—å—à–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –±—ã–ª–æ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —Å–ª–æ–≤–∞, —Ç–æ —Ç–µ–ø–µ—Ä—å –¥–ª—è –¥–≤—É—Ö —Å–ª–æ–≤ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å, —è–≤–ª—è—é—Ç—Å—è –æ–Ω–∏ —Å–æ—Å–µ–¥–Ω–∏–º–∏ –∏–ª–∏ –Ω–µ—Ç."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://i.ibb.co/NNB4XcQ/from.png\" width=\"400\"></center>\n",
    "<center><img src =\"https://i.ibb.co/2vGfSy9/to.png\" width=\"400\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå –ö–∞–∫–∞—è –ø—Ä–æ–±–ª–µ–º–∞ –≤–æ–∑–Ω–∏–∫–Ω–µ—Ç, –µ—Å–ª–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã? –ö–∞–∫ –µ–µ —Ä–µ—à–∏—Ç—å?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–µ–ø–µ—Ä—å –∫–æ—Ä–ø—É—Å –≤—ã–≥–ª—è–¥–∏—Ç –ø–æ-–¥—Ä—É–≥–æ–º—É: –Ω–∞ –≤—Ö–æ–¥ –º—ã –ø–æ–¥–∞–µ–º –ø–∞—Ä—ã —Å–ª–æ–≤ –∏ –º–µ—Ç–∫—É 1 –∏–ª–∏ 0 (—è–≤–ª—è—é—Ç—Å—è –ª–∏ –æ–Ω–∏ —Å–æ—Å–µ–¥–Ω–∏–º–∏).\n",
    "\n",
    "–ß—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–∏–º–µ—Ä—ã –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞,\n",
    "–¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ü–µ–ª–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞ –≤ –ø–∞—Ä—É –¥–æ–±–∞–≤–ª—è—é—Ç—Å—è —Å–ª—É—á–∞–π–Ω—ã–µ —Å–ª–æ–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —è–≤–ª—è—é—Ç—Å—è —Å–æ—Å–µ–¥–Ω–∏–º–∏. –ù–∞ –∫–∞–∂–¥—ã–π –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—Ä–∏–º–µ—Ä –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –æ—Ç 5 –¥–æ 20 –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://i.ibb.co/7rjSfZp/input-output.png\" width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—É—Å—Ç—å —Ç–µ–ø–µ—Ä—å –Ω–∞—à –∫–æ—Ä–ø—É—Å –≤—ã–≥–ª—è–¥–∏—Ç —Å–ª–µ–¥—É—é—â–∏–º –æ–±—Ä–∞–∑–æ–º:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://i.ibb.co/WKsSD4Y/corpus2.png\" width=\"750\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–£—Å—Ç–∞–Ω–æ–≤–∏–º –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã: —Ä–∞–∑–º–µ—Ä –æ–∫–Ω–∞ ‚Äî 2, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ ‚Äî 3. –î–ª—è –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ–≤–∞ \"Ned\" —Å–ª–æ–≤–æ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ ‚Äî \"Stark\", —Å–ª–æ–≤–∞ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞ ‚Äî \"pimples\", \"zebra\", \"idiot\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–£–º–Ω–æ–∂–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∞ –Ω–∞ –≤–µ—Å–∞ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://i.ibb.co/KWjbsJh/ns-f-1.png\" width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–£–º–Ω–æ–∂–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∞ (—Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å–∫—Ä—ã—Ç–æ–≥–æ —Å–ª–æ—è) –Ω–∞ –≤–µ—Å–∞ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ—è. –ü—Ä–∏–º–µ–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ (—Å–∏–≥–º–æ–∏–¥—É)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://i.ibb.co/VwSdzCh/ns-f-2.png\" width=\"700\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í—ã—á–∏—Ç–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –º–µ—Ç–æ–∫ –∫–ª–∞—Å—Å–æ–≤ –∏–∑ –ø–æ–ª—É—á–∏–≤—à–∏—Ö—Å—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://i.ibb.co/ysrnQ7n/ns-b-1.png\" width=\"500\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ–¥—Å—á–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –¥–ª—è $W_{input}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://i.ibb.co/gRZ4r4d/ns-b-2.png\" width=\"750\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ–¥—Å—á–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞ –¥–ª—è $W^T_{output}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://i.ibb.co/bsBPWy3/ns-b-3.png\" width=\"650\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://i.ibb.co/z4KR0qZ/ns-b-4.png\" width=\"950\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå –ö–∞–∫–∏–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏ –µ—Å—Ç—å —É word2vec?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FastText –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–ª—É—á–∏—Ç—å –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –Ω–µ –¥–ª—è —Ü–µ–ª—ã—Ö —Å–ª–æ–≤, –∞ –¥–ª—è n-–≥—Ä–∞–º–º.\n",
    "\n",
    "- –°–æ–¥–µ—Ä–∂–∞—Ç –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é\n",
    "- –†–µ—à–µ–Ω–∏–µ –¥–ª—è —Å–ª–æ–≤, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ —Å–ª–æ–≤–∞—Ä–µ\n",
    "\n",
    "[P. Bojanowski, E. Grave, A. Joulin ,T. Mikolov (2017). Enriching Word Vectors with Subword Information](https://arxiv.org/pdf/1607.04606v2.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://i.ibb.co/D4HZ4dR/ngrams.png\" width=\"350\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–†–∞–∑–º–µ—Ä n-–≥—Ä–∞–º–º ‚Äî –≥–∏–ø–µ—Ä–º–∞—Ä–∞–º–µ—Ç—Ä. –í—Å–µ n-–≥—Ä–∞–º–º—ã —Ä–∞–Ω–∂–∏—Ä—É—é—Ç—Å—è –∏ –±–µ—Ä—É—Ç—Å—è —Ç–æ–ª—å–∫–æ —Å–∞–º—ã–µ —á–∞—Å—Ç–æ—Ç–Ω—ã–µ (–ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–ª–ª–∏–æ–Ω–æ–≤)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://i.ibb.co/8Dvyp8r/fasttext.png\" width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–ª—è —Å–≤–æ–∏—Ö –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã—Ö –Ω—É–∂–¥ –∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –±—ã–≤–∞–µ—Ç –ø–æ–ª–µ–∑–Ω–æ —Å–∞–º–æ–º—É –Ω–∞—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ –Ω—É–∂–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ —Å –Ω—É–∂–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏. –ù–æ –¥–ª—è –∫–∞–∫–∏—Ö-—Ç–æ –æ–±—â–∏—Ö —Ü–µ–ª–µ–π –º–æ–¥–µ–ª–∏ —É–∂–µ –µ—Å—Ç—å.\n",
    "\n",
    "–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–ª–∏ –æ–±—É—á–∏—Ç—å —Å–≤–æ—é –º–æ–∂–Ω–æ —Å –ø–æ–º–æ—â—å—é –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ [gensim](https://radimrehurek.com/gensim/models/word2vec.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ó–∞–≥—Ä—É–∑–∏–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –æ—Ç Google. –ú–æ–¥–µ–ª—å –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è –≤–µ–∫—Ç–æ—Ä—ã –¥–ª—è 3 –º–∏–ª–ª–∏–æ–Ω–æ–≤ —Å–ª–æ–≤ –∏ –æ–±—É—á–∞–ª–∞—Å—å –Ω–∞ 100 –º–∏–ª–ª–∏–∞—Ä–¥–∞—Ö —Å–ª–æ–≤ –∏–∑ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö Google News. –î–ª–∏–Ω–∞ –≤–µ–∫—Ç–æ—Ä–∞ —Ä–∞–≤–Ω–∞ 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://edunet.kea.su/repo/EduNet_NLP-web_dependencies/datasets/GoogleNews-vectors-negative300.bin.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∫–ª–∞—Å—Å [KeyedVectors](https://radimrehurek.com/gensim/models/keyedvectors.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "google_news_model = KeyedVectors.load_word2vec_format(f'GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–µ–º –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü–æ–¥—Å—á–µ—Ç –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π –º–µ—Ä—ã –±–ª–∏–∑–æ—Å—Ç–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–î–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π —Å—Ö–æ–∂–µ—Å—Ç–∏ –≤–µ–∫—Ç–æ—Ä–æ–≤ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–µ—Ä–∞ –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º ¬´–∫–æ—Å–∏–Ω—É—Å–Ω–∞—è –±–ª–∏–∑–æ—Å—Ç—å¬ª. –§–∞–∫—Ç–∏—á–µ—Å–∫–∏ —ç—Ç–æ –∫–æ—Å–∏–Ω—É—Å —É–≥–ª–∞ –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–∞–º–∏ ‚Äî —Å–∫–∞–ª—è—Ä–Ω–æ–µ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ —ç—Ç–∏—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤, —Ä–∞–∑–¥–µ–ª–µ–Ω–Ω–æ–µ –Ω–∞ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –∏—Ö –¥–ª–∏–Ω.\n",
    "\n",
    "–ß–µ–º –±–æ–ª—å—à–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π –º–µ—Ä—ã, —Ç–µ–º –±–ª–∏–∂–µ –¥—Ä—É–≥ –∫ –¥—Ä—É–≥—É –≤–µ–∫—Ç–æ—Ä—ã."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://i.ibb.co/4gkSDNx/cosine-similarity.png\" width=\"550\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í –±–∏–±–ª–∏–æ—Ç–µ–∫–µ gensim –µ—Å—Ç—å –≤—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è [similarity](https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.similarity.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Gensim cosine similarity between 'coffee' and 'tea': \\\n",
    "{round(google_news_model.similarity('coffee','tea').item(), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Size of vector 'coffee': {google_news_model['coffee'].shape}\")\n",
    "print(f\"Size of vector 'tea': {google_news_model['tea'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–æ–∂–µ–º –Ω–∞–ø–∏—Å–∞—Ç—å —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–π –±–ª–∏–∑–æ—Å—Ç–∏ –∏ —Å—Ä–∞–≤–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–µ–π."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cos_similarity(model, word1, word2):\n",
    "  a = model[word1]\n",
    "  b = model[word2]\n",
    "  cos_sim = np.dot(a, b)/(norm(a)*norm(b))\n",
    "  return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Custom cosine similarity between 'coffee' and 'tea': \\\n",
    "{round(cos_similarity(google_news_model, 'coffee','tea').item(), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ—Å—á–∏—Ç–∞–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω—É—é –º–µ—Ä—É –¥–ª—è —Å–ª–æ–≤ \"–º—è—á\" –∏ \"–∫—Ä–æ–∫–æ–¥–∏–ª\" –¥–≤—É–º—è —Å–ø–æ—Å–æ–±–∞–º–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Gensim cosine similarity between 'ball' and 'crocodile': \\\n",
    "{round(google_news_model.similarity('ball','crocodile').item(), 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Custom cosine similarity between 'ball' and 'crocodile': \\\n",
    "{round(cos_similarity(google_news_model, 'ball','crocodile').item(), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –±–ª–∏–∑–∫–∏–µ —Å–ª–æ–≤–∞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–° –ø–æ–º–æ—â—å—é word2vec –º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ *n* —Å–∞–º—ã—Ö –ø–æ—Ö–æ–∂–∏—Ö —Å–ª–æ–≤ –¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä–æ–≥–æ —Ü–µ–ª–µ–≤–æ–≥–æ —Å–ª–æ–≤–∞. –î–ª—è —ç—Ç–æ–≥–æ –Ω—É–∂–Ω–æ –≤—ã—á–∏—Å–ª–∏—Ç—å –∫–æ—Å–∏–Ω—É—Å–Ω—É—é –±–ª–∏–∑–æ—Å—Ç—å –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–æ–º –∑–∞–¥–∞–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞ –∏ –≤–µ–∫—Ç–æ—Ä–∞–º–∏ –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞ –≤ –º–æ–¥–µ–ª–∏.\n",
    "\n",
    "–í–æ—Å–ø–æ–ª—å–∑—É–µ–º—Å—è —Ñ—É–Ω–∫—Ü–∏–µ–π [most_similar](https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.most_similar.html) –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Top-5 most similar words to 'cat':\")\n",
    "for x in google_news_model.most_similar('cat', topn=5):\n",
    "    print(f\"{x[0]} {round(x[1], 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–∞–π–¥–µ–º –±–ª–∏–∂–∞–π—à–∏–µ –ø–æ–Ω—è—Ç–∏—è –¥–ª—è –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã—Ö \"–ø–ª–æ—Ö–æ–π\" –∏ \"—Ö–æ—Ä–æ—à–∏–π\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Top-5 most similar words to 'bad':\")\n",
    "for x in google_news_model.most_similar('bad', topn=5):\n",
    "    print(f\"{x[0]} {round(x[1], 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Top-5 most similar words to 'good':\")\n",
    "for x in google_news_model.most_similar('good', topn=5):\n",
    "    print(f\"{x[0]} {round(x[1], 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–ø–æ—Ä—Ü–∏–∏ (–∞–Ω–∞–ª–æ–≥–∏–∏)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–∞–∫–∂–µ –º–æ–∂–Ω–æ –æ—Å—É—â–µ—Å—Ç–≤–∏—Ç—å –≤–µ–∫—Ç–æ—Ä–Ω—É—é –∞—Ä–∏—Ñ–º–µ—Ç–∏–∫—É: —Å–∫–ª–∞–¥—ã–≤–∞—Ç—å –∏ –≤—ã—á–∏—Ç–∞—Ç—å –≤–µ–∫—Ç–æ—Ä–∞ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å–ª–æ–≤. –ù–∞–ø—Ä–∏–º–µ—Ä, —Å–ª–æ–∂–∏–≤ –¥–≤–∞ –≤–µ–∫—Ç–æ—Ä–∞ –∏ –≤—ã—á—Ç—è –∏–∑ –Ω–∏—Ö —Ç—Ä–µ—Ç–∏–π –≤–µ–∫—Ç–æ—Ä, –º—ã –º–æ–∂–µ–º —Ä–µ—à–∏—Ç—å —Å–≤–æ–µ–æ–±—Ä–∞–∑–Ω—É—é –ø—Ä–æ–ø–æ—Ä—Ü–∏—é.\n",
    "\n",
    "–î–ª—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏—Ö –∞–Ω–∞–ª–æ–≥–∏–π —Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –º–µ—Ç–æ–¥ [most_similar](https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.most_similar.html). –û–Ω –≤—ã–ø–æ–ª–Ω—è–µ—Ç —Å–ª–æ–∂–µ–Ω–∏–µ –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ (`positive`) –∏ –≤—ã—á–∏—Ç–∞–Ω–∏–µ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö (`negative`). –ó–∞—Ç–µ–º –∏–∑ –ø–æ–ª—É—á–µ–Ω–Ω–æ–π –ø–æ–∑–∏—Ü–∏–∏ –≤—ã–≤–æ–¥–∏—Ç—Å—è —Å–ø–∏—Å–æ–∫ –±–ª–∏–∂–∞–π—à–∏—Ö –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://i.ibb.co/ysPWz5x/analogies.png\" width=\"900\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "king - man + woman = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"king - man + woman = \\\n",
    "{google_news_model.most_similar(positive=['king','woman'], negative=['man'], topn=1)[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ—Å—Ç—Ä–æ–∏–º —Å–ª–µ–¥—É—é—â–∏–µ –∞–Ω–∞–ª–æ–≥–∏–∏:\n",
    "- –¥–ª—è –≥–ª–∞–≥–æ–ª—å–Ω—ã—Ö —Ñ–æ—Ä–º: swimming - walking + walked = ?\n",
    "- –¥–ª—è —Å—Ç—Ä–∞–Ω –∏ –≥–æ—Ä–æ–¥–æ–≤: Turkey - Russia + Moscow = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"swimming - walking + walked = \\\n",
    "{google_news_model.most_similar(positive=['swimming','walked'], negative=['walking'], topn=1)[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Turkey - Russia + Moscow = \\\n",
    "{google_news_model.most_similar(positive=['Turkey','Moscow'], negative=['Russia'], topn=1)[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü–æ–∏—Å–∫ –ª–∏—à–Ω–µ–≥–æ —Å–ª–æ–≤–∞ –ø–æ —Å–º—ã—Å–ª—É"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–∞–∫–æ–Ω–µ—Ü, –º—ã –º–æ–∂–µ–º –Ω–∞–π—Ç–∏ –ª–∏—à–Ω–µ–µ –ø–æ —Å–º—ã—Å–ª—É —Å–ª–æ–≤–æ –≤ –≥—Ä—É–ø–ø–µ —Å–ª–æ–≤. –õ–∏—à–Ω–∏–º —Å–ª–æ–≤–æ–º —è–≤–ª—è–µ—Ç—Å—è —Ç–æ, –≤–µ–∫—Ç–æ—Ä –∫–æ—Ç–æ—Ä–æ–≥–æ –Ω–∞–∏–±–æ–ª–µ–µ —É–¥–∞–ª–µ–Ω –æ—Ç –¥—Ä—É–≥–∏—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ —Å–ª–æ–≤.\n",
    "- –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –≤—Å–µ—Ö —Å–ª–æ–≤-–≤–µ–∫—Ç–æ—Ä–æ–≤ (—Ü–µ–Ω—Ç—Ä);\n",
    "- –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –æ—Ç  —Ü–µ–Ω—Ç—Ä–∞ –¥–æ –∫–∞–∂–¥–æ–≥–æ —Å–ª–æ–≤–∞;\n",
    "- –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ —Å–ª–æ–≤–æ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ—Å–∏–Ω—É—Å–Ω—ã–º —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ–º –æ—Ç —Ü–µ–Ω—Ç—Ä–∞.\n",
    "\n",
    "–û–±—Ä–∞—Ç–∏—Ç–µ –≤–Ω–∏–º–∞–Ω–∏–µ, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–º–µ–Ω–Ω–æ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ, –∞ –Ω–µ –∫–æ—Å–∏–Ω—É—Å–Ω–∞—è –±–ª–∏–∑–æ—Å—Ç—å.\n",
    "\n",
    "$distance (A, B) = 1 ‚Äì similarity(A, B)$\n",
    "\n",
    "–ß–µ–º –±–æ–ª—å—à–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–∞–º–∏, —Ç–µ–º –º–µ–Ω—å—à–µ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://i.postimg.cc/rwx8dXMW/doesnt-match.png\" width=\"550\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–≠—Ç–æ –º–æ–∂–Ω–æ –æ—Å—É—â–µ—Å—Ç–≤–∏—Ç—å —Å –ø–æ–º–æ—â—å—é —Ñ—É–Ω–∫—Ü–∏–∏ [doesnt_match](https://tedboy.github.io/nlps/generated/generated/gensim.models.Word2Vec.doesnt_match.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['friday', 'saturday', 'sunday', 'weekday', 'spoon']\n",
    "print(f\"The word '{google_news_model.doesnt_match(words)}' \\\n",
    "from the given list {words} doesn‚Äôt go with the others.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–∞–π–¥–µ–º –ª–∏—à–Ω–µ–µ –ø–æ —Å–º—ã—Å–ª—É —Å–ª–æ–≤–æ –≤ —Å–ª–µ–¥—É—é—â–∏—Ö –≥—Ä—É–ø–ø–∞—Ö:\n",
    "- prince, king, fork, queen, castle\n",
    "- dinner, breakfast, cereal, lunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['prince', 'king', 'fork', 'queen', 'castle']\n",
    "print(f\"The word '{google_news_model.doesnt_match(words)}' \\\n",
    "from the given list {words} doesn‚Äôt go with the others.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['dinner', 'breakfast', 'cereal', 'lunch']\n",
    "print(f\"The word '{google_news_model.doesnt_match(words)}' \\\n",
    "from the given list {words} doesn‚Äôt go with the others.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è ‚Äî —ç—Ç–æ –∑–∞–¥–∞—á–∞ —Ä–∞–∑–±–∏–µ–Ω–∏—è –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ –≥—Ä—É–ø–ø—ã, –Ω–∞–∑—ã–≤–∞–µ–º—ã–µ –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏. –í–Ω—É—Ç—Ä–∏ –∫–∞–∂–¥–æ–π –≥—Ä—É–ø–ø—ã –¥–æ–ª–∂–Ω—ã –æ–∫–∞–∑–∞—Ç—å—Å—è ¬´–ø–æ—Ö–æ–∂–∏–µ¬ª –æ–±—ä–µ–∫—Ç—ã, –∞ –æ–±—ä–µ–∫—Ç—ã —Ä–∞–∑–Ω—ã—Ö –≥—Ä—É–ø–ø—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∫–∞–∫ –º–æ–∂–Ω–æ –±–æ–ª–µ–µ –æ—Ç–ª–∏—á–Ω—ã. –ì–ª–∞–≤–Ω–æ–µ –æ—Ç–ª–∏—á–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –æ—Ç –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ –ø–µ—Ä–µ—á–µ–Ω—å –≥—Ä—É–ø–ø —á–µ—Ç–∫–æ –Ω–µ –∑–∞–¥–∞–Ω –∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ä–∞–±–æ—Ç—ã –∞–ª–≥–æ—Ä–∏—Ç–º–∞.\n",
    "\n",
    "–ë—É–¥–µ–º –æ—Å—É—â–µ—Å—Ç–≤–ª—è—Ç—å –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://i.postimg.cc/5098tmdc/text-clustreing.webp\" width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/Xeanst/NLP_course_FBB/main/data/articles_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_raw = pd.read_csv(\"articles_data.csv\")\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò–Ω—Ç–µ—Ä–µ—Å—É—é—â–∏–µ –Ω–∞—Å –∫–æ–ª–æ–Ω–∫–∏: –∑–∞–≥–æ–ª–æ–≤–æ–∫ (title), –æ–ø–∏—Å–∞–Ω–∏–µ (description) –∏ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ (content).\n",
    "\n",
    "–°–æ–∑–¥–∞–¥–∏–º –Ω–æ–≤—ã–π –¥–∞—Ç–∞—Ñ—Ä–µ–π–º —Å –æ–¥–Ω–æ–π –∫–æ–ª–æ–Ω–∫–æ–π \"—Ç–µ–∫—Å—Ç\", –∫—É–¥–∞ –¥–æ–±–∞–≤–∏–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —ç—Ç–∏—Ö —Ç—Ä–µ—Ö –∫–æ–ª–æ–Ω–æ–∫."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_columns = [\"title\", \"description\", \"content\"]\n",
    "df = df_raw.loc[:,text_columns]\n",
    "\n",
    "for col in text_columns:\n",
    "    df[col] = df[col].astype(str)\n",
    "\n",
    "df[\"text\"] = df[text_columns].apply(lambda x: \" | \".join(x), axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk import word_tokenize\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = set(stopwords.words(\"english\") + [\"news\", \"new\", \"top\"])\n",
    "\n",
    "def clean_text(text, tokenizer, stopwords):\n",
    "\n",
    "    text = text.lower() # –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
    "    text = re.sub(r\"\\[(.*?)\\]\", \"\", text)  # —É–¥–∞–ª—è–µ–º —Å–ª–æ–≤–∞ –≤ –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã—Ö —Å–∫–æ–±–∫–∞—Ö –∫–∞–∫ [+300 chars]\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # —É–¥–∞–ª—è–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –ø—Ä–æ–±–µ–ª, —Ç–∞–±—É–ª—è—Ü–∏—é –∏ –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏\n",
    "    text = re.sub(r\"\\w+‚Ä¶|‚Ä¶\", \"\", text)  # —É–¥–∞–ª—è–µ–º –º–Ω–æ–≥–æ—Ç–æ—á–∏–µ\n",
    "    text = re.sub(r\"(?<=\\w)-(?=\\w)\", \" \", text) # –∑–∞–º–µ–Ω—è–µ–º –¥–µ—Ñ–∏—Å –Ω–∞ –ø—Ä–æ–±–µ–ª\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)  # —É–¥–∞–ª—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é\n",
    "\n",
    "    tokens = tokenizer(text) # —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º\n",
    "    tokens = [t for t in tokens if not t in stopwords]  # —É–¥–∞–ª—è–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞\n",
    "    tokens = [\"\" if t.isdigit() else t for t in tokens]  # —É–¥–∞–ª—è–µ–º —á–∏—Å–ª–∞\n",
    "    tokens = [t for t in tokens if len(t) > 1]  # —É–¥–∞–ª—è–µ–º —Ç–æ–∫–µ–Ω—ã –¥–ª–∏–Ω—ã 1\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tokens\"] = df[\"text\"].map(lambda x: clean_text(x, word_tokenize, custom_stopwords))\n",
    "\n",
    "# —É–¥–∞–ª—è–µ–º –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –¥–≤–∞ —Å—Ç–æ–ª–±—Ü–∞: —Ç–µ–∫—Å—Ç—ã –∏ —Ç–æ–∫–µ–Ω—ã\n",
    "df = df.loc[df.tokens.map(lambda x: len(x) > 0), [\"text\", \"tokens\"]]\n",
    "\n",
    "# —É–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã —Å—Ç—Ä–æ–∫\n",
    "import numpy as np\n",
    "_, idx = np.unique(df[\"tokens\"], return_index=True)\n",
    "df = df.iloc[idx, :]\n",
    "\n",
    "docs = df[\"text\"].values\n",
    "tokenized_docs = df[\"tokens\"].values\n",
    "\n",
    "print(f\"–ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {df_raw.shape}\")\n",
    "print(f\"–î–∞–Ω–Ω—ã–µ –ø–æ—Å–ª–µ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –û–±—É—á–µ–Ω–∏–µ word2vec –Ω–∞ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å —Å –ø–æ–º–æ—â—å—é –º–æ–¥—É–ª—è [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec). –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã:\n",
    "\n",
    "- sentences ‚Äî –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ,\n",
    "- vector_size ‚Äî —Ä–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–∞ (default 100),\n",
    "- window ‚Äî —Ä–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è (default 5),\n",
    "- min_count ‚Äî –º–∏–Ω. —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å —Å–ª–æ–≤–∞ –≤ –∫–æ—Ä–ø—É—Å–µ (default 5),\n",
    "- sg ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –æ–±—É—á–µ–Ω–∏—è (default 0 ‚Äî CBOW, 1 ‚Äî Skip-gram),\n",
    "- workers ‚Äî –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —è–¥–µ—Ä (default 3),\n",
    "- alpha ‚Äî —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è (default 0.025)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "articles_model = Word2Vec(sentences=tokenized_docs, workers=1, seed=42)\n",
    "len(articles_model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Top-5 most similar words to 'president':\")\n",
    "for x in articles_model.wv.most_similar('president', topn=5):\n",
    "    print(f\"{x[0]} {round(x[1], 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"spain - germany + berlin = \")\n",
    "for x in articles_model.wv.most_similar(positive=['spain','berlin'], negative=['germany'], topn=6):\n",
    "    print(f\"{x[0]} ({round(x[1], 2)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –±—É–¥–µ—Ç –æ—Å—É—â–µ—Å—Ç–≤–ª—è—Ç—å—Å—è –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π).\n",
    "\n",
    "–£—Å—Ä–µ–¥–Ω–∏–º –≤–µ–∫—Ç–æ—Ä—ã —Å–ª–æ–≤ –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞, —á—Ç–æ–±—ã —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–µ–∫—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞.\n",
    "\n",
    "- –°–æ–∑–¥–∞–µ–º numpy –º–∞—Å—Å–∏–≤ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–æ–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ `features`, —Å–æ—Å—Ç–æ—è—â–∏–π –∏–∑ –Ω—É–ª–µ–π\n",
    "  - —Ä–∞–∑–º–µ—Ä –º–∞—Å—Å–∏–≤–∞: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ $\\times$ —Ä–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–∞ –≤ –º–æ–¥–µ–ª–∏\n",
    "- –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ —Å–ø–∏—Å–∫—É —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ `tokenized_docs`\n",
    "  \n",
    "  - –°–æ–∑–¥–∞–µ–º numpy –º–∞—Å—Å–∏–≤ –≤–µ–∫—Ç–æ—Ä–æ–≤ —Å–ª–æ–≤ `vectors`, —Å–æ—Å—Ç–æ—è—â–∏–π –∏–∑ –Ω—É–ª–µ–π `np.zeros`\n",
    "    - —Ä–∞–∑–º–µ—Ä –º–∞—Å—Å–∏–≤–∞: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–µ $\\times$ —Ä–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–∞ –≤ –º–æ–¥–µ–ª–∏\n",
    "  - –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ —Ç–æ–∫–µ–Ω–∞–º –¥–æ–∫—É–º–µ–Ω—Ç–∞ `tokens`\n",
    "    - –ï—Å–ª–∏ –≤–µ–∫—Ç–æ—Ä –¥–ª—è —Å–ª–æ–≤–∞ `token` –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ –º–æ–¥–µ–ª–∏ `model`, –¥–æ–±–∞–≤–ª—è–µ–º –≤–µ–∫—Ç–æ—Ä —Å–ª–æ–≤–∞ –≤ –º–∞—Å—Å–∏–≤ `vectors`\n",
    "\n",
    "  - –°—á–∏—Ç–∞–µ–º —Å—Ä–µ–¥–Ω–µ–µ `np.mean` –¥–ª—è –º–∞—Å—Å–∏–≤–∞ `vectors` (—Å —É–∫–∞–∑–∞–Ω–∏–µ–º –æ—Å–∏), –ø–æ–ª—É—á–∞–µ–º —É—Å—Ä–µ–¥–Ω–µ–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–∞\n",
    "\n",
    "  - –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª—É—á–µ–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä –≤ –º–∞—Å—Å–∏–≤ `features`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2vec(tokenized_docs, model):\n",
    "\n",
    "    features = np.zeros((len(tokenized_docs), model.wv.vector_size))\n",
    "    for i,tokens in enumerate(tokenized_docs):\n",
    "        vectors = np.zeros((len(tokens), model.wv.vector_size))\n",
    "        for j,token in enumerate(tokens):\n",
    "            if token in model.wv:\n",
    "                vectors[j] = model.wv[token]\n",
    "        avg_vec = np.mean(vectors, axis=0)\n",
    "        features[i] = avg_vec\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_docs = doc2vec(tokenized_docs, model=articles_model)\n",
    "vectorized_docs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ú–µ—Ç–æ–¥ k-—Å—Ä–µ–¥–Ω–∏—Ö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ú–µ—Ç–æ–¥ k-—Å—Ä–µ–¥–Ω–∏—Ö –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ —Ä–∞–∑–±–∏–µ–Ω–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞ –Ω–∞ –∑–∞—Ä–∞–Ω–µ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–µ —á–∏—Å–ª–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ $k$. –ê–ª–≥–æ—Ä–∏—Ç–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–æ–Ω–Ω—É—é –ø—Ä–æ—Ü–µ–¥—É—Ä—É, –≤ –∫–æ—Ç–æ—Ä–æ–π –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è —Å–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:\n",
    "\n",
    "1. –í—ã–±–∏—Ä–∞–µ—Ç—Å—è —á–∏—Å–ª–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ $k$.\n",
    "2. –ò–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö —Å–ª—É—á–∞–π–Ω—ã–º –æ–±—Ä–∞–∑–æ–º –≤—ã–±–∏—Ä–∞—é—Ç—Å—è $k$ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π, –∫–æ—Ç–æ—Ä—ã–µ –±—É–¥—É—Ç —Å–ª—É–∂–∏—Ç—å –Ω–∞—á–∞–ª—å–Ω—ã–º–∏ —Ü–µ–Ω—Ç—Ä–∞–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤.\n",
    "3. –î–ª—è –∫–∞–∂–¥–æ–≥–æ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –∏—Å—Ö–æ–¥–Ω–æ–≥–æ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –±–ª–∏–∂–∞–π—à–∏–π –∫ –Ω–µ–º—É —Ü–µ–Ω—Ç—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞.\n",
    "4. –í—ã—á–∏—Å–ª—è—é—Ç—Å—è —Ü–µ–Ω—Ç—Ä–æ–∏–¥—ã ‚Äî —Ü–µ–Ω—Ç—Ä—ã —Ç—è–∂–µ—Å—Ç–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤. –ö–∞–∂–¥—ã–π —Ü–µ–Ω—Ç—Ä–æ–∏–¥ ‚Äî —ç—Ç–æ –≤–µ–∫—Ç–æ—Ä, —ç–ª–µ–º–µ–Ω—Ç—ã –∫–æ—Ç–æ—Ä–æ–≥–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Å–æ–±–æ–π —Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã–µ –ø–æ –≤—Å–µ–º –∑–∞–ø–∏—Å—è–º –∫–ª–∞—Å—Ç–µ—Ä–∞.\n",
    "5. –¶–µ–Ω—Ç—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞ —Å–º–µ—â–∞–µ—Ç—Å—è –≤ –µ–≥–æ —Ü–µ–Ω—Ç—Ä–æ–∏–¥, –ø–æ—Å–ª–µ —á–µ–≥–æ —Ü–µ–Ω—Ç—Ä–æ–∏–¥ —Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è —Ü–µ–Ω—Ç—Ä–æ–º –Ω–æ–≤–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞.\n",
    "6. 3-–π –∏ 4-–π —à–∞–≥–∏ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ –ø–æ–≤—Ç–æ—Ä—è—é—Ç—Å—è. –ù–∞ –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –∏ —Å–º–µ—â–µ–Ω–∏–µ –∏—Ö —Ü–µ–Ω—Ç—Ä–æ–≤. –í —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ –º–∏–Ω–∏–º–∏–∑–∏—Ä—É–µ—Ç—Å—è —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É —ç–ª–µ–º–µ–Ω—Ç–∞–º–∏ –≤–Ω—É—Ç—Ä–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –∏ —É–≤–µ–ª–∏—á–∏–≤–∞—é—Ç—Å—è –º–µ–∂–¥—É–∫–ª–∞—Å—Ç–µ—Ä–Ω—ã–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è.\n",
    "\n",
    "–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç—Å—è —Ç–æ–≥–¥–∞, –∫–æ–≥–¥–∞ –≥—Ä–∞–Ω–∏—Ü—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –∏ —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏—è —Ü–µ–Ω—Ç—Ä–æ–∏–¥–æ–≤ –Ω–µ –ø–µ—Ä–µ—Å—Ç–∞–Ω—É—Ç –∏–∑–º–µ–Ω—è—Ç—å—Å—è –æ—Ç –∏—Ç–µ—Ä–∞—Ü–∏–∏ –∫ –∏—Ç–µ—Ä–∞—Ü–∏–∏, —Ç.–µ. –Ω–∞ –∫–∞–∂–¥–æ–π –∏—Ç–µ—Ä–∞—Ü–∏–∏ –≤ –∫–∞–∂–¥–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ –±—É–¥–µ—Ç –æ—Å—Ç–∞–≤–∞—Ç—å—Å—è –æ–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ –Ω–∞–±–æ—Ä –Ω–∞–±–ª—é–¥–µ–Ω–∏–π."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src =\"https://i.ibb.co/5FzWHX1/kmeans.png\" width=\"700\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ó–∞–¥–∞—á–∞ –æ—Ü–µ–Ω–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ —è–≤–ª—è–µ—Ç—Å—è –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω–æ–π –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –æ—Ü–µ–Ω–∫–æ–π –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏. –ù–∞–º –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã –∏—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ –æ–±—ä–µ–∫—Ç–æ–≤, –ø–æ—ç—Ç–æ–º—É –Ω—É–∂–Ω—ã —Ç–∞–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –≤—ã—á–∏—Å–ª–∏—Ç—å –ø–æ –Ω–µ—Ä–∞–∑–º–µ—á–µ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ.\n",
    "\n",
    "–ö—Ä–∏—Ç–µ—Ä–∏–π —Å–∏–ª—É—ç—Ç–∞ (–∞–Ω–≥–ª *Silhouette*) ‚Äî —ç—Ç–æ –º–µ—Ç—Ä–∏–∫–∞, –∫–æ—Ç–æ—Ä–∞—è –Ω–µ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç –∑–Ω–∞–Ω–∏—è –∏—Å—Ç–∏–Ω–Ω—ã—Ö –º–µ—Ç–æ–∫ –æ–±—ä–µ–∫—Ç–æ–≤, –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –æ—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ —Å–∞–º—É –Ω–µ—Ä–∞–∑–º–µ—á–µ–Ω–Ω—É—é –≤—ã–±–æ—Ä–∫—É –∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏.\n",
    "\n",
    "- —Ç–æ—á–∫–∏ –≤–Ω—É—Ç—Ä–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞ –¥–æ–ª–∂–Ω—ã –ª–µ–∂–∞—Ç—å –æ—á–µ–Ω—å –±–ª–∏–∑–∫–æ –¥—Ä—É–≥ –∫ –¥—Ä—É–≥—É, —Ç–æ –µ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å *–ø–ª–æ—Ç–Ω—ã–º*\n",
    "- —Å–∞–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞ –¥–æ–ª–∂–Ω—ã –ª–µ–∂–∞—Ç—å –∫–∞–∫ –º–æ–∂–Ω–æ –¥–∞–ª—å—à–µ –¥—Ä—É–≥ –æ—Ç –¥—Ä—É–≥–∞\n",
    "\n",
    "–ú–µ—Ç—Ä–∏–∫–∞ —Å–∏–ª—É—ç—Ç–∞ –ø–æ–∑–≤–æ–ª—è–µ—Ç —É—á–∏—Ç—ã–≤–∞—Ç—å –æ–±–∞ —ç—Ç–∏—Ö —Ñ–∞–∫—Ç–∞ –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Å—Ä–µ–¥–Ω–µ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ –æ–±—ä–µ–∫—Ç–æ–≤ —Å–≤–æ–µ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞ –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –æ—Ç —Å—Ä–µ–¥–Ω–µ–≥–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –¥–æ –æ–±—ä–µ–∫—Ç–æ–≤ –¥—Ä—É–≥–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤. –ú–µ–Ω—è–µ—Ç—Å—è –≤ –∏–Ω—Ç–µ—Ä–≤–∞–ª–µ –æ—Ç $-1$ –¥–æ $1$ (—á–µ–º –≤—ã—à–µ, —Ç–µ–º –ª—É—á—à–µ)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í—ã–±–µ—Ä–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å –ø–æ–º–æ—â—å—é –∫—Ä–∏—Ç–µ—Ä–∏—è —Å–∏–ª—É—ç—Ç–∞. –†–∞—Å—Å–º–æ—Ç—Ä–∏–º –∑–Ω–∞—á–µ–Ω–∏—è –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ –æ—Ç 2 –¥–æ 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = -1\n",
    "for i in range(2,11):\n",
    "    kmeans = KMeans(n_clusters=i, n_init=\"auto\", random_state=42)\n",
    "    data = kmeans.fit(vectorized_docs)\n",
    "    silhouette_avg = silhouette_score(vectorized_docs, data.labels_)\n",
    "    print(f\"For n_clusters = {i} mean Silhouette score is: {silhouette_avg:.2f}\")\n",
    "    if silhouette_avg > metrics:\n",
    "      metrics = silhouette_avg\n",
    "      result_data = data\n",
    "      clusters = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–∞–∏–±–æ–ª—å—à–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Å–∏–ª—É—ç—Ç–∞ –ø–æ–ª—É—á–∞–µ–º –ø—Ä–∏ 3 –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö.\n",
    "\n",
    "–†–∞—Å—Å–º–æ—Ç—Ä–∏–º –∑–Ω–∞—á–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞ –æ—Ç–¥–µ–ª—å–Ω–æ: —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ, –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_silhouette_values = silhouette_samples(vectorized_docs, data.labels_)\n",
    "silhouette_values = []\n",
    "for i in range(clusters):\n",
    "    cluster_silhouette_values = sample_silhouette_values[data.labels_ == i]\n",
    "    silhouette_values.append((i, cluster_silhouette_values.shape[0], cluster_silhouette_values.mean(), cluster_silhouette_values.min(), cluster_silhouette_values.max(),))\n",
    "silhouette_values = sorted(silhouette_values, key=lambda tup: tup[2], reverse=True)\n",
    "for s in silhouette_values:\n",
    "    print(f\"Cluster {s[0]}: size: {s[1]}, mean: {s[2]:.2f}, minimum: {s[3]:.2f}, maximum: {s[4]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–†–∞—Å—Å–º–æ—Ç—Ä–∏–º –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(clusters):\n",
    "    tokens_per_cluster = \"\"\n",
    "    most_representative = articles_model.wv.most_similar(positive=[kmeans.cluster_centers_[i]], topn=5)\n",
    "    for t in most_representative:\n",
    "        tokens_per_cluster += f\"{t[0]} \"\n",
    "    print(f\"Cluster {i}: {tokens_per_cluster}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(clusters):\n",
    "  print(f\"Cluster {i}:\")\n",
    "  most_representative_docs = np.argsort(\n",
    "      np.linalg.norm(vectorized_docs - kmeans.cluster_centers_[i], axis=1)\n",
    "  )\n",
    "  for d in most_representative_docs[:2]:\n",
    "    print(docs[d])\n",
    "    print(\"-------------\")\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–†–µ–∞–ª–∏–∑—É–µ–º –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ —Å –ø–æ–º–æ—â—å—é –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω–æ–≥–æ –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω–∞. –í–º–µ—Å—Ç–æ —Ç–æ–≥–æ, —á—Ç–æ–±—ã –æ–±—É—á–∞—Ç—å —Å–ª–æ–π `nn.Embeddings`, –æ–±—É—á–∏–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∑–∞—Ä–∞–Ω–µ–µ —Å –ø–æ–º–æ—â—å—é word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import pandas as pd\n",
    "from string import punctuation # –Ω–∞–±–æ—Ä –∑–Ω–∞–∫–æ–≤ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è\n",
    "from collections import Counter # —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ –æ–±—ä–µ–∫—Ç–æ–≤\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn # —Å–ª–æ–∏\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler # —Ä–∞–±–æ—Ç–∞ —Å –¥–∞–Ω–Ω—ã–º–∏\n",
    "import torch.nn.functional as F # –ø–∞–¥–¥–∏–Ω–≥\n",
    "from torch.nn.utils.rnn import pad_sequence # –ø–∞–¥–¥–∏–Ω–≥\n",
    "import torch.optim as optim # –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í –∫–∞—á–µ—Å—Ç–≤–µ –¥–∞–Ω–Ω—ã—Ö –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–π –∫–æ—Ä–ø—É—Å –∫–æ—Ä–æ—Ç–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤ [RuTweetCorp](https://www.elibrary.ru/item.asp?id=20399632).\n",
    "\n",
    "–ö–æ—Ä–ø—É—Å —Ä–∞–∑–±–∏—Ç –Ω–∞ –¥–≤–∞ –∫–ª–∞—Å—Å–∞: —Ç–≤–∏—Ç—ã —Å –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ–π –æ–∫—Ä–∞—Å–∫–æ–π (`positive.csv`) –∏ —Ç–≤–∏—Ç—ã —Å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–æ–π –æ–∫—Ä–∞—Å–∫–æ–π (`negative.csv`).\n",
    "\n",
    "---\n",
    "\n",
    "Twitter ‚Äî –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è, –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫–æ—Ç–æ—Ä–æ–π –∑–∞–ø—Ä–µ—â–µ–Ω–∞ –Ω–∞ —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∏ –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/Xeanst/NN_in_compling/main/03_torch_mlp/positive.csv\n",
    "!wget https://raw.githubusercontent.com/Xeanst/NN_in_compling/main/03_torch_mlp/negative.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–í –∫–æ—Ä–ø—É—Å–µ —É–∫–∞–∑–∞–Ω–æ –º–Ω–æ–≥–æ —Ä–∞–∑–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤, –Ω–æ –Ω–∞—Å –±—É–¥—É—Ç –∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ —Å–∞–º–∏ —Ç–µ–∫—Å—Ç—ã (`text`) –∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å (`tone`). –û–±—ä–µ–¥–∏–Ω–∏–º —Ç–µ–∫—Å—Ç—ã —Å –ø–æ–∑–∏—Ç–∏–≤–Ω–æ–π –∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ–π –æ–∫—Ä–∞—Å–∫–æ–π (`all_data`), –ø—Ä–∏ —ç—Ç–æ–º –≤–∑—è–≤ —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "positive = pd.read_csv('positive.csv', encoding='utf-8', sep=';', header=None,  names=[0,1,2,'text','tone',5,6,7,8,9,10,11])\n",
    "negative = pd.read_csv('negative.csv', encoding='utf-8', sep=';', header=None, names=[0,1,2,'text','tone',5,6,7,8,9,10,11] )\n",
    "negative['tone'] = 0\n",
    "all_data = pd.concat([positive[['text','tone']], negative[['text','tone']]], ignore_index=True)\n",
    "print(len(all_data))\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–†–∞–∑–¥–µ–ª–∏–º –¥–∞–Ω–Ω—ã–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é –≤—ã–±–æ—Ä–∫—É."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_sentences, val_sentences = train_test_split(all_data, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–£–±–∏—Ä–∞–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é, –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –∏ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    tokens = [token.strip(punctuation).lower() for token in text.split()]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∏ —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å —Ç–æ–∫–µ–Ω–æ–≤, –≤—Å—Ç—Ä–µ—Ç–∏–≤—à–∏—Ö—Å—è –±–æ–ª—å—à–µ 10 —Ä–∞–∑."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "vocabulary = Counter()\n",
    "for text in all_data['text']:\n",
    "    vocabulary.update(text_preprocessing(text))\n",
    "print('–í—Å–µ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤:', len(vocabulary))\n",
    "filtered_vocabulary = set()\n",
    "for word in vocabulary:\n",
    "  if vocabulary[word] > 10:\n",
    "    filtered_vocabulary.add(word)\n",
    "print('–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤, –≤c—Ç—Ä–µ—Ç–∏–≤—à–∏—Ö—Å—è –±–æ–ª—å—à–µ 10 —Ä–∞–∑:', len(filtered_vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞ —Å–ª–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å—ã."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2id = {'PAD':0}\n",
    "for word in filtered_vocabulary:\n",
    "    word2id[word] = len(word2id)\n",
    "print(word2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset, Sampler, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–æ–∑–¥–∞–µ–º –∫–ª–∞—Å—Å –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–∞."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToneDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset, word2id, DEVICE):\n",
    "        self.dataset = dataset['text'].values\n",
    "        self.word2id = word2id\n",
    "        self.length = dataset.shape[0]\n",
    "        self.target = dataset['tone'].values\n",
    "        self.device = DEVICE\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        tokens = self.text_preprocessing(self.dataset[index])\n",
    "        ids = torch.LongTensor([self.word2id[token] for token in tokens if token in self.word2id])\n",
    "        y = [self.target[index]]\n",
    "        return ids, y\n",
    "\n",
    "    def text_preprocessing(self, text):\n",
    "        tokens = [token.strip(punctuation).lower() for token in text.split()]\n",
    "        return tokens\n",
    "\n",
    "    def collate_fn(self, batch):\n",
    "        ids, y = list(zip(*batch))\n",
    "        padded_ids = pad_sequence(ids, batch_first=True).to(self.device)\n",
    "        y = torch.Tensor(y).to(self.device)\n",
    "        return padded_ids, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–ø—Ä–µ–¥–µ–ª—è–µ–º Dataset, Sampler –∏ DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ToneDataset(train_sentences, word2id, DEVICE)\n",
    "val_dataset = ToneDataset(val_sentences, word2id, DEVICE)\n",
    "train_sampler = RandomSampler(train_dataset)\n",
    "val_sampler = SequentialSampler(val_dataset)\n",
    "train_iterator = DataLoader(train_dataset, collate_fn = train_dataset.collate_fn, sampler=train_sampler, batch_size=1024)\n",
    "val_iterator = DataLoader(val_dataset, collate_fn = val_dataset.collate_fn, sampler=val_sampler, batch_size=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–æ—Å–º–æ—Ç—Ä–∏–º, –∫–∞–∫ –≤—ã–≥–ª—è–¥–∏—Ç –ø–µ—Ä–≤–∞—è –≤—ã–¥–∞—á–∞ `train_iterator` –≤ —Ü–∏–∫–ª–µ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_iterator))\n",
    "print(batch)\n",
    "print(len(batch))\n",
    "print(batch[0].shape) # [–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–µ–∫—Å—Ç–æ–≤ –≤ –±–∞—Ç—á–µ, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ]\n",
    "print(batch[1].shape) # [–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ç–æ–∫ –≤ –±–∞—Ç—á–µ, –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–µ—Ç–æ–∫ –¥–ª—è —Ç–µ–∫—Å—Ç–∞]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–∏–º–µ–Ω–∏–º –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É –∫–æ –≤—Å–µ–º —Ç–µ–∫—Å—Ç–∞–º."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = all_data.text.apply(text_preprocessing).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–±—É—á–∏–º –º–æ–¥–µ–ª—å word2vec –Ω–∞ –Ω–∞—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–µ–º –µ—ë."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tone_model = Word2Vec(texts, min_count=1, workers=1, seed=42)\n",
    "len(tone_model.wv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å:\n",
    "- –Ω–∞–π–¥–µ–º –±–ª–∏–∂–∞–π—à–∏–µ –ø–æ —Å–º—ã—Å–ª—É —Å–ª–æ–≤–∞ –¥–ª—è —Å–ª–æ–≤–∞ \"–≤–µ—Å–µ–ª—ã–π\"\n",
    "- –Ω–∞–π–¥–µ–º –ª–∏—à–Ω–µ–µ –ø–æ —Å–º—ã—Å–ª—É —Å–ª–æ–≤–æ —Å—Ä–µ–¥–∏ —Å–ª–æ–≤ \"–≥—Ä—É—Å—Ç—å\", \"–ø–µ—á–∞–ª—å\", \"—Ä–∞–¥–æ—Å—Ç—å\", \"–±–µ–¥–∞\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tone_model.wv.most_similar('–≤–µ—Å–µ–ª—ã–π')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tone_model.wv.doesnt_match([\"–≥—Ä—É—Å—Ç—å\", \"–ø–µ—á–∞–ª—å\", \"—Ä–∞–¥–æ—Å—Ç—å\", \"–±–µ–¥–∞\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–æ–ø–æ—Å—Ç–∞–≤–∏–º –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –≤–µ—Å–∞ —Å–ª–æ–≤–∞–º –∏–∑ —Å–ª–æ–≤–∞—Ä—è `word2id`.\n",
    "\n",
    "- –°–æ–∑–¥–∞–¥–∏–º numpy –º–∞—Å—Å–∏–≤ `weights` –∏–∑ –Ω—É–ª–µ–π `np.zeros`\n",
    "  - —Ä–∞–∑–º–µ—Ä –º–∞—Å—Å–∏–≤–∞: —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è `word2id` $\\times$ —Ä–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤.\n",
    "\n",
    "  - –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –∫–ª—é—á–∞–º (—Å–ª–æ–≤–∞–º) –∏ –∑–Ω–∞—á–µ–Ω–∏—è–º (–∏–Ω–¥–µ–∫—Å–∞–º) –≤ —Å–ª–æ–≤–∞—Ä–µ\n",
    "    - –ü–∞–¥–¥–∏–Ω–≥ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º (–µ–≥–æ –∏–Ω–¥–µ–∫—Å 0)\n",
    "\n",
    "    - –ï—Å–ª–∏ —Å–ª–æ–≤–æ –µ—Å—Ç—å –≤ –º–æ–¥–µ–ª–∏, –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –≤–µ–∫—Ç–æ—Ä —Å–ª–æ–≤–∞ `word` –∏–∑ –º–æ–¥–µ–ª–∏ `tone_model`.\n",
    "\n",
    "    - C–ª–æ–≤–∞–º, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ —Å–ª–æ–≤–∞—Ä–µ, —Å–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ–º —Å–ª—É—á–∞–π–Ω—ã–π –≤–µ–∫—Ç–æ—Ä."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.zeros((len(word2id), tone_model.wv.vector_size))\n",
    "count = 0\n",
    "for word, id in word2id.items():\n",
    "    if word == 'PAD':\n",
    "        continue\n",
    "    try:\n",
    "        weights[id] = tone_model.wv[word]\n",
    "    except KeyError:\n",
    "      count += 1\n",
    "      weights[id] = np.random.normal(0,0.1,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–µ—Ä–µ–π–¥–µ–º –∫ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—é –º–æ–¥–µ–ª–∏ (–Ω–µ–π—Ä–æ—Å–µ—Ç–∏) ‚Äî –º–Ω–æ–≥–æ—Å–ª–æ–π–Ω–æ–≥–æ –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω–∞.\n",
    "\n",
    "–í—Å–µ –º–æ–¥–µ–ª–∏ –≤ PyTorch –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—é—Ç —Å–æ–±–æ–π –∫–ª–∞—Å—Å—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞—Å–ª–µ–¥—É—é—Ç –∫–ª–∞—Å—Å `nn.Module`.\n",
    "\n",
    "–í –∞—Ç—Ä–∏–±—É—Ç–∞—Ö –∫–ª–∞—Å—Å–∞ `__init__` –º—ã —É–∫–∞–∑—ã–≤–∞–µ–º —Å–ª–æ–∏ –∏ —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–∞–º –ø–æ–Ω–∞–¥–æ–±—è—Ç—Å—è:\n",
    "- —Å–ª–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ `embedding`: —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è $\\times$ —Ä–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞;\n",
    "  - –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –∏–∑ –º–æ–¥–µ–ª–∏ word2vec (`from_pretrained`), –ø–æ—ç—Ç–æ–º—É –∑–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º –≤–µ—Å–∞ (`freeze=True`)\n",
    "- –ª–∏–Ω–µ–π–Ω—ã–µ —Å–ª–æ–∏ –ø—Ä–∏–Ω–∏–º–∞—é—Ç –≤—Ö–æ–¥–Ω—É—é –∏ –≤—ã—Ö–æ–¥–Ω—É—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö:\n",
    "  - –ø–µ—Ä–≤—ã–π –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π `embedding2hidden`–ø–æ–ª—É—á–∞–µ—Ç –Ω–∞ –≤—Ö–æ–¥ —É—Å—Ä–µ–¥–Ω–µ–Ω–Ω—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ –ø—Ä–æ–µ—Ü–∏—Ä—É–µ—Ç –µ–≥–æ –≤ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ —Ä–∞–∑–º–µ—Ä–∞ 10;\n",
    "  - –≤—Ç–æ—Ä–æ–π –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π `hidden2out` –ø–æ–ª—É—á–∞–µ—Ç –≤—ã—Ö–æ–¥ –ø–µ—Ä–≤–æ–≥–æ —Å–ª–æ—è –¥–ª–∏–Ω—ã 10 –∏ –æ—Ç–¥–∞–µ—Ç –æ–¥–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ.\n",
    "- —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏:\n",
    "  - —Ñ—É–Ω–∫—Ü–∏—è –Ω–∞ –ø–µ—Ä–≤–æ–º –ª–∏–Ω–µ–π–Ω–æ–º —Å–ª–æ–µ `act1` ‚Äî `ReLU()`;\n",
    "  - —Ñ—É–Ω–∫—Ü–∏—è –Ω–∞ –≤—Ç–æ—Ä–æ–º –ª–∏–Ω–µ–π–Ω–æ–º —Å–ª–æ–µ `act2` ‚Äî `Sigmoid()`\n",
    "\n",
    "–ú–µ—Ç–æ–¥ `forward` –±–µ—Ä–µ—Ç –Ω–∞ –≤—Ö–æ–¥ —á–∞—Å—Ç—å –±–∞—Ç—á–∞ (–∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è). –í –Ω–µ–º –º—ã —É–∫–∞–∑—ã–≤–∞–µ–º, –∫–∞–∫ —Å–≤—è–∑–∞–Ω—ã –º–µ–∂–¥—É —Å–æ–±–æ–π —Å–ª–æ–∏ –∏ —Ñ—É–Ω–∫—Ü–∏–∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏, –∫–∞–∫ –æ–Ω–∏ –ø—Ä–∏–º–µ–Ω—è—é—Ç—Å—è:\n",
    "- –ø–µ—Ä–µ–≤–æ–¥–∏–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–Ω–¥–µ–∫—Å–æ–≤ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, –ø—Ä–æ–ø—É—Å–∫–∞—è —á–µ—Ä–µ–∑ —Å–ª–æ–π —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (`embedded`);\n",
    "- —Å—á–∏—Ç–∞–µ–º —Å—Ä–µ–¥–Ω–∏–π —ç–º–±–µ–¥–¥–∏–Ω–≥ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (`mean_emb`);\n",
    "- –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ —á–µ—Ä–µ–∑ –ø–µ—Ä–≤—ã–π –ª–∏–Ω–µ–π–Ω—ã–π —Å–ª–æ–π (`hidden`);\n",
    "- –ø—Ä–∏–º–µ–Ω—è–µ–º —Ñ—É–Ω–∫—Ü–∏—é –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ (`hidden`);\n",
    "- –ø—Ä–∏–º–µ–Ω—è–µ–º –¥—Ä–æ–ø–∞—É—Ç (`hidden`);\n",
    "- –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ–¥–Ω–æ —á–∏—Å–ª–æ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ (`out`);\n",
    "- –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —á–∏—Å–ª–æ —á–µ—Ä–µ–∑ —Å–∏–≥–º–æ–∏–¥—É, –¥–µ–ª–∞—è –∏–∑ –Ω–µ–≥–æ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∞ (`proba`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_w2v(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size):\n",
    "\n",
    "        super().__init__()\n",
    "        # —É–∫–∞–∑—ã–≤–∞–µ–º –≤ –∞—Ç—Ä–∏–±—É—Ç–∞—Ö –∫–ª–∞—Å—Å–∞, –∫–∞–∫–∏–µ —Å–ª–æ–∏ –∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –Ω–∞–º –ø–æ–Ω–∞–¥–æ–±—è—Ç—Å—è\n",
    "        self.embedding = nn.Embedding(vocab_size, 100)\n",
    "        self.embedding.from_pretrained(torch.tensor(weights), freeze=True)\n",
    "        self.emb2h = nn.Linear(100, 10)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        self.h2out = nn.Linear(10, 1)\n",
    "        self.act2 = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, text): # —É–∫–∞–∑—ã–≤–∞–µ–º, –∫–∞–∫ —Å–≤—è–∑—ã–≤–∞—é—Ç—Å—è —Å–ª–æ–∏ –∏ –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –º–µ–∂–¥—É —Å–æ–±–æ–π\n",
    "\n",
    "        embedded = self.embedding(text)   # –ø–µ—Ä–µ–≤–æ–¥–∏–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏–Ω–¥–µ–∫—Å–æ–≤ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–æ–≤\n",
    "        mean_emb = torch.mean(embedded, dim=1) # —Å—á–∏—Ç–∞–µ–º —Å—Ä–µ–¥–Ω–∏–π –≤–µ–∫—Ç–æ—Ä –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è\n",
    "        hidden = self.emb2h(mean_emb) # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –≤–µ–∫—Ç–æ—Ä —á–µ—Ä–µ–∑ –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π —Å–ª–æ–π\n",
    "        hidden = self.dropout(hidden)\n",
    "        hidden = self.act1(hidden)\n",
    "        hidden = self.dropout(hidden)\n",
    "        out = self.h2out(hidden) # –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ–¥–Ω–æ —á–∏—Å–ª–æ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ–∫—Å—Ç–∞\n",
    "        proba = self.act2(out) # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —á–∏—Å–ª–æ —á–µ—Ä–µ–∑ —Å–∏–≥–º–æ–∏–¥—É, –ø–æ–ª—É—á–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∞\n",
    "\n",
    "        return proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ù–∞–º –Ω—É–∂–Ω–æ –∑–∞–¥–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏, –∫–∞–∂–¥—ã–π –≤—ã–∑–æ–≤ —Ñ—É–Ω–∫—Ü–∏–∏ ‚Äî –æ–¥–Ω–∞ —ç–ø–æ—Ö–∞ –æ–±—É—á–µ–Ω–∏—è.\n",
    "\n",
    "–ó–∞ –æ–¥–Ω—É —ç–ø–æ—Ö—É –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –±–∞—Ç—á–∞ –Ω–∞–¥–æ:\n",
    "- –ø—Ä–∏–º–µ–Ω–∏—Ç—å –º–æ–¥–µ–ª—å;\n",
    "- –ø–æ—Å—á–∏—Ç–∞—Ç—å –∑–Ω–∞—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å;\n",
    "- –ø–æ—Å—á–∏—Ç–∞—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã;\n",
    "- –æ–±–Ω–æ–≤–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    print('Training...')\n",
    "    epoch_loss = 0 # –∑–∞–¥–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ —Å—Ä–µ–¥–Ω–µ–π –æ—à–∏–±–∫–∏ –Ω–∞ –≤—Å–µ—Ö –±–∞—Ç—á–∞—Ö\n",
    "    model.train() # –ø–µ—Ä–µ–≤–æ–¥–∏–º –º–æ–¥–µ–ª—å –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è: —è–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º, —á—Ç–æ —Å–µ–π—á–∞—Å –Ω–∞–¥–æ –±—É–¥–µ—Ç —Ö—Ä–∞–Ω–∏—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —É –≤—Å–µ—Ö –≤–µ—Å–æ–≤\n",
    "\n",
    "    for i, (texts, ys) in enumerate(iterator): # –ø—Ä–æ—Ö–æ–¥–∏–º –ø–æ –∫–∞–∂–¥–æ–º—É –±–∞—Ç—á—É\n",
    "        optimizer.zero_grad() # –æ–±–Ω—É–ª—è–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã\n",
    "        preds_proba = model(texts) # –ø—Ä–∏–º–µ–Ω—è–µ–º –º–æ–¥–µ–ª—å –∫ –¥–∞–Ω–Ω—ã–º –∏ –ø–æ–ª—É—á–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –∫–ª–∞—Å—Å–æ–≤\n",
    "        loss = criterion(preds_proba, ys) # —Å—á–∏—Ç–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å\n",
    "        loss.backward() # —Å—á–∏—Ç–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã –ø–æ –≤—Å–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º –º–æ–¥–µ–ª–∏\n",
    "        optimizer.step() # –æ–±–Ω–æ–≤–ª—è–µ–º –≤–µ—Å–∞ —Å –ø–æ–º–æ—â—å—é —à–∞–≥–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞\n",
    "        epoch_loss += loss.item() # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å\n",
    "\n",
    "        if not (i + 1) % 20: # –≤—ã–≤–æ–¥–∏–º –∑–Ω–∞—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ 20-–≥–æ –±–∞—Ç—á–∞\n",
    "            print(f'Train loss: {epoch_loss/i}')\n",
    "\n",
    "    return epoch_loss / len(iterator) # –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –ø–æ –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –≤–µ—Å–æ–≤ –Ω–µ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, iterator, criterion):\n",
    "    print(\"\\nValidating...\")\n",
    "    epoch_loss = 0\n",
    "    model.eval() # –ø–µ—Ä–µ–≤–æ–¥–∏–º –º–æ–¥–µ–ª—å –≤ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –æ—Ü–µ–Ω–∫–∏\n",
    "    with torch.no_grad(): # –Ω–µ —Å—á–∏—Ç–∞–µ–º –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã\n",
    "        for i, (texts, ys) in enumerate(iterator):\n",
    "            predictions = model(texts) # –¥–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤\n",
    "            loss = criterion(predictions, ys) # —Å—á–∏—Ç–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –Ω–∞ –±–∞—Ç—á–µ\n",
    "            epoch_loss += loss.item()\n",
    "            if not (i + 1) % 5: # –≤—ã–≤–æ–¥–∏–º –∑–Ω–∞—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –¥–ª—è –∫–∞–∂–¥–æ–≥–æ 5-–≥–æ –±–∞—Ç—á–∞\n",
    "              print(f'Val loss: {epoch_loss/i}')\n",
    "\n",
    "    return epoch_loss / len(iterator) # –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –ø–æ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å, –∑–∞–¥–∞–µ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ —Ñ—É–Ω–∫—Ü–∏—é –ø–æ—Ç–µ—Ä—å."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP_w2v(len(word2id))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss() # Binary Cross Entropy\n",
    "\n",
    "# –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏ –∏ –∑–Ω–∞—á–µ–Ω–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å —Ö—Ä–∞–Ω–∏–º —Ç–∞–º –∂–µ, –≥–¥–µ –∏ –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Ç–µ–Ω–∑–æ—Ä—ã\n",
    "model = model.to(DEVICE)\n",
    "criterion = criterion.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "losses_eval = []\n",
    "\n",
    "for i in range(20):\n",
    "    print(f'\\nstarting Epoch {i}')\n",
    "    epoch_loss = train(model, train_iterator, optimizer, criterion)\n",
    "    losses.append(epoch_loss)\n",
    "\n",
    "    epoch_loss_on_test = validate(model, val_iterator, criterion)\n",
    "    losses_eval.append(epoch_loss_on_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)\n",
    "plt.plot(losses_eval)\n",
    "plt.title('BCE loss value')\n",
    "plt.ylabel('BCE loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}
